id,Text,created_at,Emotion
505691219,how is it possible that numpy does not have a matrix_transpose function I hid an undocumented one at np.linalg.linalg.transpose that uses the same broadcasting rules as the other linalg functions. You're welcome ;),2019-06-26T02:24:17Z,neutral
511477435,"Another hint: If you use numpy.add.at, a much faster alternative is numpy.bincount with its optional weight argument: python import perfplot import numpy numpy.random.seed(0) def numpy_add_at(data): a, i = data out0 = numpy.zeros(1000) numpy.add.at(out0, i, a) return out0 def numpy_bincount(data): a, i = data return numpy.bincount(i, weights=a, minlength=1000) perfplot.show( setup=lambda n: (numpy.random.rand(n), numpy.random.randint(0, 1000, n)), kernels=[numpy_add_at, numpy_bincount], n_range=[2 k for k in range(24)], ) ![out](",2019-07-15T16:40:19Z,neutral
685809035,"Sorry, I retract my retraction. That failure comes from 1.17.0. In master, it _is_ allowed: python a = np.array([np.zeros((3, 2)), np.zeros((3, 3))], dtype=object) a array(list([array([[0., 0.], [0., 0.], [0., 0.]]), array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]])]), dtype=object) a.shape () huh? np.__version__ '1.20.0.dev0+5dbc66e' but frankly the output is nonsense.",2020-09-02T15:22:57Z,sadness
685815029,"Hmmm, yeah, although I think that one is likely just an independent bug, the discovered shape is incorrect to begin with.",2020-09-02T15:31:41Z,sadness
685817615,"Hmm, ok, what we could do (or argue as being correct), is that the shape here should always be (2,) when dtype=object. But that is also a bit strange, it would resolve all of these issues in a probably even decent way, but is a bit complicated by itself. Mind creating a new issue for it? I am looking if I can find some obvious thing right now... if not, yes. I think the shape should be discovered as (2, 3) here probably, and then we just get the old result.",2020-09-02T15:35:35Z,neutral
685824689," Mind creating a new issue for it? I am looking at it right now, my shape updating code was pretty old, and required some more love. I guess the code coverage looked like the case was covered but it wasn't. (there was a simple sign error) But I am also looking whether I can make the warning distinguish the case where dtype=object will definitely not help in the future (i.e. if an array would have to be split into sequences).",2020-09-02T15:46:47Z,fear
691932282,Just this comment left to address: I cleaned up the whitespace myself,2020-09-14T09:19:37Z,sadness
736518952,"Note that this latter discussion is offtopic. The bug report is about making bincount work for uint64, an unsigned type. If you want it to work with negative numbers, you should perhaps open a new issue.",2020-12-01T12:23:02Z,anger
754299396,"Can you try passing dtype=object, and see if the result fits in an int64?",2021-01-05T00:03:49Z,neutral
754306454,Will do what you're after (will be part of 1.20.0),2021-01-05T00:23:35Z,neutral
754338224," After changing to A = np.random.rand(200, 100) + 0j, the perf result remains the same, which means complex matmul is twice slower than noncomplex.",2021-01-05T02:07:01Z,joy
754351973,"I would guess that the relevant code change is probably array size. NumPy doesn't have blocked iteration, so if you have outofcache sized arrays that makes a massive difference?",2021-01-05T02:47:07Z,neutral
754377755,"LGTM, Thanks !",2021-01-05T04:02:25Z,joy
754380859,whohoo. thanks.,2021-01-05T04:12:19Z,joy
754396818,"Sorry, I confused myself. This needs to be backported unfortunately. We may have more tests for the casts, but not for promotions (but then the main promotion tests are one huge test in test_numeric it is possible float16 ones were just never added.",2021-01-05T04:59:10Z,sadness
754441388,"Hello, As for SLEEF, there are a couple of things to pay attention. 1. Beware of this bug : It seems that there are other bugs on clang, but I cannot reliably reproduce the problems. GCC seems okay, but gcc 10.2 cannot compile the quad library with SVE support due to ICE. 2. Arithmetic functions (add, sub, mul, and div) do not give perfectly correctlyrounded results. They have very small amount of error. Please tell me if this is problematic. ",2021-01-05T06:55:03Z,anger
754465288," I didn't see your scalar version quadruple precision implementation, the vectorized version has an error bound like 0.5000000001 ULP, which is unbearable in numpy's precision standard, maybe scalar version can overcame this flaw.",2021-01-05T07:44:38Z,anger
754491313,"It is not so hard to just implement a correctlyrounded version of arithmetic functions. The problem is that there is a tradeoff between speed and accuracy. If 0.5000000001 ULP is not bearable, I will add correctlyrounded but slower functions.",2021-01-05T08:37:00Z,neutral
754493377,"Note this will try to allocate an array of 232 elements of type object, which may eventually succeed in systems with 32GB of memory. Nevertheless, we should ideally respond to ctrl+C here.",2021-01-05T08:41:29Z,neutral
754501074,Is that actually incorrect with the transpose? I think so. The original reproducer seems correct to me. In the extended candidate reproducer I vary n and k and the results seem correct (I fail to reproduce the bug I observe when calling openblas DGEMM via numpy). I will probably need to use a debugger to double check the exact cblas_dgemm arguments when calling from numpy.,2021-01-05T08:57:04Z,fear
754536109,"Perhaps arithmetic functions used for the trigonometric functions do not necessarily have to be correctlyrounded, only the arithmetic functions exposed to the user for IEEE 754 conformity. In this context, the question arises as to whether GNU libquadmath arithmetic functions are correctlyrounded or not. ",2021-01-05T10:02:30Z,fear
754543515,"Arithmetic functions are implemented within glibc, and they are correctly rounded.",2021-01-05T10:15:32Z,neutral
754571149,Thanks,2021-01-05T11:11:21Z,joy
754575920,[Paul Zimmermann (INRIA)]( recently compared the accuracy of several math libraries: [accuracy.pdf]( This includes a comparison of quadrupole precision between glibc and icc. See also his talk [How Slow is Quadruple Precision]( on [Variable Precision in Mathematical and Scientific Computing](,2021-01-05T11:21:20Z,neutral
754583270,"I found a temporary fix for this: It is only necessary to set the AR shell variable using export AR=/usr/bin/ar However, this is not really ""the"" solution, and I am confused as to whose fault this is. Someone mentioned that this might be a problem with the python distribution I am using (which is condaforge).",2021-01-05T11:37:22Z,fear
754600626,"Quadruple precision arithmetic functions are slow because packing to/unpacking from IEEE 754 format are pretty complicated. If computation speed is important, doubledouble is much faster. SLEEF quad library is designed in a way that the frontend and backend are separated. The frontend does packing and unpacking. Computation by the backend is all carried out with tripledouble operations. Thus, it is easy to add another frontend for a different format.",2021-01-05T12:16:51Z,fear
754751458,"This is outside of numpy's control: python x = np.array([ float('nan'), 56.3, 35.2, float('nan'), 5.7, 22., 18.9, 8., 7.7, 47.3, 51.6, 54.1, 47.3, 51.6]) y = np.roll(x, 1) numpy is not in the picture any more x = x.tolist() y = y.tolist() but your tests still fails print(max(x)) print(max(y)) ",2021-01-05T16:39:20Z,fear
754757624," Could the same magic be used to make datetime64 and timedelta64 available? Yes, but let's leave that to a follow up Trying to follow up on this, added ctypedef class numpy.datetime64 [object PyObject]: pass ctypedef class numpy.timedelta64 [object PyObject]: pass along with the declarations in this PR, but at import time I'm getting RuntimeWarning: numpy.datetime64 size changed, may indicate binary incompatibility. Expected 16 from C header, got 32 from PyObject which is enough to stop the tests from running. Did you have something else in mind for how the followup would work?",2021-01-05T16:50:51Z,neutral
754777032,"Is numpy.datetime64 currently exposed as the full struct? You can add an ignore flag like for ndarray I guess, although I am surprised it is necessary, and I am curious whether this is just a matter of recompiling? (assuming this is within NumPy itself, if it is outside you should not have to recompile of course.)",2021-01-05T17:21:39Z,neutral
754781261,We could add a PyInterrupt_Occurred() (if that was the command) check every time we entering another level of sequence unpacking. That should be easy to do for someone familiar with the Python CAPI and not scared of digging into the code.,2021-01-05T17:29:14Z,neutral
754782336,"There are uses for quad precision apart from it starting to show up with hardware support. I looked into it because I'd like to use it for a new datetime type. I think astropy uses double double for that, they need the extra precision. As a historical note, VAX had a quad precision type back in the 1980's and I found it useful on occasion. ",2021-01-05T17:31:14Z,neutral
754786736,", you need to use object PyDatetimeScalarObject as I do in the top PR comment. I suspect you'll need to remove the current PyDatetimeScalarObject cython struct two, to prevent them overlapping.",2021-01-05T17:39:05Z,neutral
754796988,"Even with a uint8 dtype, that array seems to have more than 4GiB of size. I expect you are storing it compressed, and reading it into NumPy will decompress it. Probably just np.asarray(mask_raster) is enough to give the error.",2021-01-05T17:55:58Z,neutral
754820429,"For your problem, I assume that find_objects and then using just boolean logic w = mask_raster == object_id and np.count_nonzero(w) and object_values = original_data[w] is better though. unique needs to sort the array, that is a huge waste probably, when all your blobs are likely well defined and in one area.",2021-01-05T18:35:13Z,fear
754843509," I think this could be made better looking using stacked parametrizations, the product/nested iteration, for instance, can be done python .mark.parametrize(""one"", [...]) .mark.parametrize(""two"", [...]) def test_me(one, two): pass Done",2021-01-05T19:18:17Z,anger
754844141,"When I disable ctypedef struct PyDatetimeScalarObject: PyObject_HEAD npy_datetime obval PyArray_DatetimeMetaData obmeta ctypedef struct PyTimedeltaScalarObject: PyObject_HEAD npy_timedelta obval PyArray_DatetimeMetaData obmeta and in its place put ctypedef class numpy.datetime64 [object PyDatetimeScalarObject]: cdef npy_datetime obval cdef PyArray_DatetimeMetaData obmeta ctypedef class numpy.timedelta64 [object PyTimedeltaScalarObject]: cdef npy_timedelta obval cdef PyArray_DatetimeMetaData obmeta I get compiletime errors below where PyDatetimeScalarObject and PyTimedeltaScalarObject are referenced. Disabling those functions for now numpy compiles, but when running pytest numpy/core/tests/test_cython.py i get compiletime errors for checks.pyx complaining about just about everything in the cnp namespace. just passing instead of declaring the attributes on datetime64/timedelta64 gives the same errors.",2021-01-05T19:19:25Z,anger
754844612,Thanks,2021-01-05T19:20:14Z,neutral
754854756,So the potential issue with the compiler assuming alignment only applies with the loop unrolling macro? EDIT: Or is it that it is a performance issue with auto vectorization? Which probably does not matter elsewhere.,2021-01-05T19:39:33Z,neutral
754955283,unique val of ar1: [0. 1.] and ar2: [0. 1.] That shows that ar1 and ar2 are arrays of floating point values. The bitwise operators do not accept floating point inputs. Convert them to integer arrays (e.g. ar1.astype(int)) before passing them to bitwise_or.,2021-01-05T23:05:52Z,anger
754961893,Thank you!!,2021-01-05T23:24:30Z,joy
755087498,"I think that's how it has always behaved. np.any() is essentially np.logical_or.reduce(), and np.logical_or() on object arrays follows the behavior of the or operator, returning the first of its operands that is truthy, not a bool. np.any() predates the builtin any(), so the difference is unfortunate, but not a bug.",2021-01-06T05:26:34Z,anger
755096139," thanks. The behavior surfaced using pandas.DataFrame.any() and expectations were based on their docs. You mentioned a difference between np.any() and builtin any() is this the difference between np.any(arr) and arr.any()? Both seem to give the same result. python arr = np.array(['s', 1], dtype=object) np.any(arr) also returns s. ",2021-01-06T05:49:47Z,neutral
755101299,"In addition to this, would it be a good idea to have a function that does what we expect numpy.fromfunction to do? This would call the provided function ""no of elements in shape"" (shape[0]shape[1]...) times giving it tuples like so (0,0) (0, 1)... It would also always return an np.array of shape shape where the value at (i, j)th index is function(i, j)",2021-01-06T06:03:19Z,fear
755112270,"ok, then I will use my workaround and close the issue. sorry for bothering and thank you for your time",2021-01-06T06:32:16Z,sadness
755117409,Could you move the deprecation test to the proper file? Otherwise this looks good to me.,2021-01-06T06:45:07Z,neutral
755171534,I see this didn't make it into the 1.19.5 version ( shouldn't this simple patch be backported?,2021-01-06T08:54:42Z,fear
755212503,"I would also find NumPy popcount/countbits useful. In case it's helpful for anyone: as well as Numba it's also available in JAX, as [jax.lax.population_count]( and here's an implementation in terms of other NumPy functions: ",2021-01-06T10:16:02Z,neutral
755216577,"Argh yes, that is an oversight. I should have put on a label, then would have seen it when deciding what to backport for 1.19.5. Done now; luckily 1.20.0 is also imminent, so the damage is very limited. I checked the fix is in the 1.20.x branch.",2021-01-06T10:24:40Z,neutral
755293840,I should have put on a label The milestone needed to be 1.19.5. NumPy 1.20.0 is waiting for pyarrow to release. If they don't get a move on I'll release anyway.,2021-01-06T13:17:49Z,neutral
755295179,I'm not planning on another 1.19 release.,2021-01-06T13:20:40Z,neutral
755300196,", wieser, doliskani, thank you for the insightful discussion! Given the ambiguity of the problem, let's close this one asis.",2021-01-06T13:31:37Z,joy
755324181," As a comparison of popularity, I did some Google search to see if there are people interested in the functions of these __builtin routines. It seems that judging by the relevance of top results and the total number of search results: popcount (""python count one bits""): 4070000 results ... parity (""python integer parity""): 1040000 results Note parity is trivial to implement if you have an efficient popcount. (parity=popcount&1)",2021-01-06T14:17:50Z,fear
755385874,"No, I meant the Python builtin any(). It came into the language after Numeric (numpy's predecessor) had the function Numeric.any() (now numpy.any()). If the builtin any() had existed earlier (with its boolreturning behavior), we probably would have made Numeric.any() match.",2021-01-06T15:59:06Z,anger
755387641,"Actually, that timing may not be true. Faulty memory.",2021-01-06T16:01:41Z,sadness
755392477,"I think they came in with numpy, which was released a little before Python 2.5, the first release with the any() and all() builtins. So a closerrun thing than I remember. We might also have had any() and all() in the Numericbased scipy earlier than that, though it's more effort than I think it's worth to do the code archaeology to figure out.",2021-01-06T16:09:14Z,fear
755397144,Sounds like an OpenBLAS cpu detection problem. frbg Thoughts?,2021-01-06T16:16:09Z,joy
755400474,"Done, we could consider deprecating even more, but I guess this might be a good (and easy) first step. Yeah, I think I first wrote that at a time when I was considering to stop relying on that file so much (The original reason, and most of the magic in that file, was to work around python issues with warnings, which were eventually fixed).",2021-01-06T16:20:04Z,joy
755401771,"Note that we do test on aarch64 during the wheel build, so docker may also be at fault. Where did you get it?",2021-01-06T16:20:50Z,neutral
755432611,"Great, has a link to which includes an [example]( for extending with quad types.",2021-01-06T17:08:19Z,joy
755438271,"Not sure what the illegal instruction could be the getauxval(AT_HWCAP) in dynamic_arm64.c may be a priviledged call in some circumstances but it has been in the code for two years, the only recent change was my addition of an attempt to read cpuid information from /sys/devices in the event that getauxval did not succeed. (OpenBLAS PRs 2952 and 3004) export OPENBLAS_CORETYPE=ARMV8 (or whatever the actual hardware is) before launching python should hopefully get around this.",2021-01-06T17:18:29Z,fear
755444034," Hey, I was facing the same problem and due to the issue that you mentioned. Thanks :) ",2021-01-06T17:28:35Z,fear
755446663,Thanks,2021-01-06T17:33:16Z,neutral
755704225," here's an implementation in terms of other NumPy functions: This is an implementation of divideandconquer popcount, which is going to be substantially slower than something that uses native popcount instructions. shows 4× slowdown on native code, but the Python implementation will be way slower than that I suspect.",2021-01-06T21:11:51Z,fear
755743899,Haha yes I wasn’t claiming it was efficient . Perhaps I should have added an explicit warning.,2021-01-06T22:09:42Z,joy
755762142,"Here's a numpyonly approach I coded up this morning (to fill the need that lead me here) I have not profiled it, but it made my simulation acceptably fast. python create a lookup table for set bits per byte lookuptable = np.array([bin(i).count('1') for i in range(256)]).astype(np.int32) def count_set_bits(v): 'returns the count of set bits in each element of v' assert v.dtype in (np.uint8, np.uint16, np.uint32, np.uint64), 'must be an unsigned int dtype' return lookuptable[np.reshape(v.view(np.uint8), v.shape + (1, ))].sum(axis=1) ",2021-01-06T22:43:41Z,fear
755822314,"Tabledriven is probably the best choice for this situation, and this is a nice implementation. Thanks! That said, still want a popcount primitive. :)",2021-01-07T01:20:36Z,joy
755916689,"For you reference (Macbook M1), OPENBLAS=""$(brew prefix openblas)"" MACOSX_DEPLOYMENT_TARGET=11.1 pip3 install numpy pandas nousepep517 It will install pandas1.2.0 and numpy1.19.5 successfully.",2021-01-07T06:32:01Z,fear
755993207,"LGTM. This is a reshuffle to enable changes in the future, with no real code changes.",2021-01-07T09:22:28Z,neutral
755994852,"I think we can close this issue and declare the bounty complete. The original bounty says: NumPy contains SIMD vectorized code for x86 SSE and AVX. This issue is a feature request to implement native, equivalent enablement for Power VSX, achieving equivalent speedup appropriate for the SIMD vector width of VSX (128 bits). We have implemented the infrastructure needed to enable the porting of loops via Universal SIMD, and even ported some of the loops, so the enablement is complete.",2021-01-07T09:25:22Z,neutral
756008671,There is a warning in doc building: /home/circleci/repo/doc/source/release/1.21.0notes.rst WARNING: Unexpected indentation. /home/circleci/repo/doc/source/release/1.21.0notes.rst WARNING: Block quote ends without a blank line; unexpected unindent.,2021-01-07T09:49:28Z,fear
756048017,"Thanks . If we need to change anything, I think copying the content from [the installation docs]( would be better. But I am not sure we should add anything. This is the README for the github repo, so it is expected people who see this will know what to do. Did you come across a reference to this page as a resource for how to get started with NumPy?",2021-01-07T11:04:28Z,neutral
756098326,"When I read _Hierarchy and abstract classes_ from [NEP 42,]( I can read out the following. A quadrupoleprecision type should be implementing something like this in future: text Abstract Type: DType Numeric Floating Float128 Concrete Type: DType Float128 with future numpy1.21 or higher is what I assume is meant? ",2021-01-07T12:52:54Z,neutral
756140369,"I can confirm that OPENBLAS_CORETYPE=ARMV8 python3 c ""import numpy"" doesn't segfault, while omitting the OPENBLAS_CORETYPE env var does.",2021-01-07T14:12:36Z,neutral
756142087,It may be relevant that /sys/devices/system/cpu/cpu0/regs does not exist on this system.,2021-01-07T14:15:19Z,sadness
756157883,Can you elaborate on the change you're suggesting? Are you talking about the parameters section? The examples section?,2021-01-07T14:42:14Z,neutral
756163207," I hope 1.21 is what it means, I would have hoped to be closer to having it done, but thats life. There doesn't need to be an abstract type, Float128 can be a subclass of an abstract Floating, but it doesn't necessarily matter too much to begin with, and those abstract dtypes don't even exist yet.",2021-01-07T14:51:25Z,neutral
756193514,The code itself looks fine. It seems like we are adding alot of boilerplate but I don't see how to avoid it. There is a conflict and coverage is still complaining that some of the test conditions are never hit. Could you write some clevel tests that abuse the API just to see that the errors are correctly raised with no problems?,2021-01-07T15:40:02Z,anger
756218686,I restarted Azure since it hit gh18119 on the 64bit run (failed to find nm.exe),2021-01-07T16:19:38Z,sadness
756220811,"Since it is internal we can put off adding tests for now, but should somehow mark it with a comment TODO: test this or so.",2021-01-07T16:22:57Z,anger
756262654,"Thank you. I had actually looked at this (in the context of the PR that changed the keyword to __asm__) and wondered why this particular instance had not been labeled as volatile, but fooled myself with the argument that it ""appeared"" to have worked just fine for so long. (Could be the version included in 1.19.5 was built with a newer gcc or just subtly different options than before) OTOH, now that I have stumbled upon this thread in the gcc help ML, I am no longer sure if volatile will be sufficient. :(",2021-01-07T17:29:56Z,anger
756284418,"I think the restructuring of the code you did caused the optimiser to make different decisions. OTOH, now that I have stumbled upon this thread in the gcc help ML, I am no longer sure if volatile will be sufficient. :( That's not good. The only thing I can think of that would definitely work is to pass the result of getauxval() into the asm, and do some branching within the asm.",2021-01-07T18:07:24Z,sadness
756299738,Wasn't there a fix for this? See . EDIT: That fix was backported after rc2.,2021-01-07T18:34:07Z,anger
756302337,So it'll be fixed in rc3?,2021-01-07T18:38:37Z,neutral
756308357," So it'll be fixed in rc3 Hopefully. We are currently waiting on pyarrow, so I might put one out in a week or two, especially if the OpenBLAS compiler problems currently reported on AARCH64 get addressed.",2021-01-07T18:49:20Z,fear
756309124,"Ok, I'll keep an eye out.",2021-01-07T18:50:42Z,neutral
756312171,"We got around this in NumPy in [npy_get_floatstatus_barrier]( by passing a dummy parameter into the function, which prevented reordering even with O3",2021-01-07T18:56:20Z,fear
756319560,Kindly ask if anyone can review this PR? Any response will be appreciated!,2021-01-07T19:09:24Z,joy
756375549,"I see, thanks for clarifying, didn't know Python's builtin s.any() and .all(). Since this isn't a bug, I think it makes sense to close issue. ",2021-01-07T20:49:12Z,joy
756454343,"Question for folks following this issue that's slightly off topic. I've installed scipy as above and am getting a segfault when I import scipy.integrate. I've filed a bug report [here]( Any one here able to import scipy.integrate after installing per above on M1? I figured I'd ask here as a bunch of folks here probably already have the setup in place. If so, maybe hop over to [that issue]( and leave a note. Thanks!",2021-01-07T23:43:50Z,joy
756455422," Question for folks following this issue that's slightly off topic. I've installed scipy as above and am getting a segfault when I import scipy.integrate. I've filed a bug report [here]( Any one here able to import scipy.integrate after installing per above on M1? I figured I'd ask here as a bunch of folks here probably already have the setup in place. If so, maybe hop over to [that issue]( and leave a note. Thanks! segfault here too.",2021-01-07T23:47:27Z,joy
756460058,"Well, I added such a comment to the private methods and the internal fromspec (in the header). Also squashed and the doctest flag switched (which means codecov will probably be useless now).",2021-01-08T00:01:08Z,anger
756485337,I completely agree. Sayed and the NumPy core developers have done an amazing job! Do you want me to close the issue? I have been deferring to the NumPy developers.,2021-01-08T01:17:09Z,anger
756571176,Thanks . Agreed that the solution exceeded expectations. I will close it then.,2021-01-08T06:14:47Z,joy
756576062,"This is really low on our priority list. If you wish to move forward with it, please relate to the [last comment on the issue]( We would like to suggest that contributors show the output from the error. This will require you think about the error: is the code path actually hit? Does it make sense to chain the exception?",2021-01-08T06:28:41Z,anger
756648188,"Awesome, thanks everyone for the excellent work!",2021-01-08T09:24:54Z,joy
756688023,"I'm talking about def stack(arrays, axis=0, out=None): the definition is not consistent with other numpy functions like concatenate. It took me a few minutes to realise the use was the same. not a big deal of course, if I looked at the examples I would have realised it, but I think it would be better if it were consistent over all functions",2021-01-08T10:46:41Z,sadness
756689367,What are you suggesting that line should be instead?,2021-01-08T10:49:56Z,neutral
756695977,"Sorry, but we have to use legal python syntax: pycon def stack((a1, a2, ...), axis=0, out=None): pass File ""<ipythoninput62524e6e6c5d59"", line 1 def stack((a1, a2, ...), axis=0, out=None): pass ^ SyntaxError: invalid syntax ",2021-01-08T11:04:30Z,sadness
756711449,Then the behavior is as expected. It is impossible for your original example to produce a correct result because the correct result cannot be stored in the return type.,2021-01-08T11:40:35Z,neutral
756768319,(also note OpenBLAS has released version 0.3.13 ...),2021-01-08T13:57:41Z,neutral
756797245,"There was only one significant comment, on the mailing list from . I addressed that here with the same answer I gave on the mailing list. I'd like to merge this PR when CI is happy and then follow up on the mailing list to propose this NEP moves from Draft to Accepted.",2021-01-08T14:55:07Z,joy
756813378,"Oh man, should have just run the tests even for a comment change I guess...",2021-01-08T15:23:18Z,sadness
756816696,"The concatenate docstring is a bit weird, because its signature line comes from C and is just a string, but actually does include the invalid syntax. I admit it is probably nicer to read, so I am not sure we have to change it.",2021-01-08T15:29:17Z,neutral
756823948,"Apologies, this is a duplicate issue ",2021-01-08T15:42:04Z,sadness
756846438,"The problem has been fixed in Homebrew, so with the latest python installed a simple pip install numpy works.",2021-01-08T16:16:46Z,neutral
756912296,"Thanks, but I am going to close, this. I could imagine adding a link to the NumPy website at but I don't think we need an installation section in the readme probably. I expect those actually interested in a source install will find the docs from there or by searching?",2021-01-08T18:05:06Z,joy
756931917," The wheel is for intel chips. It doesn't work for M1 chips. I couldn't find a whl for M1 chips (arm64). Didn't solve the issue. I just updated brew, brew install python and pip3 install numpy didn't work. Big Sur 11.0.1 Apple M1 chip python 3.9.1 pip3 20.3.3 ",2021-01-08T18:45:56Z,fear
756936573,"You are right, the problem was only solved for x86.",2021-01-08T18:56:15Z,neutral
757064324,"Don't worry about it, I would be extremely surprised if it is measurable in this place. We could add something like that to the benchmarks (I don't think we have a ""many sequences"" one), but really no need. ",2021-01-09T00:43:32Z,neutral
757073968,"Ok, I will go ahead and just remove the test for now then",2021-01-09T01:38:35Z,neutral
757099973,Thanks! wieser I'll give a thought about it.,2021-01-09T05:30:36Z,joy
757106135,Thank you for everything <3,2021-01-09T06:41:38Z,joy
757109839, please be aware that __cpu_features__ affected by the environment variable NPY_DISABLE_CPU_FEATURES which allows the user to disable certain features in runtime.,2021-01-09T07:21:16Z,fear
757119822,"Thank you for your report. This bug has been reported by , and already resolved by as mentioned.",2021-01-09T09:04:06Z,joy
757123004,I had the same issue. Had to Reinstall NumPy version 1.19.4 manually to fix the error.,2021-01-09T09:33:46Z,sadness
757131548,"You are using pip version 19.0.3, however version 20.3.3 is available. You should consider upgrading via the 'pip install upgrade pip' command.",2021-01-09T10:52:05Z,neutral
757137765,"This seems to be caused by Generator.shuffle() calling ndim() in order to support the axis parameter: It seems like this step can be skipped when axis = 0. Not only because of this warning, but also because of the overhead of asarray.",2021-01-09T11:47:10Z,fear
757199994,"OpenBLAS is more memory greedy by default. The short history of this difficulty is: 1.19.3 uses newer OpenBLAS to work around fmod problem, uses more memory by default 1.19.4 uses older OpenBLAS, has fmod problem 1.19.5 uses even newer OpenBLAS to work around fmod problem, tries to use less memory These memory problems seem docker specific, I expect the environment isn't providing accurate information about resources. The best solution going forward may be to using OPENBLAS_NUM_THREADS to limit the number of threads. frbg Here we go again.",2021-01-09T13:39:52Z,neutral
757220992,"Hm, I thought numpy had settled for building OpenBLAS with BUFFERSIZE=22 to keep the ""old"" memory footprint with 32MB GEMM buffers (at the expense of risking segfaults with large matrix sizes) ?",2021-01-09T14:01:53Z,anger
757243417,"frbg It is actually BUFFERSIZE=20. This may be a different problem, but it looks the same at first glance.",2021-01-09T14:28:37Z,neutral
757331040,"Since the function is documented to support sequences, I agree that this was an oversight. Technically a regression in 1.18 (the API was new in 1.17 though).",2021-01-09T16:29:07Z,joy
757335661," while sharing worries about duplication, your solution does seem substantially nicer than using jinja2 we wrapped a C library using ufuncs and the resulting jinja [template file]( is not pretty. Much of the work also ends up being done in python anyway, as we base the template on c function comments. That said, wieser is also right to warn about problems for newcomers I never really understood what is happening in the .c.src files...",2021-01-09T17:02:25Z,fear
757350003,Thanks,2021-01-09T18:45:51Z,neutral
757351184,what is the lower bound of the softlimit for 1.19.3 (and if it is not too much trouble 1.19.2)?,2021-01-09T18:55:00Z,fear
757351473,Thanks,2021-01-09T18:57:24Z,neutral
757354641," In Python 3, ""int"" is an arbitrary precision type Yes but numpy still uses ""int"" to represent the C type ""long""? np.dtype(int) is the C long type but np.array(262) will always pick np.int64 which is sometimes ""long long"". ",2021-01-09T19:23:40Z,fear
757365054,"Don't worry about the warning. No issue to fix in NumPy, Matti already gave the correct workaround, so I'll close this now. ",2021-01-09T20:48:27Z,neutral
757367743,"Thanks , we hope to do so when it's feasible (in addition to thin arm64 wheels, which seem preferable longterm). One blocker is that we build all our wheels on CI services, and none of them support MacOS ARM64 yet. And none of us have the hardware at the moment. We also still have some issues, in particular this build issue: gh17807. And this performance issue: gh17989. I'll link here as probably the most relevant issue for universal2 wheel building.",2021-01-09T21:10:57Z,joy
757368305,This should be picking up a 1.19.x wheel though maybe pip too old?,2021-01-09T21:15:03Z,neutral
757377383,wieser has there been any progress with this?,2021-01-09T22:32:48Z,neutral
757393322," Do you have any specifics here? No :) I'm just spitballing. Someone who knows more about how OpenBLAS preallocates memory will need to address that. I find it curious that the memory usage didn't change between 1.19.3 and 1.19.5, my understanding is that it should have gone down, although I suppose that it is possible that the number of threads allocated increased in 1.19.5.",2021-01-10T01:02:53Z,anger
757395097,I'm running into this on an NVIDIA Jetson (aarch64) took me a while to isolate it. Funny thing is that it fails in a virtualenv but seems to be working if you install it at the system level. The NVIDIA NGC containers from install numpy directly into the system libraries and they don't have this issue.,2021-01-10T01:19:53Z,fear
757396917,Does this need a backport?,2021-01-10T01:33:31Z,neutral
757397032,"I tagged it for backport, if it doesn't need it, please remove the tag.",2021-01-10T01:34:51Z,neutral
757406071,"Just curious about something since I've been following the work to add typing support for dtypes: won't making a mutable container type covariant cause issues? For the usual reasons that, e.g. if my_func accepts ndarray[Any, np.dtype[np.float64]) it could assign values of float64 to the array, but calling my_func with ndarray[Any, np.dtype[np.float32]) would no longer raise typecheck errors despite being wrong.",2021-01-10T03:15:59Z,anger
757415875,There's open issue already,2021-01-10T04:55:24Z,neutral
757433729,"Yes, I should have marked it for backport since otherwise compilation with visual studio 2019 fails.",2021-01-10T07:47:10Z,sadness
757447665,"Accelerate should be avoided for NumPy as it has bugs. NumPy doesn't use gfortran, SciPy does. But there currently no arm64darwin OpenBLAS build on I am not sure how we would build that without support for arm64darwin from a CI system.",2021-01-10T09:50:55Z,fear
757448101,I opened MacPython/openblaslibs about building OpenBLAS for Allple Apple M1 silicon.,2021-01-10T09:54:07Z,joy
757451297," I am not sure how we would build that without support for arm64darwin from a CI system. For building you only need a CI system that provides macOS 11 hosts, or with some effort macOS 10.15 hosts with Xcode 12.2. For testing you actually need M1 systems in the CI system, which will likely be a blocker for you and could take some time (I have no idea how constrained the supply of M1 systems is, but I expect that a CI provider like Azure Pipelines will require a lot of M1 systems). ",2021-01-10T10:20:56Z,neutral
757451600,Accelerate should be avoided for NumPy as it has bugs. Have you (the numpy project) files bugs about this in Apple's tracker? Or are these bugs in numpy's use of Accelerate?,2021-01-10T10:22:54Z,fear
757455123,"I feel your pain, I'm hearing similar experiences from app developers and most issues I've filed with them have gone completely unanswered :( ",2021-01-10T10:48:56Z,sadness
757459007,Could you add the actual module path as well (Cython.__file__ or so)?,2021-01-10T11:19:24Z,neutral
757465313,"Nope, OpenBLAS does not currently do resource detection beyond counting the number of available cores.Is that number constant for all entries in your table of minimum softrlimits above ? (In a way it would make me happy if the problem turned out to stem from something entirely different than the GEMM buffer, but I do not remember anybody creating huge sinkholes elsewhere in the code)",2021-01-10T12:05:48Z,joy
757476401,"k13, I gdb'd into the code, specifically FLOAT_divide, that in turn goes into loops_arithm_fp.dispatch.c.src Nothing to do with loops_arithm_fp.dispatch.c.src, FLOAT_divide only performs true divide. However, this issue has been fixed by since if nonzero divided by zero should return inf. ",2021-01-10T13:28:31Z,fear
757488155,"While the error comes from a potentially unexpected place on linux, I can't reproduce the crash. Just to be sure, you are not running on a apple silicon M1?",2021-01-10T14:45:24Z,fear
757490636,"Nevermind, as the traceback reads, its a stack issue so just can be hard to reproduce.",2021-01-10T15:01:46Z,sadness
757512877,see the linked issues this casts python objects to strings which is a huge breaking change,2021-01-10T17:30:59Z,fear
757515028,"Hmmm, thanks for the note. I can't reproduce it immediately, so let me dig in. It can't really be this PR, more likely and an error in gh18115 or even gh13578",2021-01-10T17:46:27Z,neutral
757542575,"Thanks! Increasing the number makes it reproducible easily (anything between 32 and 64 probably is technically broken), I already found the underlying issue and know the fix. I will fix it in the next few days, unless someone has interest in digging into it. (It is likely that this was unreachable code for a good time – although even then it has been around for many years.)",2021-01-10T20:54:41Z,joy
757554152,Thanks .,2021-01-10T22:20:56Z,neutral
757667501,"I could find another solution, which does not require numpy 1.19.5. I am therefore closing this issue. Thank you for your help.",2021-01-11T07:11:10Z,sadness
757708833,s that number constant for all entries in your table of minimum softrlimits above ? Yes the only difference between the runs are the installed version of numpy,2021-01-11T08:27:36Z,neutral
757826959," Could you add the actual module path as well (Cython.__file__ or so)? Good idea, done.",2021-01-11T09:51:15Z,joy
757853777," Perhaps we can retire that portion of the governance document and have it be superseded by this NEP, and then have this NEP specify more clearly what defines those relationships and how those relationships work. Agreed. There's no compelling reason for that to be in the governance document itself. I'll take that along in my planned update.",2021-01-11T10:17:32Z,anger
757857653," Perhaps a sentence after the list of dollar amounts that forward reference the discussions on institutional partners. agreed that would help, done",2021-01-11T10:24:36Z,anger
757863466,Thanks,2021-01-11T10:35:44Z,neutral
757984190,"Hi , sorry for the long pause in the PR. I noticed the following TC causing crash in specific environment. I think handling return value for self_descr for overflow should take care of it. Now to the overflow: Is it defined behaviour to do np.uint(1)?",2021-01-11T14:25:05Z,sadness
757997220," Which is not something we can easily do but if we're going to expose an alias in numpy.typing as public API, why not make it the thing people should be using for new code when working with integers? If the plan is to expose them then I'd definitely agree with you. The current aliases are IMO too useful to remove in their entirety, so instead I'd propose to just give them a more apropiate. Perhaps something along the lines of _IntLike_co and _FloatLike_co (the same holds for [_ArrayLike<X](",2021-01-11T14:46:00Z,anger
758002854," There is the [numpy.typing]( docs page, though nothing specifically for ndarray[Shape, DType] yet, as this PR was just aimed at making ndarray a parametrized generic. I'm planning on exposing a runtimesubscriptable version of np.ndarray soon, something which I'd like to accompany with a documentation update.",2021-01-11T14:55:06Z,anger
758008127," Is it defined behaviour to do np.uint(1)? I think so, although it may not be formalized. C allows it, but maybe we should make an explicit choice.",2021-01-11T15:03:39Z,fear
758025507,"Lets say that it was once explicitly implemented that uint(1) will work, so for now I will say yes. For comparisons it would be best if this can't happen (the only problematic thing is probably uint64 == int64 using float and losing precision though). It is something that we could think about changing, but I am not sure it is worth the trouble and how annoying the work around is (since using 1 for maxint is not uncommon, even if technicall undefined by the C standard.)",2021-01-11T15:31:22Z,fear
758032496,_co sounds good to me.,2021-01-11T15:42:53Z,joy
758149393," oversight was able to split slip past the tests Yep, I thought to delay merging, but the tests passed and I was rushing.",2021-01-11T18:48:52Z,sadness
758153829,"Luckily our ""extended"" test suits in pandas and astropy both caught it easily ;)",2021-01-11T18:56:20Z,fear
758171894,"It is happening for me, using python 3.9 in a completely clean new virtualenv. It seems, as mentioned, it is because the change from tp_print to tp_vectorcall_offset. I cannot install numpy. I am using cython version 0.29.21.",2021-01-11T19:28:42Z,neutral
758175656,"I am still confused by this. Are you installing from source? In that case, maybe try a git clean xdf (make sure to have no code changes applied or custom site.cfg). Also just to be sure python3.9 mcython V (assuming the python executable you are using is python3.9) is the right version and the pip you are using is right, by using python3.9 mpip instead of pip directly, just in case. Maybe you had a solution? (I am not quite sure why removing ""snoflakeconnector"" would make a difference)",2021-01-11T19:35:39Z,anger
758215303,"Hmmm, test failure in: out = subprocess.check_output(['nm.exe', 'help']) because nm was not found, restarted the test probably flaky, just noting in case it happens more often.",2021-01-11T20:51:18Z,fear
758222735,I have been seeing the nm.exe failure about once a week or so.,2021-01-11T21:05:21Z,sadness
758241818,"Looks safe and good to me, lets put this in. Thanks. (I suppose another pass is that _filling_ the result array can be slow. But, I guess that will usually not do much more than double the time it already took, so it is less likely to ""trap"" someone in an interactive session.)",2021-01-11T21:38:52Z,joy
758245271,"I am sorry, I was trying to install a previous version of numpy. Current version installs ok with python 3.9. It is ok now.",2021-01-11T21:45:48Z,sadness
758247164,"Thanks for the followup, will close the issue then.",2021-01-11T21:49:47Z,joy
758258944,Thanks Sebastian.,2021-01-11T22:15:29Z,neutral
758262811,"Thank you, I'm really happy to help, if however small! Perhaps I can play around with the implementation and propose a more effecient solution in the future (if possible.) For example, perhaps it would be more practical to check for signals every max_dims/2 times. ",2021-01-11T22:23:15Z,joy
758265456,"Yeah, there may be a better place to check. I was a bit hesitant to just putting it at the entry of the function. Frankly, it is probably so lowoverhead, that would be OK as well, it just didn't seem high priority to check often.",2021-01-11T22:28:52Z,fear
758451927,"I only know basics of Python, but can offer some testing under Yocto/OpenEmbedded if it would help.",2021-01-12T07:02:12Z,fear
758453296,"I'm really sorry, but my scheduled got filled up so this set of PRs still has some loose ends. Might be able to do it later this month, I'm still interested in the topic.",2021-01-12T07:05:15Z,sadness
758454540,"We've all got our lifes, no problem. I found this series after 12+ hours fighting with SciPy and Yocto and just couldn't pass by. It's great that you haven't abandoned it. As an additional question any chance of backporting this to 1.17? That's what's included in Yocto's current LTS and would make life easier for people who do custom work adding SciPy on top of it. I'm asking more in the sense of viability, not anyone's timelines and availability.",2021-01-12T07:08:13Z,joy
758490399,anybody have an eta on the 1.20.0 release? We will make the release decision Jan 6. One of the reasons for the slight delay was to allow some downstream projects to work around an expired deprecation. Has the decision been made regarding the 1.20.0 release?,2021-01-12T08:22:17Z,anger
758497328,"Hmm bit late, not sure if a piecewise function is intended to return more output than input. Could this be done with just pandas by indexing based on your condition and then using the apply function. df[condition].apply(function) or using .loc, .iloc",2021-01-12T08:35:36Z,fear
758499362," As an additional question any chance of backporting this to 1.17? Numpy only maintains two versions at a time so the only versions this patch could reach are 1.19 onwards, and it's likely that this won't make 1.19 at all and will end up waiting till 1.20 or 1.21.",2021-01-12T08:39:47Z,fear
758552626,"As they say, apply is a convenience function & not the one to go for, if performance is important.",2021-01-12T10:12:44Z,anger
758599066,so I was able to sync the fork but now another problem has appeared. I cannot push my local repo to origin. I've tried git push origin master and git push u origin master. I have also commited the branch,2021-01-12T11:38:02Z,fear
758609525,Im using the binary version of VSC,2021-01-12T11:58:23Z,joy
758622228,You cannot push to origin. You need to fork this repo (look at the top right corner of this page) push your branch to your repo issue a pull request,2021-01-12T12:23:58Z,anger
758634557,"I believe the failures are unrelated, but I'm not sure.",2021-01-12T12:49:44Z,neutral
758651930,"It looks like this can be closed if the newest polynomial documentation is still not sufficient, feel free to reopen.",2021-01-12T13:23:24Z,joy
758661236,"Pyarrow release or end of month, whichever comes first.",2021-01-12T13:40:29Z,joy
758700164,Is there anything left to do in this issue or can it be closed?,2021-01-12T14:40:03Z,sadness
758705632,"Unless we have concrete suggestions for improvements to the current documentation, I'm assuming this issue is no longer relevant. Feel free to reopen.",2021-01-12T14:48:42Z,neutral
758707183,Closing this as it does seem to be fixed in the [current documentation](,2021-01-12T14:51:09Z,fear
758851445," Thanks for the fast reply and all the great links. I am very happy to see that there is actually some interest in this idea. I really like the idea of a ""transform"" hock/function, that would for sure address my needs. I suspect it indeed might be hard to argue for this though as direct use cases seem limited (I really only can think of the histogram one right now). I am happy to do a bit more research though and help work this out if there is something I can do with my ""limited"" knowledge of the ufunc internals.",2021-01-12T18:28:39Z,joy
759059107,The generated documentation:,2021-01-12T21:52:27Z,neutral
759060189,"As a side note: Let's wait with merging until is merged, as it would allow for the removal of an overload.",2021-01-12T21:53:36Z,anger
759088579,"I can see this being nice to have, so I am not opposed. But I am a bit surprised that we want to make nested sequences a common thing for most command? I.e. I would expect it to be mainly a valid input for np.array and friends, and pretty much all other occurrences would require you to call np.asarray manually to type correctly?",2021-01-12T22:53:40Z,neutral
759243814,trail how did you install sudo aptget install libatlasbasedev in lambda function?,2021-01-13T06:47:00Z,neutral
759329415,I worked after I installed the latest version numpy in PyCharm. I was using sympy.plot glad it worked.,2021-01-13T09:40:06Z,joy
759341379,"Thanks. We know, working on it in ",2021-01-13T10:01:12Z,joy
759344126,Oh sorry for the duplicate then,2021-01-13T10:05:43Z,sadness
759344715,"no worries, I'll leave this open till it's fixed, or someone else will open a new issue",2021-01-13T10:06:48Z,fear
759345516,"For who needs the docs, they're still accessible here: ",2021-01-13T10:08:12Z,neutral
759371165,"The new page "" "" returns 404! This helps a lot!",2021-01-13T10:55:15Z,joy
759389439,will the universal2 python.org installer pick up thin arm64 wheels if they are available and Python is started in native mode?,2021-01-13T11:31:38Z,fear
759403422,"That's more a pip question than python version, but yes pip will use arm64 wheels when running in natively on M1 Macs. ",2021-01-13T12:00:26Z,neutral
759405300," sorry just checking because I was surprised. You're saying that if I do: python m pip install numpy on an M1 machine, using the Python.org universal2 Python, and PyPI only has an arm64 wheel for Numpy, it will neverthless install that wheel, even though it does not satisfy the x86_64 part of universal2?",2021-01-13T12:04:28Z,fear
759431066," ouch! and thanks for working that through, that's very helpful.",2021-01-13T12:56:04Z,joy
759433848,"Interesting. I'm not sure that's a bad thing. There are scientific packages that already exceed the PyPI version limit, so a doubling of wheel size for what (for scientific libraries) is a very niche use case doesn't seem very sustainable. I didn't have the energy to jump in that packaging discussion, but I suspect scientific libs may prefer thin wheels. We work hard on keeping binary sizes reasonable.",2021-01-13T13:01:27Z,fear
759434296,Should be working now. Please reopen if something is still not working.,2021-01-13T13:02:20Z,neutral
759466278,"Yeah, I noticed that some some packages are very large. Those can always choose to not provide universal2 wheels at all, that will work for most users. At a risk of going completely offtopic, I'd like to have more control on what gets installed than pip is currently giving. When I'm doing something on my local machine only I might prefer a thin wheel (smaller, faster install), but when I intend to redistribute I might prefer a universal wheel (and possibly even one that's synthesised from two thin ones on PyPI). ",2021-01-13T13:59:31Z,neutral
759473439,"More control would be nice indeed, if universal2 stays perhaps as both a pip command line argument and .rcfile setting. I'd actually prefer if it didn't stay, because also for redistribution there isn't much reason to do it as universal2. The only good argument I saw was ""it's less effort to introduce"". There is no other situation where we mix OS and hardware platform support. The old universal PowerPC/Intel thing was also very annoying, and in my experience didn't work for the scientific stack anyway if you invoked it with the nonnative arch.",2021-01-13T14:11:03Z,anger
759475884,"Just superficially, wouldn't it be more simple for Python.org to provide separate M1 and x86_64 installers as it does for Windows 32 and 64 bit? It's hard to imagine many people using the universal2 Python.org Python and always / mostly doing: arch x86_64 python3 And, as Ralf says, I bet that will usually break, when you get to install packages.",2021-01-13T14:15:11Z,fear
759511015,"I don't think it's much to ask of a user that they know they have an M1 Mac, honestly. And for now, it would be reasonable to make the x86_64 installer the default, so they have to specifically ask for the M1 build. I presume an x86_64 build will also work on M1 via Rosetta? What happens for: arch x86_64 python3 m pip install numpy Does this look for an x86_64 wheel before a universal2 wheel?",2021-01-13T15:10:16Z,anger
759511567,"I don't understand what problem universal2 pip packages solve. If the package is purepython, pip will download that. If the package has cextension modules, pip should choose the proper hardware model.",2021-01-13T15:11:09Z,neutral
759527181," Universal support is more or less required for folks distributing application bundles, having separate application downloads for intel and arm is just not what Mac users expect. Universal support is not about being able to run x86_64 on arm, but about having a single binary that just works everywhere. That's specifically a general ""Mac end user"" problem though, and is unrelated to PyPI and wheels. Having thin wheels plus the right py2app tooling to glue two thin wheels together in a single .pkg/.dmg installer would be much better. ",2021-01-13T15:34:49Z,fear
759527981," I expect that it will be years before we can ignore intel Macs, even after Apple transitions all their hardware to arm64. Agreed, at least 67 years I'd think.",2021-01-13T15:36:01Z,neutral
759529087,"The shape seems clearly a bug. nanpercentile and percentile should ideally return the same thing as well. I don't have much of an opinion on error vs NaN. (Percentile does seem a bit less clear than mean, which naturally goes to 0/0, but especially with axis, a NaN result may be useful).",2021-01-13T15:37:46Z,fear
759529893,There was a brief outage of most of the website.,2021-01-13T15:38:59Z,joy
759701852,What hardware?,2021-01-13T19:52:19Z,neutral
759707302,"No :0 M1 is not supported, we are working on it but we also need hardware testing support that we do not yet have. There are other issues open for this.",2021-01-13T20:02:29Z,neutral
759708936,"Ah, thanks for the info!",2021-01-13T20:05:40Z,joy
759752952,"LGTM, thanks .",2021-01-13T21:33:16Z,joy
759753459,"In general it would be better to also use PyMem_Malloc and friends here instead, but isn't really important.",2021-01-13T21:34:26Z,sadness
759774279,Thank you!,2021-01-13T22:18:56Z,joy
759807545,I'm not convinced by this change isn't it sufficient to mark them const?,2021-01-13T23:00:50Z,fear
759814756,"Sadly, marking them const produces compiler warnings because PyArg_ParseTupleAndKeywords takes a nonconst array even though it doesn't actually modify the array. git grep F 'char kwlist[]' shows that most of these arrays were already marked static in NumPy, and none are marked const. I'm just proposing that we be consistent and add static to the ones that didn't have it already.",2021-01-13T23:05:58Z,sadness
759815730,"I just concluded the same thing. The approach seems odd, but it's even how CPython itself does it. I'll maybe play around with godbolt to see if const is ultimately better, but at the end of the day consistency matters more than microoptimization here.",2021-01-13T23:08:42Z,sadness
759816667," takes a nonconst array even though it doesn't actually modify the array As far as I remember this is due to a shortcoming in C (fixed in C++), where a T const const argument cannot legally be passed a T.",2021-01-13T23:11:17Z,fear
759844651,"Hmm, shouldn't valgrind notice this type of thing? I am wondering if we want to add a test for it, because I do not recall valgrind ever complaining about it (I run it semirgularly). OTOH, the type_tuple_type_resolver is a bit obscure and not used much (it is the dtype= argument and especially signature= to ufuncs). Maybe just going to merge this as is soon. Just curious are these PRs based on static code analysis?",2021-01-14T00:30:04Z,neutral
759853414,I made this pull request and because of [scanbuild]( warnings. was just something I noticed while looking through the code.,2021-01-14T00:54:58Z,anger
760085809,"There is a timeout. Toggling timestamps in the build/test log, it seems 20 minutes elapse between [this line]( and [this line](",2021-01-14T09:51:57Z,neutral
760170589,"Late reply, but thanks a lot for the quick followup and nice deprecation! Justed tested this, and this should work for us. There is still a difference compared to < 1.20: now __array_interface__ is accessed (and so a return value constructed), while before it was never even accessed at all. But that shouldn't be a problem for us (I only noticed it because for a mock in a test the property did raise NotImplementedError).",2021-01-14T12:39:03Z,joy
760219383,What are the next steps? Can I do anything to push this forward? I do not want this to become a stale PR.,2021-01-14T14:09:01Z,neutral
760252396,"In one of the tests added in there is a note: NOTE: We actually do still call __array__, etc. but ignore the result in the end. For dtype=object we could optimize that away. So this doesn't seem to be true in practice? If the result would actually be ignored (or the error catched and ignored), then the issue would be fixed I think? ",2021-01-14T15:02:06Z,anger
760255872,Thanks .,2021-01-14T15:07:07Z,neutral
760326857,"Seems like a fixed pandas bug, the line isinstance(np.bool_, type(np.dtype)) did change behaviour, but doesn't make much sense to begin with (if anything, it should be isinstance(np.bool, np.generic)). Unfortunately, the only solution is probably to upgrade pandas as well (I only checked that 1.0 doesn't have the fix yet).",2021-01-14T16:59:02Z,fear
760330209,"Right, so I was using incompatible version and can fix it by downgrading (which I did) or properly upgrading (which I will do). In the future, I suppose I should check pandas before posting a bug report. Thanks! Closing.",2021-01-14T17:04:21Z,neutral
760334764,"Yeah, frankly, it would be nicer if a year old pandas would work without issues. But yeah, generally it is better if the base package (NumPy) is not (too much) newer than the downstream (pandas). For NumPy the usual safe bet is that 1 year newer should be fine (i.e. you may see warnings but everything works), this is an unfortunate exception.",2021-01-14T17:11:39Z,neutral
760372847,"I am not too surprised that we still have such errors in NumPy. I couldn't reproduce it immediately using the debug version shipped with debian (maybe it doesn't have full asserts). If you can do it very quickly, does 2 == np.bool_ and/or np.array([2]) == np.bool_ have the same problem?",2021-01-14T18:14:13Z,fear
760384505,"Oh, I didn't fully parse your last comment. So I guess raising the error may be fine for shapely/you if we ensure there is a special case for object + last dimension reached (which is not relevant except for the assignment case). That special case is probably missing in the dtype discovery code right now. (Note: that would not help with np.array(..., dtype=object), however)",2021-01-14T18:25:25Z,anger
760389943,"Thanks, great debugging/tracking down! That explains where the error originates, I was thinking of the wrong place first and couldn't make sense of it. The issue is here: although it is not quite clear of GetPriority might suppress the error (I don't like error supression for no reason, but here it may make sense, unless we first ensure that we look up the priority only on instances.)",2021-01-14T18:35:23Z,neutral
760413171,"Thanks, I will just merge this. It doesn't seem to fix an actual bug, but is correct. Whether or not this causes undefined behaviour seems to not make a difference in this function (because it is used solely for error reporting, and that branch doesn't do nice error reporting). (This function is a mess, but I hope the solution is to just delete all of it in the next months.)",2021-01-14T19:19:57Z,joy
760494361,"Hi , thanks for your PR. In order to get the right people to review it, it's better to be more explicit in the title and description. Similarly, your commit message [should be descriptive]( in order to add context to this fix. ",2021-01-14T21:42:07Z,fear
760507699,"If this is still relevant, I'm wondering if the best place to add this discussion on the @ operator is [this page on linear algebra]( Note that this page is actually a mix of common linear algebra operations and functions in the np.linalg module maybe they should be split up? Finally, the [ndarray]( documentation says: Matrix operators @ and @= were introduced in Python 3.5 following PEP465. NumPy 1.10.0 has a preliminary implementation of @ for testing purposes. Further documentation can be found in the matmul documentation. Can we lose the preliminary/testing purposes warning?",2021-01-14T22:10:10Z,fear
760525444,"Is this something we still want to do? I'm guessing since this hasn't been brought up in a while that we don't need the manpage, and this issue can be closed. ",2021-01-14T22:47:58Z,sadness
760572480,"i would still like to have a manpage associated with the tool, at least that's what we prefer in Debian to have (but it's definitely not a strong requirement)",2021-01-15T00:53:30Z,neutral
760666156,same problem to me while trying installing pandas under macos big sur 11.1 python is 3.8 and pip is 20.3.3,2021-01-15T06:00:25Z,sadness
760889970,"oopsie daisy, silly me",2021-01-15T11:34:32Z,joy
761061078,"Never mind turns out there were, unfortunately, detrimental problems with the therein referenced PR",2021-01-15T17:00:51Z,fear
761099971,Pinging and reich. Continuing from it turns out that the np.number variancy could be changed without issue.,2021-01-15T18:12:32Z,anger
761101862,"This is expected. p=None triggers a (faster, more accurate) codepath for specifically for unweighted sampling. When you give it an explicit p, we use a different algorithm for weighted sampling. There is not a good way to test the explicit p that it is equivalent to being unweighted because of floating point inaccuracies. I can understand the desire that these two give the same answers, but I don't see a good way to reconcile the two except to abandon the faster and more accurate algorithm for unweighted sampling, which I don't think would be a good idea.",2021-01-15T18:16:00Z,neutral
761108997,"Thank you for your detailed explanation, I totally understand and agree with you! I assumed that they are equivalent and so this caused a bit of a headache when I was writing unit tests. Would it be possible to highlight this behaviour somehow in the documentation? ",2021-01-15T18:29:16Z,anger
761118952,"This is a duplicate of , seems to be due to the compiler rearranging code.",2021-01-15T18:49:07Z,fear
761243857,needs a rebase.,2021-01-15T23:06:57Z,neutral
761245101,needs rebase.,2021-01-15T23:10:36Z,neutral
761284488,Thanks Bas.,2021-01-16T01:24:34Z,neutral
761297213," Deprecating sctypeDict would be useful too, but I'd rather we first clean up the documentation and remove usage in SciPy first, give a heads up to others, and do it in 1.22 or 1.23. do you feel it would be worthwhile to add a comment in the release note discouraging the use of sctypeDict?",2021-01-16T02:32:51Z,neutral
761543622,"A release note sounds like a good idea, as context with this PR. I'm not sure many people read it though; adding that note to is probably more effective longterm.",2021-01-16T10:54:30Z,joy
761594949,Is this the beginning of more extensive work? It seems so far you have renamed two files. The issue spoke about There needs to be a clear distinction between testing the ufunc object (test_ufunc) and the ufuncs instances (test_umath). Currently the two types of tests are mixed between the files. The testing of the various ufuncs is far from complete.,2021-01-16T16:46:50Z,neutral
761614247,I get a segfault from _mac_os_check and I'm not even on ARM. This whole macos/brew/compiler/python3 combination keeps biting me everytime whenever I update something.,2021-01-16T18:45:14Z,anger
761661052,"k13, I created a new pr that adds fast integer division intrinsics for all SIMD extensions, it should be merged before this pr.",2021-01-16T21:12:01Z,neutral
761696583,"The QR decomposition is not unique all the way down to the signs. One can flip signs in Q as long as you flip the corresponding signs in R. Some implementations enforce positive diagonals in R, but this is just a convention. Since we defer to LAPACK for these linear algebra operations, we follow its conventions, which do not enforce such a requirement.",2021-01-16T23:22:12Z,fear
761709149,"So this is extreme bikeshedding now that the float issue has been discussed, but would you consider changing floydselect into floydrivestselect? Or perhaps just floydrivest or frselect?",2021-01-17T01:08:51Z,neutral
761737340,"Thanks, , I'll rebase once that's merged and add the dispatches  ",2021-01-17T05:31:04Z,joy
761746590,"I think this should have an attribution, something like Based on the VCL library, which is (c) Copyright 20122020 Agner Fog and licensed under the Apache License version 2.0. is that correct?",2021-01-17T07:17:32Z,neutral
761772578," I think this should have an attribution, something like Based on the VCL library, which is (c) Copyright 20122020 Agner Fog and licensed under the Apache License version 2.0. is that correct? No that sounds wrong. Reminder, we don't do Apache2, see for example: We could change that at some point, but it's a significant change and needs a strong motivation and decision on the mailing list. VCL is a little hard to find, so here is a link: If this PR took code from there, that seems like a blocker for accepting it.",2021-01-17T11:07:07Z,fear
761772753,"Now read the PR description, the whole PR is based on that, and it seems valuable. It may be worth considering, especially if there are no good alternatives. We'd be deciding to give up on GPLv2 compatibility.",2021-01-17T11:08:51Z,joy
761779667," Hmmm... Its a little vague, not sure what im supposed to do",2021-01-17T11:24:50Z,fear
761802147,"According to the above mention, the NumPy 1.19.5 release uses a workaround for the Windows 2004 bug, instead of waiting for Microsoft to fix it. NumPy 1.19.5 is a short bugfix release. Apart from fixing several bugs, the main improvement is the update to OpenBLAS 0.3.13 that works around the windows 2004 bug while not breaking execution on other platforms. This release supports Python 3.63.9 and is planned to be the last release in the 1.19.x cycle.",2021-01-17T12:12:13Z,fear
761812595,", we can ask for relicensing, since we're not taking the same exact code plus the original code itself is based on T. Granlund and P. L. Montgomery work.",2021-01-17T13:26:33Z,anger
761814845,", Why do we still support Darwin/PowerPC? this flag faltivec should be removed.",2021-01-17T13:44:07Z,anger
761819448,", Much of the work also ends up being done in python anyway That's the whole idea behind this solution. That said, wieser is also right to warn about problems for newcomers I see it as more friendly for newcomers if you compare it with web template engines, It still python after all. ",2021-01-17T14:15:56Z,fear
761836146,"I don't know about you but I find I forget how to use Jinja2 every time, and have to remind myself. ",2021-01-17T16:08:15Z,sadness
761836513," I don't know about you but I find I forget how to use Jinja2 every time, and have to remind myself. I've had that experience a couple times, but at least it's very easy to remind yourself from the documentation and stackoverflow questions.",2021-01-17T16:10:45Z,joy
761866660,I am on Ubuntu 18.04.,2021-01-17T19:31:51Z,neutral
761871065,"I thought one or more of our CI builds use ubuntu 18.04, so how do tests pass?",2021-01-17T20:02:48Z,neutral
761873215," I thought one or more of our CI builds use ubuntu 18.04, so how do tests pass? Can you point to a CI build using ubuntu 18.04 that runs f2py tests?",2021-01-17T20:19:22Z,neutral
761878808," Nice! Looks like having number invariant is not too bad after all, if that was the only substantial change. I thought e.g. arithmetic operators might need to get messier, but this version is just as clean (mixedprecision ops returned Unions before anyway so it's just a different Union EDIT: whoops I forgot that with covariance mypy simplifies the Union to just one dtype sorry). If this gets merged, then having ndarray covariant in dtype makes more sense. (I guess users could still do incorrect assignments on arrays of _abstract_ dtype, but that's fine and will probably be rare). ",2021-01-17T20:57:24Z,joy
761880914,I'll review this PR. Please don't merge yet.,2021-01-17T21:12:41Z,neutral
761885851,"Just wanted to add to this old, prescient issue by that a ShapeError and BroadcastError would be really useful!",2021-01-17T21:47:54Z,anger
761889974,I think I got away with adding AxisError a few years ago I think a sufficiently motivated contributor could do the same for BroadcastError.,2021-01-17T22:17:17Z,sadness
762016884,"Most of the code was rewritten, while some of the code is very similar to Apache 2.0 based VSL, which is not compatible with current 3clause BSD License(Apache 2.0 is more restrictive), so we should either rewrite the similar code or ask Agner Fog modify the License.",2021-01-18T06:32:15Z,neutral
762056925,Thanks,2021-01-18T07:52:59Z,neutral
762058932,"I thought the f2py tests are part of the test suite. We should be running them on at least one of the builds. Since the tests pass without this PR, I guess they are not running. We go through the trouble of installing a gfortran compiler and libraries, so we should run the tests. Maybe as part of this PR you could add whatever is needed to make them run? ",2021-01-18T07:56:56Z,anger
762059827,Now I am paranoid that the tests are not running. Could you point to where this new test is run at least once in CI?,2021-01-18T07:58:34Z,anger
762069526,See for instance,2021-01-18T08:15:41Z,neutral
762077075,Thanks,2021-01-18T08:28:26Z,neutral
762084322,"The f2py tests are running. There are other differences between local and CI ubuntu boxes that may affect the build/test results: local: 18.04.4, CI: 18.04.5 I using conda environment to build and test numpy.f2py, CI is installing numpy to system gcc version local: 9.3.0, CI: 7.5.0",2021-01-18T08:40:10Z,fear
762184132,can you tell me what this means and what i'm supposed to do?: There needs to be a clear distinction between testing the ufunc object (test_ufunc) and the ufuncs instances (test_umath). Currently the two types of tests are mixed between the files. The testing of the various ufuncs is far from complete.,2021-01-18T11:21:43Z,neutral
762222800,please review.,2021-01-18T12:34:46Z,neutral
762223632,"you should compare new implements in tril_indices and triu_indices with nonzero, Can you write a benchmark in bench_core.py?",2021-01-18T12:36:09Z,neutral
762229792,"Any thoughts on this , ?",2021-01-18T12:48:10Z,sadness
762275267,Don't merge until is resolved.,2021-01-18T14:11:59Z,neutral
762277900,It's not clear from the pr title but am I right in thinking this is actually just working around a bug in the Intel compiler where the appropriate macro doesn't end up defined? Or are we seeing this problem with gcc alone?,2021-01-18T14:16:30Z,fear
762285857," It's not clear from the pr title but am I right in thinking this is actually just working around a bug in the Intel compiler where the appropriate macro doesn't end up defined? Or are we seeing this problem with gcc alone? The problem is with gcc (I don't have icc installed to test this atm). IIUC, gcc 9 is C17 (__STDC_VERSION__ == 201710) and it does not define __STDC_NO_THREADS__.",2021-01-18T14:30:16Z,fear
762287890,"My reading of the Intel thread is that glibc itself is responsible for providing __STDC_NO_THREADS__, and that it uses a special gcc hook to inject a header into all source files that does so. The Intel issue is that the hook doesn't exist in icc but if you're using gcc, that suggests that something else is going wrong with the hook.",2021-01-18T14:33:49Z,anger
762340980,As a side note: this PR has some minor (namerelated) merge conflicts with,2021-01-18T16:05:29Z,anger
762352718,"Hey , I tried multiple things to reproduce the issue locally, tried quay.io/pypa/manylinux2010_i686 and used a native 32bit system as well, but it's not crashing. Any idea why that particular TC is failing? I followed the memory through the code, no double free or invalid dereference either.",2021-01-18T16:26:40Z,anger
762354811,"Also not sure if I am setting quay.io/pypa/manylinux2010_i686 correctly, for instance, I don't see python being installed in the image, yet it's there. I was curious about how are the packages being installed in the image.",2021-01-18T16:30:46Z,neutral
762358131," Added tests to bench_core.py. I don't know why it crashes though, any ideas? This one seems related: ",2021-01-18T16:36:39Z,anger
762443236,"Maybe that overflow assert is simply incorrect? It does seem a bit off. Otherwise, my best idea is to install gdb in the CI and the job with that. Hopefully, getting a traceback will be sufficient. I did something for example [here (the command may be useful)]( (but for the travis CI).",2021-01-18T19:52:15Z,anger
762477911," Seems harmless, although the PR title should explain what the change actually is hello, actually the PR title explains what the change actually is I didn't make any big changes all I did is a very small change is __inti_.py I think if you will see the code you will get it. :)",2021-01-18T21:27:02Z,neutral
762478548,"actually, this was my first PR so sorry if I don't make sense",2021-01-18T21:28:58Z,sadness
762512665,Looks good to me. Maybe cov instead of co? The latter has no obvious meaning.,2021-01-18T23:29:40Z,joy
762525607,"LGTM, just a couple of style suggestions.",2021-01-19T00:19:26Z,joy
762529951,The NumPy 1.16.x series was the last to support Python 2.7 and most of the shims have been stripped out. Is there a problem using 1.16?,2021-01-19T00:35:43Z,fear
762533195," The NumPy 1.16.x series was the last to support Python 2.7 and most of the shims have been stripped out. Is there a problem using 1.16? Using 1.16 is what caused this issue. We then downgraded to 1.13.2, but the issue remains.",2021-01-19T00:47:08Z,fear
762545123,"The relevant code is PY_VERSION_HEX = 0x03000000 NUMPY_IMPORT_ARRAY_RETVAL NULL NUMPY_IMPORT_ARRAY_RETVAL import_array() {if (_import_array() < 0) {PyErr_Print(); PyErr_SetString(PyExc_ImportError, ""numpy.core.multiarray failed to import""); return NUMPY_IMPORT_ARRAY_RETVAL; } } import_array1(ret) {if (_import_array() < 0) {PyErr_Print(); PyErr_SetString(PyExc_ImportError, ""numpy.core.multiarray failed to import""); return ret; } } import_array2(msg, ret) {if (_import_array() < 0) {PyErr_Print(); PyErr_SetString(PyExc_ImportError, msg); return ret; } } Looks to me like NUMPY_IMPORT_ARRAY_RETVAL is not being correctly defined for Python 2.7 for some reason.",2021-01-19T01:31:46Z,fear
762547338,"Yes, this was my original thought, but putting a NULL after RETVAL for Python 2 does not resolve the issue. ",2021-01-19T01:40:56Z,fear
762552435,"It looks like it _is_ returning NULL, and it shouldn't be. It should just be return;, i.e., no return value.",2021-01-19T02:00:34Z,joy
762556129,"I don't think is related, can you add your crash logs here? From the CI perspective, The only failure is TestMatmul.test_vector_matrix_values.",2021-01-19T02:14:26Z,sadness
762563429,Oh sorry I didn't mean that the pull request was related to the CI issue. But that the solution in might make nonzero on par with my solution. I don't get how TestMatmul.test_vector_matrix_values can crash when I add some performance tests. I'll restart and see if it's just some CI hickup.,2021-01-19T02:39:26Z,sadness
762751561," , I have replaced the MIN/MAX Macros, placed the NPY_SIMD checking guard at the proper place, merged the count_nonzero_int16/32/64 functions into a single function and added benchmarks for the 4 int types.",2021-01-19T10:28:59Z,anger
762759302,"Are you certain that 2.12 is the last version before __STDC_NO_THREADS__ was added, or is that simply the last version you tested without it? Either way, that looks reasonable though you should be checking __GLIBC__ 2 || (__GLIBC__ == 2 && __GLIBC_MINOR__ 12) for forwardscompatibility. I suppose the other option is to just discover if threads.h is available as part of config_test.",2021-01-19T10:43:41Z,neutral
762768273,"What's the status of this PR? Did all the components end up as separate PRs? If so, can we link them here then close this?",2021-01-19T11:00:45Z,sadness
762774566,"Huh, that commit is authored by someone () I've seen doing something completely different in another repository open source is a small world! Referencing the mailing list post in a comment might be a good idea",2021-01-19T11:12:23Z,fear
762795779,sorry for forgetting about this for so long! That's completely fine :),2021-01-19T11:56:48Z,sadness
762828818," Looks good to me. Maybe cov instead of co? The latter has no obvious meaning. It is used consistently as suffix in both [typeshed]( and [typing]( when referring to covariant typevariables, so I'd argue there is very much a precedent for using co.",2021-01-19T13:08:05Z,neutral
762837673,wieser Do you want me to squash the commits?,2021-01-19T13:25:58Z,neutral
762842102," If setting __all__ is not possible, a workaround might be to ""explicitly reexport"" the objects using something this: python from ._array_like import ArrayLike as ArrayLike from ._dtype_like import DTypeLike as DTypeLike Nice and simple, I feel that this is approach would neatly fix the entire issue. Would you be willing to submit a PR?",2021-01-19T13:34:19Z,neutral
762859701,Personally I prefer the first one. Feel free to use a (parenthesized) multiline import if the statement becomes too long.,2021-01-19T14:04:52Z,neutral
762864526,"Thanks , I'll try to refactor that bit. I had asked the python guys here: , seems like we can use PyLong_AsLong directly and try. The gdb in CI is very useful, thanks.",2021-01-19T14:12:39Z,joy
762877840,"Indeed, I would classify this as correct, it doesn't seem feasible to special case []. If the list may be empty, you will just have to go the long route unfortunately. Happy to reopen, especially if anyone has an idea how to handle it better.",2021-01-19T14:32:09Z,joy
762934071,Thanks Matti.,2021-01-19T15:52:40Z,neutral
762935043,Thanks Bas.,2021-01-19T15:54:01Z,neutral
762935578,Needs rebase.,2021-01-19T15:54:45Z,neutral
762939849,Thanks .,2021-01-19T16:00:44Z,neutral
762978498,"After more research, the [bug report]( is still marked as UNCONFIRMED.",2021-01-19T16:58:33Z,fear
763020388,Benchmarks after the changes: before after ratio [f5f845b7] [fe3a8975] <master <enh_14415_fast_compare + 583±4ns 622±4ns 1.07 bench_scalar.ScalarMath.time_addition('longfloat') 3.63±0.03μs 3.45±0.03μs 0.95 bench_scalar.ScalarMath.time_addition_pyint('float32') 584±0.8ns 554±2ns 0.95 bench_scalar.ScalarMath.time_multiplication('complex256') 563±7ns 531±2ns 0.94 bench_scalar.ScalarMath.time_addition('float32') 567±2ns 527±3ns 0.93 bench_scalar.ScalarMath.time_multiplication('int64') 23.7±0.2μs 11.5±0.07μs 0.49 bench_scalar.ScalarMath.time_compare('int32') 24.1±0.7μs 11.7±0.2μs 0.48 bench_scalar.ScalarMath.time_compare('int16') 25.9±0.3μs 12.1±0.05μs 0.47 bench_scalar.ScalarMath.time_compare('complex64') 26.2±0.2μs 12.0±0.07μs 0.46 bench_scalar.ScalarMath.time_compare('float32') 26.3±0.5μs 11.9±0.03μs 0.45 bench_scalar.ScalarMath.time_compare('float16'),2021-01-19T18:03:56Z,fear
763051878,Needs rebase,2021-01-19T18:55:39Z,neutral
763155300,Thanks Pearu.,2021-01-19T21:33:14Z,neutral
763172041,"Unfortunately I cannot reproduce this (the underlying problem with OpenBLAS' cpu detection code) on my hardware, so I cannot confirm that the trivial attempt at fixing it with a volatile keyword in the current develop branch actually works. Are all failure reports from Nvidia Jetson devices ?",2021-01-19T22:06:10Z,sadness
763187846,"My initial report was using a Jetson device, I can try it on a different ARM CPU tomorrow",2021-01-19T22:36:48Z,neutral
763246983,raspberry pi 4 (armv7l) has no problems.,2021-01-20T00:48:17Z,neutral
763254096,wieser The final part is about to come.,2021-01-20T01:09:19Z,sadness
763283289,Can you provide the benchmark result by running python runtests.py benchcompare master bench_core.Core?,2021-01-20T02:24:35Z,fear
763389429,"OpenBLAS does not provide DYNAMIC_ARCH for ARMV7 (no practical difference between the provided cpu targets), but I could not reproduce the problem on a Pi4 in 64bit ARMV8 mode.",2021-01-20T07:10:07Z,sadness
763403454,"I can't reproduce the issue on AWS Graviton2 either (ARMv8) but it does definitely occur in Jetson Nano, Jetson TX2 and Jetson Xavier NX. ",2021-01-20T07:40:27Z,fear
763403571,Superseeded by which includes the present fix regarding __user__.,2021-01-20T07:40:42Z,neutral
763482477,My reproduction was not on a Jetson. I'm actually not sure what it is it's just a (pretty powerful) server I ssh into.,2021-01-20T09:53:04Z,fear
763484857,"frbg if you can disassemble gotoblas_dynamic_init and look for the mrs instruction and the getauxval bl instruction before it, you can check the ordering of the tbz instruction relative to mrs (see [my previous comment](756230802)).",2021-01-20T09:56:55Z,neutral
763535153,"Thanks, for the explanation. Here's a 4line implementation of numpy_distutils.py module that is pure Python and should provide the required functionality: numpy_distutils.py import builtins as _builtins _builtins.__NUMPY_SETUP__ = True from numpy.distutils import _builtins.__NUMPY_SETUP__ = False However, as noted above, this will not be sufficient to build scipy: numpy.f2py has to be adjusted to not use numpy.core tools.",2021-01-20T11:20:16Z,fear
763636807,"I just checked and the sever that reproduced on has a Cavium ThunderX CN8890 CPU (ARMv8.1), notably not a Jetson.",2021-01-20T14:20:53Z,neutral
763744543,If it fixes either of the two problems I'll definitely backport. I will be happy if we can get over the problems that have ensued since Windows 2004.,2021-01-20T16:14:42Z,joy
763812152,"Interesting, thanks ThunderX is what the drone.io CI uses, though the OpenBLAS build has been using gcc 7.5 there. (Checked now that the current develop branch passes with gcc 10.2 _and_ the tentative patch for this issue)",2021-01-20T17:32:43Z,joy
763888885,"Closing, this is configurable with np.errstate and a deliberate choice.",2021-01-20T19:45:29Z,anger
763909860,could you confirm that this issue should indeed be closed? You should be able to find wheels in the [weekly builds]( in a few days,2021-01-20T20:24:14Z,joy
763922085,Thanks Ralf.,2021-01-20T20:47:35Z,joy
763963914,"OK, just opened a small PR to just revert it. I am very happy to add the deprecation (or even the more complex logic), but it just isn't very important, so doing anything for 1.20 already seems not even worth back porting release note additions :).",2021-01-20T21:35:32Z,joy
763970718,I think is most of your problem here,2021-01-20T21:48:23Z,neutral
763973916,"Having this issue as well, it shows up with 1.20.0rc2 while testing. Ended up just watching some random YouTube video and took a risk. arch 86_64 zsh will push you to an x86_64 instance of zsh, then running pip3 install opencvpython seemed to install numpy1.19.5cp39cp39maxosx_10_9_x86_64.whl without issue. The solution above mentioning some openblas commands failed on the third install. Not sure what's going on but I now have like 4 versions of python installed, 2 versions of python3.9, 1 version of python 3.8, and another install of python 2.7.16. that ships with macOS.",2021-01-20T21:54:07Z,fear
764006341,The new code improves the accuracy due to the use of FMA too. we will have to dispatch AVX2&FMA3 and AVX512F in runtime.,2021-01-20T22:54:20Z,neutral
764128909,Thanks Sebastian. I assume you will issue a FutureWarning at some point in 1.21 development.,2021-01-21T00:58:44Z,joy
764136996,Thanks Chunlin.,2021-01-21T01:02:37Z,neutral
764156076,"The dispatching solution for nonUFunc is not recommend in the past, If it is acceptable now, then we can have a discussion on the mailing list.",2021-01-21T01:23:00Z,neutral
764174382,"Mission Accomplished, closing now.",2021-01-21T01:51:35Z,joy
764203736,you can read the benchmark guide [here](,2021-01-21T03:11:11Z,neutral
764438030,Thanks a lot!,2021-01-21T07:21:19Z,joy
764640903,The branch has been rebased and the tests seem to be passing.,2021-01-21T13:25:46Z,neutral
764802091,RuntimeError: Broken toolchain: cannot link a simple C program,2021-01-21T17:13:20Z,fear
764802885," since the small integers are cached. I am not sure if that is a language feature or an implementation detail. While PyPy did implement a work around, other implementations may not appreciate being punished for not implementing this.",2021-01-21T17:14:17Z,anger
764810429,"I'd assume it's an implementation detail, and certainly wouldn't recommend having _behavior_ depend on it. Having performance rely on implementation details doesn't strike me as a particularly big problem though, especially since things like function call overhead are influenced by that anyway.",2021-01-21T17:25:40Z,anger
764837712,Hi! I'm new. Can I work on this issue?,2021-01-21T18:09:21Z,neutral
764860633,Why are you installing from source?,2021-01-21T18:49:52Z,neutral
764870429," Why are you installing from source? Sorry I might not understand this 100%, but we have a requirements.txt file in my repository with numpy being in it. When i run python m pip install r requirements.txt numpy fails. Is there a better way to install dependencies?",2021-01-21T19:07:03Z,sadness
764891893,"Pip is using a cached zip file, e.g., a file of source code. You can force it to use binary wheels python mpip install onlybinary numpy But the question is where the zip file came from, and if you are on a supported platform. What hardware? ",2021-01-21T19:43:50Z,neutral
764892103,"Unless there is a comment that someone is already busy on it (fairly recently), everything is always up for grabs. We don't really assign issues. So of course, go ahead.",2021-01-21T19:44:10Z,neutral
765012914,Thanks Bas.,2021-01-21T23:45:11Z,neutral
765060943,"I've added some comments on pytorch/pytorch I suspect that we'll need input from both communities to properly resolve this, so I apologize to all for the bifurcated discussion.",2021-01-22T01:57:27Z,sadness
765146469,"Sorry to hijack this thread but on a related topic on nonzero(), is there a reason why calling nonzero on a 1D array is orders of magnitude faster than a multidimensional array? For example, calling it on a Boolean array of shape (1000000,) is taking 40 µs, while it takes 1400 µs for an array of shape (1000,1000). Both arrays are identical in values and only differ in shape. Any idea what's the significant overhead cost here?",2021-01-22T05:53:04Z,neutral
765167975,"Duplicate of , . You should have led with the data point that you are using the new Apple Silicon M1 hardware. We don't currently support that hardware until we can run CI on it. You may have more luck with conda, I think they have binary packages for M1. In the meantime you can use the x86_64 Rosetta emulation.",2021-01-22T06:28:47Z,neutral
765172219," , without further investigation, I am speculating the overhead is coming from the use of an iterator and calls to get_multi_index function in whenever a multidimensional array is passed. In contrast, when a single dimensional array is passed, without an iterator the indices are computed using the npy_memchr function which is quite fast : ",2021-01-22T06:37:49Z,fear
765278133,"Perhaps a better solution could be to replace line 157: epsneg_f128 = exp2(ld(113)) by something like: if( ld(1)!=ld(1)exp2(ld(113)) ): epsneg_f128 = exp2(ld(113)) elif( ld(1)!=ld(1)exp2(ld(64)) ): epsneg_f128 = exp2(ld(64)) elif( ld(1)!=ld(1)exp2(ld(53)) ): epsneg_f128 = exp2(ld(53)) else: raise Exception(""Problem with longdouble"") ",2021-01-22T09:43:33Z,neutral
765299943,"This is not a problem localized to import, right? If someone uses numpy and happens to get an overflow error, NumPy (or any wellbehaved library) internally detect that and converts the value to a +float('inf') or float('inf') without a signal handler. We could consider adding a signal handler of our own, but that just seems like an arms race: whoever wants to detect this situation will just invent a better way to work around our mask. When would a user of scientific python want to trap overflow signals that may naturally occur in mathematical calculations? ",2021-01-22T10:15:25Z,fear
765313821,"I think that what to do with an overflow is user dependant. Some people may want use it as inf, other should stop the code with an error if overflow appears because it must not appear. The question could be ""Is it an expected behaviour that an overflow occurs ?"" During an application run, I think it is application (user and/or developer) dependant. During a library importation, I would answer no it isn't.",2021-01-22T10:39:53Z,fear
765336275,"Hey , can you have a look at this PR. Thanks.",2021-01-22T11:26:08Z,neutral
765414346,this link: has disappeared since.. it would sure nice to find that excellent explanation again from somewhere..,2021-01-22T13:55:32Z,joy
765439733,"It is but it is just a call to a LAPACK function DTRTRS so not technically impossible. I still think it is too specialized for inclusion in NumPy. Moreover, if a package wants really fast sampling for a particular covariance/precision structure, it is much simpler to write a lowlevel, highperformance implementation that can be shipped to users of the downstream package. This said, I do still there there is a case for the ""factor"" method. ",2021-01-22T14:29:52Z,neutral
765441291,I see you have done exactly that in your implementation. It is nice to see that this feature is being picked up.,2021-01-22T14:31:56Z,neutral
765455705,"I forgot about this NEP. Nice timing to comment on it . I'll at least link to it from my upcoming NEP about the array API standard. I'd also be interested to push this forward, just a matter of finding the time .....",2021-01-22T14:50:36Z,anger
765457533,Let me also link which has all the relevant public/private/inbetween parts listed.,2021-01-22T14:52:53Z,neutral
765501136,Still trying to figure out why the azure tests have been failing. Hopefully will grant a bit of insight.,2021-01-22T15:51:07Z,anger
765515918,"Thanks . charris added this to the 1.20.0 release milestone 14 hours ago would indeed be nice to take this along, the change should not require an RC3 I'd think. I'm happy to submit a PR this weekend is Sunday in time?",2021-01-22T16:10:25Z,joy
765555826,"Arrow tagged 3.0.0 Monday, but a new pyarrow isn't up on PyPI yet, so Sunday will be fine. I'm planning on making the 1.20.0 release the weekend after this.",2021-01-22T17:02:50Z,neutral
765574521,Thanks .,2021-01-22T17:36:17Z,neutral
765577363,"Maybe we should change the assert to not fail for signal 9 and xfail instead. Note that the test didn't raise, so didn't run to completion.",2021-01-22T17:41:39Z,fear
765607733," I still think it is too specialized for inclusion in NumPy. Sure, no worries I have an implementation of this here. Cool, thanks",2021-01-22T18:34:48Z,joy
765690671,Is this page what you are looking for?,2021-01-22T21:21:29Z,neutral
766038838,Thanks. Fix in gh18211.,2021-01-23T13:26:41Z,joy
766097827,+1 the whole point of this new M1 mac is to do ML and numpy is pretty integral..,2021-01-23T15:34:16Z,neutral
766157468," if the warning is actually manually set, it would be something like npy_set_floatstatus. But most likely the opposite is the case: The C code (machine code), sets an error flag (the CPU does this, or well, the basic math library) for a certain operation. And our code would have to avoid taking that code path alltogether.",2021-01-23T18:32:45Z,fear
766165770,"Thanks for the reviews. I updated the text for most of them, and replied to the ones I didn't quite agree with.",2021-01-23T19:24:13Z,joy
766175121,Sorry for the clutter :) Github isn't made for line editing documents.,2021-01-23T20:35:10Z,sadness
766248652,The final fix is nearing release and is now in beta and release channels. Hopefully out next month. Windows 10 build 19042.782 We fixed an issue that causes the 64bit fmod() and remainder() functions to damage the Floating Point Unit (FPU) stack.,2021-01-24T00:32:44Z,fear
766298724,Adopted the xfail idea.,2021-01-24T06:12:15Z,neutral
766316579,"I disagree that it is ""too specialized to be included in Numpy"". The precision matrix parametrization of any statistical model is as natural as the covariance matrix parametrization. There is no good reason to consider the covariance matrix as ""natural"" and the precision matrix as ""specialized"". Secondly, a key feature of Numpy is speed and (related to this) scalability. Inverting a matrix is neither fast nor scalable.",2021-01-24T09:19:56Z,fear
766323349,Thanks . It would be helpful if you could add a benchmark for matrix_power in benchmarks/benchmarks/bench_linalg.py that shows the performance benefit.,2021-01-24T10:16:48Z,joy
766323665,"Merged, thanks . Please note that we prefer not to get pure style change PRs, because they're on average not worth the churn, review effort and CI time. If you want to try another PR, tackling a bug or documentation issue would be great.",2021-01-24T10:19:46Z,fear
766323935,are you planning to update this PR with the change requested by ?,2021-01-24T10:22:22Z,neutral
766324287," I should probably also figure out why they're segfaulting at some point. /proc/cpuinfo mentions AVX2 and FMA but not FMA3, and that roughly exhausts my knowledge of how to debug the crashes. or any advice here on how to debug?",2021-01-24T10:25:29Z,anger
766335257," Sorry for the clutter :) Github isn't made for line editing documents. No worries, thanks for the copy edits. All adopted.",2021-01-24T11:52:04Z,sadness
766373481,This is a meaningless change. Please do not practice your git skills here: make your own repo for that.,2021-01-24T16:04:16Z,anger
766373607,"This is a meaningless change. Please do not practice your git skills here, use a personal repo.",2021-01-24T16:05:08Z,anger
766394990,"Let's give it a shot. We might want to make the xfail more general, i.e., exitcode anything but 0, but this should cover the immediate problem. ",2021-01-24T17:03:54Z,neutral
766402826,Did you mean to update the version of the theme in [the requirements file]( They have released 0.4.2,2021-01-24T17:55:27Z,neutral
766403090,Are you sure this was merged? I see a WARNING: unsupported theme option 'logo_link' given,2021-01-24T17:57:16Z,neutral
766403491,"Right, that was just a rebase just now. Will push once I confirm the docs in my local.",2021-01-24T17:59:56Z,fear
766406214,"Whoops you're right, the merge wasn't included in this release, gotta wait until they do I guess.",2021-01-24T18:14:42Z,neutral
766421548,Thanks Ralf.,2021-01-24T19:55:44Z,joy
766424010,"Hmm, ignoring the warning seems correct. Strange that it doesn't show in most test runs. Maybe warning tests can still sometimes be flaky :(. I thought the new python versions (and pytest) were smart enough to avoid flakiness... Thanks Chuck!",2021-01-24T20:11:23Z,fear
766429209,", we use the term FMA3 instead of FMA to avoid any confusion with AMD/FMA4. , any advice here on how to debug? Testing CPU detecting mechanism python runtests.py t numpy/core/tests/test_cpu_features.py NOTE: You will have to patch test_cpu_features.py to enables Cygwin. Testing _SIMD module itself without any dispatched features: python runtests.py cpudispatch=""none"" t numpy/core/tests/test_simd_module.py python runtests.py cpudispatch=""none"" t numpy/core/tests/test_simd.py Testing _SIMD module with certain dispatched features, e.g. sse41 python runtests.py simdtest=""baseline sse41"" t numpy/core/tests/test_simd.py Checking the build log, and finally GDB or LLDB ",2021-01-24T20:47:06Z,fear
766487394,The performance improvements looks good to me.,2021-01-25T01:44:13Z,joy
766593745,Thanks,2021-01-25T06:59:15Z,neutral
766623509,"The code change here should not trigger the CI test failure of numpy.core.tests.test_multiarray.TestMethods.test_no_dgemv, will restart for verification.",2021-01-25T07:55:49Z,fear
766623512,"It may look confusing to those who have written kLOCs of Python2 code. But every new Pythonista learns that 1 / 60 is a float. And there are 60 minutes in an hour, not 60.0. Of course I can remove these changes. But that would be my question: would you actually like to get the code to Python3 level entirely, i.e. remove all Python2 relics? ",2021-01-25T07:55:50Z,anger
766623533,Thanks,2021-01-25T07:55:52Z,neutral
766636563,"I think this change is fine but there are subtleties to watch out for 1 / float(x) and 1/x have different types when x has type np.float64. The two aren't quite interchangeable, but the former is sometimes risky as on longdouble it causes precision loss.",2021-01-25T08:18:30Z,sadness
766705174,"This was likely closed by , though it's still missing the acceptance.",2021-01-25T10:10:02Z,sadness
766711296,"Thanks vetinari, I forgot about this one. I'll close it through the next PR which changes the status to accepted and adds the mailing list links.",2021-01-25T10:20:20Z,joy
766754039,A couple of release highlights for mypy 0.800: By far the most interesting seems to be the added support for [PEP 604]( _i.e._ we can now use the | operator for writing unions: python def func(a: int | None) None: ...,2021-01-25T11:34:41Z,joy
766782445,"Anyone knows what's up with the DOWNLOAD_OPENBLAS=1 ATLAS=None tests on [travis]( It seems to be failing quite a bit recently, despite a lack of any clear reason as to why.",2021-01-25T12:28:21Z,anger
766876833,Thanks,2021-01-25T15:01:30Z,neutral
766884163,Thanks and dependabot.,2021-01-25T15:11:31Z,neutral
766887364,Comparing an anchor of the new [version of the theme]( to the [current version]( they fixed the problem with the header covering the test. (issue ),2021-01-25T15:16:13Z,anger
766888175,Closed by updating the theme version to 0.4.2.,2021-01-25T15:17:18Z,joy
766891406,"Good old dependabot, what a dependable fellow.",2021-01-25T15:21:59Z,joy
766912054,"Something changed. [This page]( (from the CI build of the new theme) no longer overlaps for us. For the next few minutes, until CI finishes, you can see the old page [here](",2021-01-25T15:51:14Z,fear
766922727,"Hmm, on my browser (firefox, Ubuntu), both links give the incorrect offset ..",2021-01-25T16:05:32Z,sadness
766927114,On Firefox I can confirm the title is still overlapping for the CI build of the new theme.,2021-01-25T16:12:04Z,neutral
766943522,"Closing, pyarrow should be fine now (and about to release).",2021-01-25T16:36:02Z,neutral
766968094,Any progress?,2021-01-25T17:12:12Z,neutral
766969826,Ping. Any update on progress getting the ARM infrastructure in Azure?,2021-01-25T17:14:58Z,fear
766972598,This looks ready to go. Comments?,2021-01-25T17:17:21Z,joy
766972624," Ping. Any update on progress getting the ARM infrastructure in Azure? Unfortunately, I cannot access the Azure infra, but the code looks great and it works great IMHO.",2021-01-25T17:17:24Z,joy
766973461,Needs rebase. Is this still relevant?,2021-01-25T17:18:38Z,neutral
766974379,What is the status of this?,2021-01-25T17:20:01Z,neutral
766975313,wieser Look good to you?,2021-01-25T17:21:24Z,neutral
766980838," yes, made this PR to close , waiting to be released",2021-01-25T17:29:29Z,joy
767014840,"nop, no clues. Most of the things I fix are due to another unrelated project that does not use sphinx/docutils. Might be from numpy/polynomial/, some functions seem to have a lot of c , and maybe sphinx don't resolve them properly ? and a few functions like polyvander seem to reference c but have no c parameters",2021-01-25T18:18:40Z,anger
767017247,still present in version 1.19.2,2021-01-25T18:22:48Z,anger
767019270,"Ahh, I see. I chose a page that is quite small. Because of differing zoom betwee the two versions, in one case the whole page fit better and showed the header, on the morezoomedin one it did now. ",2021-01-25T18:25:51Z,anger
767025725,"I tried to give some more details about integers in the release notes, and mention the precision version float64 rather than float_ in the message. So hopefully it is a bit more clear, it may be slightly opinionated.",2021-01-25T18:35:48Z,neutral
767029488,Needs rebase. I'm inclined to close this unless it is still relevant. wieser thoughts?,2021-01-25T18:42:27Z,sadness
767031271,Is this still relevant after recent changes to stride_tricks and the addition of a rolling window?,2021-01-25T18:45:42Z,neutral
767033103,Does the SIMD work supercede this? Thoughts?,2021-01-25T18:48:52Z,sadness
767040538,"Since it wold require a rebase, lets close it. I think there may still be some value in it, so if anyone wants to pick it up, that sounds reasonable to me. But I agree that in the majority of cases the new sliding window function or broadcast_to probably will do the trick better. So the need should be much lower, and a keyword argument is probably not as convenient as the alternatives.",2021-01-25T19:01:03Z,neutral
767062505,Thanks for making the PR.,2021-01-25T19:34:37Z,joy
767063935,"Hmmm, I thought I would look into this. But I am honestly not sure where we should be adding this flag, or if we actually should (additionally) even prod Python itself to add this flag. Am I right to think that most clang specific flags currently are inherited from Python itself?",2021-01-25T19:37:11Z,fear
767080881,Where do we stand with this? Has it been discussed on the mailing list? Does it need to be or is this niche enough we can merge it as is?,2021-01-25T20:07:16Z,neutral
767085153,Last night's wheels built cleanly. Can we close this?,2021-01-25T20:15:08Z,joy
767086166,"This has been mentioned multiple times on the list, just not super recently.",2021-01-25T20:16:58Z,anger
767089123,"Yes. This was a problem on aarch64, but it seems fixed now. Triggering rebuilds succeeded in 2 out of 3, and the last after the fix went in (showed xfail).",2021-01-25T20:22:11Z,sadness
767183632,"I had the same problem. But I came up with a pretty simple solution. Please refer to the link below. Note: I think enabling/disabling mmap_mode is not really related to this problem. So in this case, feel free to do whatever you think is better for your particular situations.",2021-01-25T23:41:21Z,anger
767257136,"I've validated I can get basic loading under softlimit 400MB with numpy1.21.0.dev0+485.gbec2b07dbcp38cp38manylinux2010_x86_64.whl, and a manually invoked build seems to build OK under 1GB softlimit. Our formal build processes won't have any change until 1.19.6 or later comes out on pip Thanks for your help",2021-01-26T03:02:43Z,fear
767284249,Thanks vetinari .,2021-01-26T04:11:31Z,neutral
767288919,What was the reason to change the 1.20.0 release note?,2021-01-26T04:29:38Z,neutral
767291192,"Thanks, have to try how to fix that reference. Will fix (maybe tomorrow morning). There was some discussion that this release note/deprecation is maybe a bit unclear. I hope this is a bit clearer (and ""recipe style""), so the idea was to change it before the release.",2021-01-26T04:37:45Z,neutral
767341664,Thanks,2021-01-26T06:49:59Z,neutral
767366158,"Thanks for the review , I shall fix the scalar conversion of other and then add the tests you mentioned as well.",2021-01-26T07:48:33Z,joy
767394663,"No more followup, and indeed looks like a duplicate. I've also changed the title, because this seems unrelated to Alpine.",2021-01-26T08:47:29Z,sadness
767404482,I don't see a difference in the rendered documents [before]( and [after]( Am I missing something?,2021-01-26T09:05:47Z,sadness
767468332,"I have been bitten several times by this problem. Since the docstring says about shift If a tuple, then axis must be a tuple of the same size. it would be great to enforce this in the code and raise an exception if this condition is not fulfilled, rather than carrying out an operation which the documentation clearly declares invalid.",2021-01-26T11:01:29Z,fear
767663721,"I just hit this through pyupgrade also. The fact that %.4fformatting works but fstrings/.format doesn't, seems like a genuine bug.",2021-01-26T16:30:39Z,fear
767667405,"I checked that the latest master, and geopandas tests are passing again now (didn't check with the 1.20.x maint branch, but assuming that that will be OK as well since the fix is backported). Thanks!",2021-01-26T16:36:10Z,neutral
767670613,The documentation is correct in its current form. The following paragraphs explain the difference between __array__ and other methods like __array_ufunc__ and __array_function__ in terms of how output types are handled.,2021-01-26T16:40:56Z,neutral
767682734,"No opinions from me note that I originally opened the issue only to track a TODO that was removed when some documentation was updated. I personally don't use the setops often, so I'm not sure what the most common usecases are.",2021-01-26T16:59:28Z,anger
767684403,"Allow me to insist: the arr object is of type DiagonalArray, not ndarray, so the current block is incorrect. The following paragraph explains how one can make np.multiply return a DiagonalArray instead of a ndarray: How can we pass our custom array type through this function?... I believe that the current version assumes that arr is the output from np.multiply above, but it's not.",2021-01-26T17:01:47Z,neutral
767695913," I believe that the current version assumes that arr is the output from np.multiply above, but it's not. The output of np.multiply(arr, 2) is the object returned by __array__, i.e. the result of self._i np.eye(self._N): np.eye creates an ndarray object. Broken into finer steps: python arr = DiagonalArray(5, 1) type(arr) __main__.DiagonalArray b = np.multiply(arr, 2) type(b) numpy.ndarray ",2021-01-26T17:19:50Z,fear
767700021," I understand all that but please read the document. It goes like this: py arr = DiagonalArray(5, 1) type(arr) __main__.DiagonalArray np.multiply(arr, 2) type(arr) numpy.ndarray which is wrong. Note how the output of np.multiply is not arr and it's not assigned to anything.",2021-01-26T17:26:32Z,neutral
767719630,"Just to be clear, type(np.multiply(arr, 2)) is calling type() on the output of np.multiply(arr, 2), which, as you point out, is not arr. Thus the original docs are correct. In your above example, you have python np.multiply(arr, 2) type(arr) Which would be incorrect, but is different than the original documentation.",2021-01-26T17:56:57Z,neutral
767725268," Thus the original docs are correct. Sorry, no but is different than the original documentation. No, it's not different. Are we looking at the same document? ",2021-01-26T18:05:54Z,sadness
767727763,"Wow huge brainfart on my part I just failed to interpret the diff correctly (I was assuming your change was what was already in the docs). So when I say ""the original docs are correct"", what I meant was ""your original proposed changes are correct! Sorry for all the noise.",2021-01-26T18:10:10Z,sadness
767730276,No wonder I couldn't figure out why we were disagreeing even though I felt like we were saying the same thing :) thanks for bearing with me,2021-01-26T18:13:37Z,anger
767740064,"This looks great, thanks !",2021-01-26T18:29:26Z,joy
767797166,"Thanks Eric and Ross for figuring out how to correctly link the user guide... applied the changes, so should be good to go (assuming CI passes now).",2021-01-26T20:09:18Z,fear
767803640," Any update on progress getting the ARM infrastructure in Azure? Let me be careful here... there is _progress_, but there is no _update_... yeah I think I can say that much. :)",2021-01-26T20:21:03Z,neutral
767828083," I think the test results above hold, for my purposes its working ok.",2021-01-26T21:07:09Z,joy
767831325,Thanks sebastian.,2021-01-26T21:13:15Z,neutral
767896522,Thanks .,2021-01-26T23:26:15Z,neutral
768003980," and thanks for the comments/suggestions. I apologize for letting this sit, the Holidays were much more hectic than I imagined. I made most of the suggested changes, but then hit checks were not successful again. I'm having some trouble building NumPy with pip install .. I hit the import error: We have compiled some common reasons and troubleshooting tips at: Am I missing a step in the build process? or doing it wrong?",2021-01-27T03:52:09Z,anger
768067439,More information needed. What error exactly are you seeing?,2021-01-27T06:34:07Z,anger
768074889,The documentation for the [__array__ function]( is not very descriptive. It describes what the ufunc will not do if __array__ is present. It should describe what the prospective __array__ user is expected to do with self and optional dtype. That would be nice to expand upon here or in a followup PR.,2021-01-27T06:51:52Z,joy
768147317,"There is a stray "" in longer build or would give incorrect results.""",2021-01-27T09:15:08Z,fear
768317608,"It seems like we now have a (draft) pep for variadic generics, _i.e._ generics with not one but an arbitrary number of variables (like typing.Tuple): [PEP 646]( ",2021-01-27T14:21:24Z,sadness
768393910,"Just noticed this function has been added to version 1.20.0, hooray! Although writing np.lib.stride_tricks.sliding_window_view is a mouthful, would be great if this could be moved to the main module and have its name shortened ",2021-01-27T16:11:05Z,fear
768397281,"Well, you're free to do python from numpy.lib.stride_stricks import sliding_window_view I think voiced against putting this in the main namespace on the mailing list, although I may be mistaken. But yes, sliding_window_view(arr, window_shape=(2, 2)) should do exactly what this issues asks for, so I think this issue can be closed. If you want to discuss naming, I'd recommend finding and responding to the mailing list thread, or opening a new issue.",2021-01-27T16:16:03Z,anger
768417915,Sounds like we should consider this a bug (although when fixing I would probably just go with a VisibleDeprecationWarning just in case).,2021-01-27T16:47:24Z,fear
768422683,"Seems correct to me, in the sense that max(..., 0) can (and does in the test run) return an integer and it is entirely possible for you to ensure that it does not. In many cases defining otypes may be a good thing to ensure this in any case.",2021-01-27T16:54:16Z,fear
768477669,"On my side things are ok, just pending the indentation issues. Thanks, !",2021-01-27T18:17:51Z,anger
768490168," On my side things are ok, just pending the indentation issues. Thanks, ! Thanks ! regarding indentation, do you mean the outputs or the code blocks? I think I caught the indentation issues we discussed above. I'll build again and check formatting. ",2021-01-27T18:39:19Z,fear
768515076,Checkout the CI failure on cirecleCI build details. It is complaining about indentation and line 34.,2021-01-27T19:17:04Z,anger
768518680,"We had discussed this, and we somewhat agreed that this is desireable, and should have FutureWarnings (or VisibleDeprecationWarning. A specific issue may be np.loadtext and genfromtxt since that might be a more common use case for using 0 and 1 as boolean, so that may need some special consideration. I hope to look at this, hopefully, FutureWarnings (and opting in to future behaviour) will be reasonably possible.",2021-01-27T19:22:51Z,joy
768546192," Checkout the CI failure on cirecleCI build details. It is complaining about indentation and line 34. Thanks, I see that. I don't know why its complaining about the indent for the list. I tried making it one line in the last commit since it doesn't like my indents (third time's a charm). ",2021-01-27T20:09:54Z,anger
768551057,"SeedSequence.spawn(n) always returns a list of n elements, even when n==1. Use child_seq = old_seed_seq.spawn(1)[0] to make sure that the child_seq is the actual desire SeedSequence.",2021-01-27T20:19:03Z,fear
768580300,"Agreed in fact the documentation is so unclear, that I am not even sure what it is saying! Thus I am afraid that I might not be the one to come up with better wording.",2021-01-27T21:12:16Z,fear
768585641,"If you are up for it, that would be great anyway, we can always iterate together (no pressure though). The only subtlety I can really think of, is that it should be completely fine to ignore dtype which would just trigger casting in NumPy itself.",2021-01-27T21:22:07Z,neutral
768601498,"I argued against step_size when we designed this function, because you can get that behavior already with sliding_window_view(x, 3)[::2].",2021-01-27T21:53:05Z,anger
768610616," Checkout the CI failure on cirecleCI build details. It is complaining about indentation and line 34. Thanks, I see that. I don't know why its complaining about the indent for the list. I tried making it one line in the last commit since it doesn't like my indents (third time's a charm). L34 indentation was changed to a single line. It fixed the circleci build test. Looks like we are ready to merge. ",2021-01-27T22:07:21Z,anger
768644629,"On second thought, we should probably just get rid of make dist completely, and build all release artifacts via CI. make dist is really weird, the Makefile for the docs should just build the docs, not jump through weird hoops to build numpy and put it in a nonstandard place.",2021-01-27T23:24:44Z,fear
768814989,"OK, let's put this in asis, it is a good first step.",2021-01-28T05:44:53Z,joy
770383740,"Restarting CI, something weird happened.",2021-01-31T13:36:40Z,sadness