id,body,created_at
313938514,"To partially fix this, you could do:

```python
a = np.cumsum(x)
a *= np.sum(x) / a[-1]
```

Which would correct the final value, at the expense of the immediate values.

A better option would be to split the summation into segments, using the more accurate method for between-segment sums, for instance calculating:
```python
a = np.cumsum(x)
```
as
```python
s = 0
a = np.empty(len(x))
for i in range(0, len(x), 4):
    a[i:i+4] = s + np.cumsum(x[i:i+4])
    s += a[i:i+4]
```

I actually think this would be a reasonable feature request for numpy - if we do pairwise summation for `.add.reduce`, I think adding a special case for `.add.accumulate` is reasonable.

Or perhaps `accumulate` should gain a `chunking` argument, to apply this type of precision improvement to any `ufunc` with a reorderable-none identity",2017-07-09T18:51:38Z
505691219,"> how is it possible that numpy does not have a `matrix_transpose` function

I hid an undocumented one at `np.linalg.linalg.transpose` that uses the same broadcasting rules as the other linalg functions. You're welcome ;)",2019-06-26T02:24:17Z
511477435,"Another hint: If you use `numpy.add.at`, a much faster alternative is `numpy.bincount` with its optional `weight` argument:
```python
import perfplot
import numpy

numpy.random.seed(0)

def numpy_add_at(data):
    a, i = data
    out0 = numpy.zeros(1000)
    numpy.add.at(out0, i, a)
    return out0

def numpy_bincount(data):
    a, i = data
    return numpy.bincount(i, weights=a, minlength=1000)

perfplot.show(
    setup=lambda n: (numpy.random.rand(n), numpy.random.randint(0, 1000, n)),
    kernels=[numpy_add_at, numpy_bincount],
    n_range=[2 ** k for k in range(24)],
)
```
![out](https://user-images.githubusercontent.com/181628/109859400-35b83700-7c5d-11eb-99a6-a1f609c012e9.png)",2019-07-15T16:40:19Z
604571385,"> It seemed to me the main change is that resids previously had (in some sense) a keepdims in there, while now it does not? I.e. you do not add a dimension of size 1 at the end anymore.

It's a little worse than that. The old behavior was to return any of:

* A result of shape `(0,)` for an underconstrained solution (for any input shape)
* A result of shape `(1,)` for a well-constrained solution with input shape (M,)
* A result of shape `(K,)` for a well-constrained solution with input shape (M, K)

The new behavior is

* A result of shape `(..., N,)` for input shape `(..., N, M)`
* A result of shape `()` for input shape `(M,)`

So there are two incompatible changes here, not just one.",2020-03-26T17:35:40Z
671426864,"Here:

```
Python 3.10.0a0 (heads/master:39042e0, Aug 10 2020, 09:38:02) 
[GCC 8.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy
>>> import sys
>>> numpy.__version__
'1.19.1'
>>> sys.version
'3.10.0a0 (heads/master:39042e0, Aug 10 2020, 09:38:02) \n[GCC 8.3.0]'
>>> 
```",2020-08-10T15:32:49Z
685809035,"Sorry, I retract my retraction. That failure comes from 1.17.0. In master, it _is_ allowed:
```python
>>> a = np.array([np.zeros((3, 2)), np.zeros((3, 3))], dtype=object)
>>> a
array(list([array([[0., 0.],
       [0., 0.],
       [0., 0.]]), array([[0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.]])]), dtype=object)
>>> a.shape
()  # huh?
>>> np.__version__
'1.20.0.dev0+5dbc66e'
```
but frankly the output is nonsense.",2020-09-02T15:22:57Z
685815029,"Hmmm, yeah, although I think that one is likely just an independent bug, the discovered shape is incorrect to begin with.",2020-09-02T15:31:41Z
685817615,"Hmm, ok, what we could do (or argue as being correct), is that the shape here should always be `(2,)` when `dtype=object`.  But that is also a bit strange, it would resolve all of these issues in a probably even decent way, but is a bit complicated by itself.

> Mind creating a new issue for it?

I am looking if I can find some obvious thing right now... if not, yes.  I think the shape should be discovered as `(2, 3)` here probably, and then we just get the old result.",2020-09-02T15:35:35Z
685824689,"> Mind creating a new issue for it?

I am looking at it right now, my shape updating code was pretty old, and required some more love. I guess the code coverage looked like the case was covered but it wasn't. (there was a simple sign error)

But I am also looking whether I can make the warning distinguish the case where `dtype=object` will definitely not help in the future (i.e. if an array would have to be split into sequences).",2020-09-02T15:46:47Z
691932282,"Just this comment left to address: https://github.com/numpy/numpy/pull/16987#discussion_r487152916

I cleaned up the whitespace myself",2020-09-14T09:19:37Z
731236307,"Build failure:

<details>

```
clang -bundle -undefined dynamic_lookup -Wl,-headerpad,0x1000 -arch arm64 -arch x86_64 build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/alloc.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/arrayobject.o build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/arraytypes.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/array_assign_scalar.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/array_assign_array.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/arrayfunction_override.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/buffer.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/calculation.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/compiled_base.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/common.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/convert.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/convert_datatype.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/conversion_utils.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/ctors.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/datetime.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/datetime_strings.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/datetime_busday.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/datetime_busdaycal.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/descriptor.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/dragon4.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/dtype_transfer.o build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/einsum.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/flagsobject.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/getset.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/hashdescr.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/item_selection.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/iterators.o build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/lowlevel_strided_loops.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/mapping.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/methods.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/multiarraymodule.o build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/nditer_templ.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/nditer_api.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/nditer_constr.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/nditer_pywrap.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/number.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/refcount.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/sequence.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/shape.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/scalarapi.o build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/scalartypes.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/strfuncs.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/temp_elide.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/typeinfo.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/usertypes.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/vdot.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/umathmodule.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/reduction.o build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/loops.o build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/matmul.o build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/clip.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/ufunc_object.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/extobj.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/cpuid.o build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/scalarmath.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/ufunc_type_resolution.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/override.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath/npy_math.o build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath/ieee754.o build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath/npy_math_complex.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath/halffloat.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/common/array_assign.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/common/mem_overlap.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/common/npy_longdouble.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/common/ucsnarrow.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/common/ufunc_override.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/common/numpyos.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/common/cblasfuncs.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/common/python_xerbla.o -L/usr/local/opt/openblas/lib -Lbuild/temp.macosx-10.14.6-arm64-3.8 -L/usr/local/opt/openblas/lib -lnpymath -lnpysort -lopenblas -lopenblas -o build/lib.macosx-10.14.6-arm64-3.8/numpy/core/_multiarray_umath.cpython-38-darwin.so
    ld: warning: ignoring file /usr/local/opt/openblas/lib/libopenblas.dylib, building for macOS-arm64 but attempting to link with file built for macOS-x86_64
    building 'numpy.core._umath_tests' extension
    compiling C sources
    C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64
  
    compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/include/python3.8 -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -c'
    clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/_umath_tests.c
    clang -bundle -undefined dynamic_lookup -Wl,-headerpad,0x1000 -arch arm64 -arch x86_64 build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/_umath_tests.o -Lbuild/temp.macosx-10.14.6-arm64-3.8 -o build/lib.macosx-10.14.6-arm64-3.8/numpy/core/_umath_tests.cpython-38-darwin.so
    building 'numpy.core._rational_tests' extension
    compiling C sources
    C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64
  
    compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/include/python3.8 -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -c'
    clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/_rational_tests.c
    clang -bundle -undefined dynamic_lookup -Wl,-headerpad,0x1000 -arch arm64 -arch x86_64 build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/_rational_tests.o -Lbuild/temp.macosx-10.14.6-arm64-3.8 -o build/lib.macosx-10.14.6-arm64-3.8/numpy/core/_rational_tests.cpython-38-darwin.so
    building 'numpy.core._struct_ufunc_tests' extension
    compiling C sources
    C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64
  
    compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/include/python3.8 -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -c'
    clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/_struct_ufunc_tests.c
    clang -bundle -undefined dynamic_lookup -Wl,-headerpad,0x1000 -arch arm64 -arch x86_64 build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/_struct_ufunc_tests.o -Lbuild/temp.macosx-10.14.6-arm64-3.8 -o build/lib.macosx-10.14.6-arm64-3.8/numpy/core/_struct_ufunc_tests.cpython-38-darwin.so
    building 'numpy.core._operand_flag_tests' extension
    compiling C sources
    C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64
  
    compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/include/python3.8 -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -c'
    clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/_operand_flag_tests.c
    clang -bundle -undefined dynamic_lookup -Wl,-headerpad,0x1000 -arch arm64 -arch x86_64 build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/_operand_flag_tests.o -Lbuild/temp.macosx-10.14.6-arm64-3.8 -o build/lib.macosx-10.14.6-arm64-3.8/numpy/core/_operand_flag_tests.cpython-38-darwin.so
    building 'numpy.fft._pocketfft_internal' extension
    compiling C sources
    C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64
  
    creating build/temp.macosx-10.14.6-arm64-3.8/numpy/fft
    compile options: '-Inumpy/core/include -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/include/python3.8 -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -c'
    clang: numpy/fft/_pocketfft.c
    clang -bundle -undefined dynamic_lookup -Wl,-headerpad,0x1000 -arch arm64 -arch x86_64 build/temp.macosx-10.14.6-arm64-3.8/numpy/fft/_pocketfft.o -Lbuild/temp.macosx-10.14.6-arm64-3.8 -o build/lib.macosx-10.14.6-arm64-3.8/numpy/fft/_pocketfft_internal.cpython-38-darwin.so
    building 'numpy.linalg.lapack_lite' extension
    compiling C sources
    C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64
  
    creating build/temp.macosx-10.14.6-arm64-3.8/numpy/linalg
    creating build/temp.macosx-10.14.6-arm64-3.8/numpy/linalg/lapack_lite
    compile options: '-DNO_ATLAS_INFO=3 -DHAVE_CBLAS -Inumpy/core/include -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/include/python3.8 -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -c'
    extra options: '-faltivec -I/System/Library/Frameworks/vecLib.framework/Headers'
    clang: numpy/linalg/lapack_litemodule.c
    clang: numpy/linalg/lapack_lite/python_xerbla.c
    clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly
    clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly
    clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly
    clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly
    error: Command ""clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64 -DNO_ATLAS_INFO=3 -DHAVE_CBLAS -Inumpy/core/include -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/include/python3.8 -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -c numpy/linalg/lapack_lite/python_xerbla.c -o build/temp.macosx-10.14.6-arm64-3.8/numpy/linalg/lapack_lite/python_xerbla.o -MMD -MF build/temp.macosx-10.14.6-arm64-3.8/numpy/linalg/lapack_lite/python_xerbla.o.d -faltivec -I/System/Library/Frameworks/vecLib.framework/Headers"" failed with exit status 1
    ----------------------------------------
    ERROR: Failed building wheel for numpy
    Running setup.py clean for numpy
    ERROR: Command errored out with exit status 1:
     command: /Library/Developer/CommandLineTools/usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/pip-install-wbzj2i_m/numpy/setup.py'""'""'; __file__='""'""'/private/var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/pip-install-wbzj2i_m/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all
         cwd: /private/var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/pip-install-wbzj2i_m/numpy
    Complete output (10 lines):
    Running from numpy source directory.
  
    `setup.py clean` is not supported, use one of the following instead:
  
      - `git clean -xdf` (cleans all files)
      - `git clean -Xdf` (cleans all versioned files, doesn't touch
                          files that aren't checked into the git repo)
  
    Add `--force` to your command to use it anyway if you must (unsupported).
  
    ----------------------------------------
    ERROR: Failed cleaning build dir for numpy
  Failed to build numpy
  Installing collected packages: setuptools, wheel, Cython, numpy
      Running setup.py install for numpy: started
      Running setup.py install for numpy: finished with status 'error'
      ERROR: Command errored out with exit status 1:
       command: /Library/Developer/CommandLineTools/usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/pip-install-wbzj2i_m/numpy/setup.py'""'""'; __file__='""'""'/private/var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/pip-install-wbzj2i_m/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/pip-record-7i3k2v9o/install-record.txt --single-version-externally-managed --prefix /private/var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/pip-build-env-3upezx90/overlay --compile --install-headers /private/var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/pip-build-env-3upezx90/overlay/include/python3.8/numpy
           cwd: /private/var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/pip-install-wbzj2i_m/numpy/
      Complete output (290 lines):
      Running from numpy source directory.
  
      Note: if you need reliable uninstall behavior, then install
      with pip instead of using `setup.py install`:
  
        - `pip install .`       (from a git repo or downloaded source
                                 release)
        - `pip install numpy`   (last NumPy release on PyPi)
  
  
      blas_opt_info:
      blas_mkl_info:
      customize UnixCCompiler
        libraries mkl_rt not found in ['/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib', '/usr/local/lib', '/usr/lib']
        NOT AVAILABLE
  
      blis_info:
      customize UnixCCompiler
        libraries blis not found in ['/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib', '/usr/local/lib', '/usr/lib']
        NOT AVAILABLE
  
      openblas_info:
      customize UnixCCompiler
      customize UnixCCompiler
      customize UnixCCompiler
      customize UnixCCompiler
        FOUND:
          libraries = ['openblas', 'openblas']
          library_dirs = ['/usr/local/opt/openblas/lib']
          language = c
          define_macros = [('HAVE_CBLAS', None)]
          runtime_library_dirs = ['/usr/local/opt/openblas/lib']
  
        FOUND:
          libraries = ['openblas', 'openblas']
          library_dirs = ['/usr/local/opt/openblas/lib']
          language = c
          define_macros = [('HAVE_CBLAS', None)]
          runtime_library_dirs = ['/usr/local/opt/openblas/lib']
  
      /bin/sh: svnversion: command not found
      non-existing path in 'numpy/distutils': 'site.cfg'
      lapack_opt_info:
      lapack_mkl_info:
      customize UnixCCompiler
        libraries mkl_rt not found in ['/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib', '/usr/local/lib', '/usr/lib']
        NOT AVAILABLE
  
      openblas_lapack_info:
      customize UnixCCompiler
      customize UnixCCompiler
      customize UnixCCompiler
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64
  
      creating /var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/tmplbef6oap/var
      creating /var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/tmplbef6oap/var/folders
      creating /var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/tmplbef6oap/var/folders/qx
      creating /var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/tmplbef6oap/var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn
      creating /var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/tmplbef6oap/var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T
      creating /var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/tmplbef6oap/var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/tmplbef6oap
      compile options: '-c'
      clang: /var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/tmplbef6oap/source.c
      clang /var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/tmplbef6oap/var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/tmplbef6oap/source.o -L/usr/local/opt/openblas/lib -lopenblas -o /var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/tmplbef6oap/a.out
      ld: warning: ignoring file /usr/local/opt/openblas/lib/libopenblas.dylib, building for macOS-arm64 but attempting to link with file built for macOS-x86_64
      Undefined symbols for architecture arm64:
        ""_zungqr_"", referenced from:
            _main in source.o
      ld: symbol(s) not found for architecture arm64
      clang: error: linker command failed with exit code 1 (use -v to see invocation)
        NOT AVAILABLE
  
      openblas_clapack_info:
      customize UnixCCompiler
      customize UnixCCompiler
      customize UnixCCompiler
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64
  
      creating /var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/tmpwowximr4/var
      creating /var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/tmpwowximr4/var/folders
      creating /var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/tmpwowximr4/var/folders/qx
      creating /var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/tmpwowximr4/var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn
      creating /var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/tmpwowximr4/var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T
      creating /var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/tmpwowximr4/var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/tmpwowximr4
      compile options: '-c'
      clang: /var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/tmpwowximr4/source.c
      clang /var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/tmpwowximr4/var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/tmpwowximr4/source.o -L/usr/local/opt/openblas/lib -lopenblas -llapack -o /var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/tmpwowximr4/a.out
      ld: warning: ignoring file /usr/local/opt/openblas/lib/libopenblas.dylib, building for macOS-arm64 but attempting to link with file built for macOS-x86_64
      ld: warning: ignoring file /usr/local/opt/openblas/lib/liblapack.dylib, building for macOS-arm64 but attempting to link with file built for macOS-x86_64
      Undefined symbols for architecture arm64:
        ""_zungqr_"", referenced from:
            _main in source.o
      ld: symbol(s) not found for architecture arm64
      clang: error: linker command failed with exit code 1 (use -v to see invocation)
        NOT AVAILABLE
  
      flame_info:
      customize UnixCCompiler
        libraries flame not found in ['/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib', '/usr/local/lib', '/usr/lib']
        NOT AVAILABLE
  
      atlas_3_10_threads_info:
      Setting PTATLAS=ATLAS
      customize UnixCCompiler
        libraries lapack_atlas not found in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib
      customize UnixCCompiler
        libraries tatlas,tatlas not found in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib
      customize UnixCCompiler
        libraries lapack_atlas not found in /usr/local/lib
      customize UnixCCompiler
        libraries tatlas,tatlas not found in /usr/local/lib
      customize UnixCCompiler
        libraries lapack_atlas not found in /usr/lib
      customize UnixCCompiler
        libraries tatlas,tatlas not found in /usr/lib
      <class 'numpy.distutils.system_info.atlas_3_10_threads_info'>
        NOT AVAILABLE
  
      atlas_3_10_info:
      customize UnixCCompiler
        libraries lapack_atlas not found in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib
      customize UnixCCompiler
        libraries satlas,satlas not found in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib
      customize UnixCCompiler
        libraries lapack_atlas not found in /usr/local/lib
      customize UnixCCompiler
        libraries satlas,satlas not found in /usr/local/lib
      customize UnixCCompiler
        libraries lapack_atlas not found in /usr/lib
      customize UnixCCompiler
        libraries satlas,satlas not found in /usr/lib
      <class 'numpy.distutils.system_info.atlas_3_10_info'>
        NOT AVAILABLE
  
      atlas_threads_info:
      Setting PTATLAS=ATLAS
      customize UnixCCompiler
        libraries lapack_atlas not found in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib
      customize UnixCCompiler
        libraries ptf77blas,ptcblas,atlas not found in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib
      customize UnixCCompiler
        libraries lapack_atlas not found in /usr/local/lib
      customize UnixCCompiler
        libraries ptf77blas,ptcblas,atlas not found in /usr/local/lib
      customize UnixCCompiler
        libraries lapack_atlas not found in /usr/lib
      customize UnixCCompiler
        libraries ptf77blas,ptcblas,atlas not found in /usr/lib
      <class 'numpy.distutils.system_info.atlas_threads_info'>
        NOT AVAILABLE
  
      atlas_info:
      customize UnixCCompiler
        libraries lapack_atlas not found in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib
      customize UnixCCompiler
        libraries f77blas,cblas,atlas not found in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib
      customize UnixCCompiler
        libraries lapack_atlas not found in /usr/local/lib
      customize UnixCCompiler
        libraries f77blas,cblas,atlas not found in /usr/local/lib
      customize UnixCCompiler
        libraries lapack_atlas not found in /usr/lib
      customize UnixCCompiler
        libraries f77blas,cblas,atlas not found in /usr/lib
      <class 'numpy.distutils.system_info.atlas_info'>
        NOT AVAILABLE
  
      accelerate_info:
      customize UnixCCompiler
        libraries accelerate not found in ['/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib', '/usr/local/lib', '/usr/lib']
      Library accelerate was not found. Ignoring
      customize UnixCCompiler
        libraries veclib not found in ['/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib', '/usr/local/lib', '/usr/lib']
      Library veclib was not found. Ignoring
        FOUND:
          extra_compile_args = ['-faltivec', '-I/System/Library/Frameworks/vecLib.framework/Headers']
          extra_link_args = ['-Wl,-framework', '-Wl,Accelerate']
          define_macros = [('NO_ATLAS_INFO', 3), ('HAVE_CBLAS', None)]
  
        FOUND:
          extra_compile_args = ['-faltivec', '-I/System/Library/Frameworks/vecLib.framework/Headers']
          extra_link_args = ['-Wl,-framework', '-Wl,Accelerate']
          define_macros = [('NO_ATLAS_INFO', 3), ('HAVE_CBLAS', None)]
  
      /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/distutils/dist.py:274: UserWarning: Unknown distribution option: 'define_macros'
        warnings.warn(msg)
      running install
      running build
      running config_cc
      unifing config_cc, config, build_clib, build_ext, build commands --compiler options
      running config_fc
      unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options
      running build_src
      build_src
      building py_modules sources
      building library ""npymath"" sources
      get_default_fcompiler: matching types: '['gnu95', 'nag', 'absoft', 'ibm', 'intel', 'gnu', 'g95', 'pg']'
      customize Gnu95FCompiler
      Found executable /usr/local/bin/gfortran
      customize Gnu95FCompiler
      customize Gnu95FCompiler using config
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64
  
      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/include/python3.8 -c'
      clang: _configtest.c
      clang _configtest.o -o _configtest
      success!
      removing: _configtest.c _configtest.o _configtest.o.d _configtest
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64
  
      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/include/python3.8 -c'
      clang: _configtest.c
      _configtest.c:1:5: warning: incompatible redeclaration of library function 'exp' [-Wincompatible-library-redeclaration]
      int exp (void);
          ^
      _configtest.c:1:5: note: 'exp' is a builtin with type 'double (double)'
      1 warning generated.
      _configtest.c:1:5: warning: incompatible redeclaration of library function 'exp' [-Wincompatible-library-redeclaration]
      int exp (void);
          ^
      _configtest.c:1:5: note: 'exp' is a builtin with type 'double (double)'
      1 warning generated.
      clang _configtest.o -o _configtest
      success!
      removing: _configtest.c _configtest.o _configtest.o.d _configtest
        adding 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath' to include_dirs.
      None - nothing done with h_files = ['build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath/npy_math_internal.h']
      building library ""npysort"" sources
        adding 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common' to include_dirs.
      None - nothing done with h_files = ['build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common/npy_sort.h', 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common/npy_partition.h', 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common/npy_binsearch.h']
      building extension ""numpy.core._dummy"" sources
        adding 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy/config.h' to sources.
        adding 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy/_numpyconfig.h' to sources.
      executing numpy/core/code_generators/generate_numpy_api.py
        adding 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy/__multiarray_api.h' to sources.
      numpy.core - nothing done with h_files = ['build/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy/config.h', 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy/_numpyconfig.h', 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy/__multiarray_api.h']
      building extension ""numpy.core._multiarray_tests"" sources
      building extension ""numpy.core._multiarray_umath"" sources
        adding 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy/config.h' to sources.
        adding 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy/_numpyconfig.h' to sources.
      executing numpy/core/code_generators/generate_numpy_api.py
        adding 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy/__multiarray_api.h' to sources.
      executing numpy/core/code_generators/generate_ufunc_api.py
        adding 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy/__ufunc_api.h' to sources.
        adding 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath' to include_dirs.
        adding 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath' to include_dirs.
        adding 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common' to include_dirs.
      numpy.core - nothing done with h_files = ['build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/funcs.inc', 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/simd.inc', 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/loops.h', 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/matmul.h', 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/clip.h', 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath/npy_math_internal.h', 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common/templ_common.h', 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy/config.h', 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy/_numpyconfig.h', 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy/__multiarray_api.h', 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy/__ufunc_api.h']
      building extension ""numpy.core._umath_tests"" sources
      building extension ""numpy.core._rational_tests"" sources
      building extension ""numpy.core._struct_ufunc_tests"" sources
      building extension ""numpy.core._operand_flag_tests"" sources
      building extension ""numpy.fft._pocketfft_internal"" sources
      building extension ""numpy.linalg.lapack_lite"" sources
        adding 'numpy/linalg/lapack_lite/python_xerbla.c' to sources.
      building extension ""numpy.linalg._umath_linalg"" sources
        adding 'numpy/linalg/lapack_lite/python_xerbla.c' to sources.
      building extension ""numpy.random.mt19937"" sources
      building extension ""numpy.random.philox"" sources
      building extension ""numpy.random.pcg64"" sources
      building extension ""numpy.random.sfc64"" sources
      building extension ""numpy.random.common"" sources
      building extension ""numpy.random.bit_generator"" sources
      building extension ""numpy.random.generator"" sources
      building extension ""numpy.random.bounded_integers"" sources
      building extension ""numpy.random.mtrand"" sources
      building data_files sources
      build_src: building npy-pkg config files
      running build_py
      copying numpy/version.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy
      copying build/src.macosx-10.14.6-arm64-3.8/numpy/__config__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy
      copying build/src.macosx-10.14.6-arm64-3.8/numpy/distutils/__config__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils
      running build_clib
      customize UnixCCompiler
      customize UnixCCompiler using build_clib
      running build_ext
      customize UnixCCompiler
      customize UnixCCompiler using build_ext
      building 'numpy.linalg.lapack_lite' extension
      compiling C sources
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64
  
      compile options: '-DNO_ATLAS_INFO=3 -DHAVE_CBLAS -Inumpy/core/include -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/include/python3.8 -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -c'
      extra options: '-faltivec -I/System/Library/Frameworks/vecLib.framework/Headers'
      clang: numpy/linalg/lapack_lite/python_xerbla.c
      clang: numpy/linalg/lapack_litemodule.c
      clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly
      clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly
      clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly
      clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly
      error: Command ""clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64 -DNO_ATLAS_INFO=3 -DHAVE_CBLAS -Inumpy/core/include -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/include/python3.8 -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -c numpy/linalg/lapack_lite/python_xerbla.c -o build/temp.macosx-10.14.6-arm64-3.8/numpy/linalg/lapack_lite/python_xerbla.o -MMD -MF build/temp.macosx-10.14.6-arm64-3.8/numpy/linalg/lapack_lite/python_xerbla.o.d -faltivec -I/System/Library/Frameworks/vecLib.framework/Headers"" failed with exit status 1
      ----------------------------------------
  ERROR: Command errored out with exit status 1: /Library/Developer/CommandLineTools/usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/pip-install-wbzj2i_m/numpy/setup.py'""'""'; __file__='""'""'/private/var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/pip-install-wbzj2i_m/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/pip-record-7i3k2v9o/install-record.txt --single-version-externally-managed --prefix /private/var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/pip-build-env-3upezx90/overlay --compile --install-headers /private/var/folders/qx/xnxcrrlj4p52w9zy976f5prw0000gn/T/pip-build-env-3upezx90/overlay/include/python3.8/numpy Check the logs for full command output.
  ----------------------------------------`
```

</details>",2020-11-20T15:29:50Z
733660900,"So I didn't realize that his used X86_64. Mine is native arm64.

I do get a similar linker warning:

`
ld: warning: ignoring file /opt/OpenBLAS/lib/libopenblas.dylib, building for macOS-x86_64 but attempting to link with file built for macOS-arm64
`
But note that mine does not say symbols not found for arm64. 

From my OpenBLAS compile:

> OpenBLAS build complete. (BLAS CBLAS)
> 
>   OS               ... Darwin
>   Architecture     ... arm64
>   BINARY           ... 64bit
>   C compiler       ... CLANG  (cmd & version : Apple clang version 12.0.0 (clang-1200.0.32.27))
> -n   Library Name     ... libopenblas_armv8p-r0.3.12.dev.a
>  (Multi-threading; Max num-threads is 8)
> WARNING: If you plan to use the dynamic library libopenblas_armv8p-r0.3.12.dev.dylib, you must run:
> 
> ""make PREFIX=/your_installation_path/ install"".

I followed this with a ""make PREFIX=/opt/OpenBLAS/ install"", and uncommented the appropriate lines in numpy's site.cfg.

Complete output (badly formatted) below:

Thanks for your help - I'll try to keep poking at this, but since I don't have a ton of experience with the guts of numpy, any hints on where to look next would speed the process up tremendously. 

<details>

```
> ERROR: Command errored out with exit status 1:
>      command: /Library/Developer/CommandLineTools/usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/pip-req-build-gxz41fjw/setup.py'""'""'; __file__='""'""'/private/var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/pip-req-build-gxz41fjw/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/pip-record-_duorvcl/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers /Users/jeff/Library/Python/3.8/include/python3.8/numpy
>          cwd: /private/var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/pip-req-build-gxz41fjw/
>     Complete output (1218 lines):
>     Running from numpy source directory.
> 
> 
>     
>     Note: if you need reliable uninstall behavior, then install
>     with pip instead of using `setup.py install`:
>     
>       - `pip install .`       (from a git repo or downloaded source
>                                release)
>       - `pip install numpy`   (last NumPy release on PyPi)
>     
>     
>     Cythonizing sources
>     Processing numpy/random/_bounded_integers.pxd.in
>     Processing numpy/random/_philox.pyx
>     Processing numpy/random/_bounded_integers.pyx.in
>     Processing numpy/random/_sfc64.pyx
>     Processing numpy/random/_mt19937.pyx
>     Processing numpy/random/bit_generator.pyx
>     Processing numpy/random/mtrand.pyx
>     Processing numpy/random/_generator.pyx
>     Processing numpy/random/_pcg64.pyx
>     Processing numpy/random/_common.pyx
>     blas_opt_info:
>     blas_mkl_info:
>     customize UnixCCompiler
>       libraries mkl_rt not found in ['/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib', '/usr/lib']
>       NOT AVAILABLE
>     
>     blis_info:
>       libraries blis not found in ['/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib', '/usr/lib']
>       NOT AVAILABLE
>     
>     openblas_info:
>     C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64
>     
>     creating /var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpw14iss19/var
>     creating /var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpw14iss19/var/folders
>     creating /var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpw14iss19/var/folders/rr
>     creating /var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpw14iss19/var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn
>     creating /var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpw14iss19/var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T
>     creating /var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpw14iss19/var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpw14iss19
>     compile options: '-c'
>     clang: /var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpw14iss19/source.c
>     clang /var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpw14iss19/var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpw14iss19/source.o -L/opt/OpenBLAS/lib -lopenblas -o /var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpw14iss19/a.out
>       FOUND:
>         libraries = ['openblas', 'openblas']
>         library_dirs = ['/opt/OpenBLAS/lib']
>         language = c
>         define_macros = [('HAVE_CBLAS', None)]
>         runtime_library_dirs = ['/opt/OpenBLAS/lib']
>     
>       FOUND:
>         libraries = ['openblas', 'openblas']
>         library_dirs = ['/opt/OpenBLAS/lib']
>         language = c
>         define_macros = [('HAVE_CBLAS', None)]
>         runtime_library_dirs = ['/opt/OpenBLAS/lib']
>     
>     lapack_opt_info:
>     lapack_mkl_info:
>       libraries mkl_rt not found in ['/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib', '/usr/lib']
>       NOT AVAILABLE
>     
>     openblas_lapack_info:
>     C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64
>     
>     creating /var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpb94nqqqe/var
>     creating /var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpb94nqqqe/var/folders
>     creating /var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpb94nqqqe/var/folders/rr
>     creating /var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpb94nqqqe/var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn
>     creating /var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpb94nqqqe/var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T
>     creating /var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpb94nqqqe/var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpb94nqqqe
>     compile options: '-c'
>     clang: /var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpb94nqqqe/source.c
>     clang /var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpb94nqqqe/var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpb94nqqqe/source.o -L/opt/OpenBLAS/lib -lopenblas -o /var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpb94nqqqe/a.out
>     Undefined symbols for architecture arm64:
>       ""_zungqr_"", referenced from:
>           _main in source.o
>     ld: symbol(s) not found for architecture arm64
>     clang: error: linker command failed with exit code 1 (use -v to see invocation)
>       NOT AVAILABLE
>     
>     openblas_clapack_info:
>     C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64
>     
>     creating /var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpze0c9tzm/var
>     creating /var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpze0c9tzm/var/folders
>     creating /var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpze0c9tzm/var/folders/rr
>     creating /var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpze0c9tzm/var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn
>     creating /var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpze0c9tzm/var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T
>     creating /var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpze0c9tzm/var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpze0c9tzm
>     compile options: '-c'
>     clang: /var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpze0c9tzm/source.c
>     clang /var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpze0c9tzm/var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpze0c9tzm/source.o -L/opt/OpenBLAS/lib -lopenblas -o /var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/tmpze0c9tzm/a.out
>     Undefined symbols for architecture arm64:
>       ""_zungqr_"", referenced from:
>           _main in source.o
>     ld: symbol(s) not found for architecture arm64
>     clang: error: linker command failed with exit code 1 (use -v to see invocation)
>       NOT AVAILABLE
>     
>     flame_info:
>       libraries flame not found in ['/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib', '/usr/lib']
>       NOT AVAILABLE
>     
>     atlas_3_10_threads_info:
>     Setting PTATLAS=ATLAS
>       libraries lapack_atlas not found in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib
>       libraries tatlas,tatlas not found in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib
>       libraries lapack_atlas not found in /usr/lib
>       libraries tatlas,tatlas not found in /usr/lib
>     <class 'numpy.distutils.system_info.atlas_3_10_threads_info'>
>       NOT AVAILABLE
>     
>     atlas_3_10_info:
>       libraries lapack_atlas not found in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib
>       libraries satlas,satlas not found in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib
>       libraries lapack_atlas not found in /usr/lib
>       libraries satlas,satlas not found in /usr/lib
>     <class 'numpy.distutils.system_info.atlas_3_10_info'>
>       NOT AVAILABLE
>     
>     atlas_threads_info:
>     Setting PTATLAS=ATLAS
>       libraries lapack_atlas not found in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib
>       libraries ptf77blas,ptcblas,atlas not found in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib
>       libraries lapack_atlas not found in /usr/lib
>       libraries ptf77blas,ptcblas,atlas not found in /usr/lib
>     <class 'numpy.distutils.system_info.atlas_threads_info'>
>       NOT AVAILABLE
>     
>     atlas_info:
>       libraries lapack_atlas not found in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib
>       libraries f77blas,cblas,atlas not found in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib
>       libraries lapack_atlas not found in /usr/lib
>       libraries f77blas,cblas,atlas not found in /usr/lib
>     <class 'numpy.distutils.system_info.atlas_info'>
>       NOT AVAILABLE
>     
>     accelerate_info:
>       libraries accelerate not found in ['/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib', '/usr/lib']
>     Library accelerate was not found. Ignoring
>       libraries veclib not found in ['/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib', '/usr/lib']
>     Library veclib was not found. Ignoring
>       FOUND:
>         extra_compile_args = ['-faltivec', '-I/System/Library/Frameworks/vecLib.framework/Headers']
>         extra_link_args = ['-Wl,-framework', '-Wl,Accelerate']
>         define_macros = [('NO_ATLAS_INFO', 3), ('HAVE_CBLAS', None)]
>     
>       FOUND:
>         extra_compile_args = ['-faltivec', '-I/System/Library/Frameworks/vecLib.framework/Headers']
>         extra_link_args = ['-Wl,-framework', '-Wl,Accelerate']
>         define_macros = [('NO_ATLAS_INFO', 3), ('HAVE_CBLAS', None)]
>     
>     /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/distutils/dist.py:274: UserWarning: Unknown distribution option: 'define_macros'
>       warnings.warn(msg)
>     running install
>     running build
>     running config_cc
>     unifing config_cc, config, build_clib, build_ext, build commands --compiler options
>     running config_fc
>     unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options
>     running build_src
>     build_src
>     building py_modules sources
>     creating build
>     creating build/src.macosx-10.14.6-arm64-3.8
>     creating build/src.macosx-10.14.6-arm64-3.8/numpy
>     creating build/src.macosx-10.14.6-arm64-3.8/numpy/distutils
>     building library ""npymath"" sources
>     Could not locate executable gfortran
>     Could not locate executable f95
>     Could not locate executable f90
>     Could not locate executable f77
>     Could not locate executable xlf90
>     Could not locate executable xlf
>     Could not locate executable ifort
>     Could not locate executable ifc
>     Could not locate executable g77
>     Could not locate executable g95
>     Could not locate executable pgfortran
>     don't know how to compile Fortran code on platform 'posix'
>     creating build/src.macosx-10.14.6-arm64-3.8/numpy/core
>     creating build/src.macosx-10.14.6-arm64-3.8/numpy/core/src
>     creating build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath/npy_math_internal.h
>       adding 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath' to include_dirs.
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath/ieee754.c
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath/npy_math_complex.c
>     None - nothing done with h_files = ['build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath/npy_math_internal.h']
>     building library ""npysort"" sources
>     creating build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common/npy_sort.h
>       adding 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common' to include_dirs.
>     creating build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npysort
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npysort/quicksort.c
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npysort/mergesort.c
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npysort/timsort.c
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npysort/heapsort.c
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npysort/radixsort.c
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common/npy_partition.h
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npysort/selection.c
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common/npy_binsearch.h
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npysort/binsearch.c
>     None - nothing done with h_files = ['build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common/npy_sort.h', 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common/npy_partition.h', 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common/npy_binsearch.h']
>     building library ""npyrandom"" sources
>     building extension ""numpy.core._multiarray_tests"" sources
>     creating build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/_multiarray_tests.c
>     building extension ""numpy.core._multiarray_umath"" sources
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/arraytypes.c
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/einsum.c
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/lowlevel_strided_loops.c
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/nditer_templ.c
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/scalartypes.c
>     creating build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/funcs.inc
>       adding 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath' to include_dirs.
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/simd.inc
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/loops.h
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/loops.c
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/matmul.h
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/matmul.c
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/clip.h
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/clip.c
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/scalarmath.c
>       adding 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath' to include_dirs.
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common/templ_common.h
>       adding 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common' to include_dirs.
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common/npy_cpu_features.c
>     numpy.core - nothing done with h_files = ['build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/funcs.inc', 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/simd.inc', 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/loops.h', 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/matmul.h', 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/clip.h', 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath/npy_math_internal.h', 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common/templ_common.h', 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy/config.h', 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy/_numpyconfig.h', 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy/__multiarray_api.h', 'build/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy/__ufunc_api.h']
>     building extension ""numpy.core._umath_tests"" sources
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/_umath_tests.c
>     building extension ""numpy.core._rational_tests"" sources
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/_rational_tests.c
>     building extension ""numpy.core._struct_ufunc_tests"" sources
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/_struct_ufunc_tests.c
>     building extension ""numpy.core._operand_flag_tests"" sources
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/_operand_flag_tests.c
>     building extension ""numpy.fft._pocketfft_internal"" sources
>     building extension ""numpy.linalg.lapack_lite"" sources
>     creating build/src.macosx-10.14.6-arm64-3.8/numpy/linalg
>     building extension ""numpy.linalg._umath_linalg"" sources
>     conv_template:> build/src.macosx-10.14.6-arm64-3.8/numpy/linalg/umath_linalg.c
>     building extension ""numpy.random._mt19937"" sources
>     building extension ""numpy.random._philox"" sources
>     building extension ""numpy.random._pcg64"" sources
>     building extension ""numpy.random._sfc64"" sources
>     building extension ""numpy.random._common"" sources
>     building extension ""numpy.random.bit_generator"" sources
>     building extension ""numpy.random._generator"" sources
>     building extension ""numpy.random._bounded_integers"" sources
>     building extension ""numpy.random.mtrand"" sources
>     building data_files sources
>     build_src: building npy-pkg config files
>     running build_py
>     creating build/lib.macosx-10.14.6-arm64-3.8
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy
>     copying numpy/conftest.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy
>     copying numpy/version.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy
>     copying numpy/_globals.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy
>     copying numpy/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy
>     copying numpy/dual.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy
>     copying numpy/_distributor_init.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy
>     copying numpy/setup.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy
>     copying numpy/ctypeslib.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy
>     copying numpy/matlib.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy
>     copying numpy/_pytesttester.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy
>     copying build/src.macosx-10.14.6-arm64-3.8/numpy/__config__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/compat
>     copying numpy/compat/py3k.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/compat
>     copying numpy/compat/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/compat
>     copying numpy/compat/setup.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/compat
>     copying numpy/compat/_inspect.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/compat
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/compat/tests
>     copying numpy/compat/tests/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/compat/tests
>     copying numpy/compat/tests/test_compat.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/compat/tests
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/umath.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/fromnumeric.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/_dtype.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/_add_newdocs.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/_methods.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/_internal.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/_string_helpers.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/multiarray.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/_asarray.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/records.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/setup_common.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/memmap.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/overrides.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/getlimits.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/_dtype_ctypes.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/defchararray.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/shape_base.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/machar.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/setup.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/numeric.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/function_base.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/einsumfunc.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/umath_tests.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/_ufunc_config.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/_exceptions.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/numerictypes.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/_type_aliases.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/cversions.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/arrayprint.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     copying numpy/core/code_generators/generate_numpy_api.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_numerictypes.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_scalar_methods.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_scalarmath.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_item_selection.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_machar.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_unicode.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_arrayprint.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_scalarbuffer.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_indexerrors.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_print.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_half.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_mem_overlap.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_shape_base.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_deprecations.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_errstate.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_records.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_scalarinherit.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_indexing.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_umath.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_numeric.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_function_base.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_datetime.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test__exceptions.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_extint128.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_umath_complex.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/_locales.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_defchararray.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_conversion_utils.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_scalarprint.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_abc.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_ufunc.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_dtype.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_umath_accuracy.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_getlimits.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_einsum.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_api.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_longdouble.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_overrides.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_scalar_ctors.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_multiarray.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_memmap.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_nditer.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_cpu_features.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_protocols.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     copying numpy/core/tests/test_regression.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/core/tests
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils
>     copying numpy/distutils/unixccompiler.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils
>     copying numpy/distutils/numpy_distribution.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils
>     copying numpy/distutils/conv_template.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils
>     copying numpy/distutils/cpuinfo.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils
>     copying numpy/distutils/ccompiler.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils
>     copying numpy/distutils/msvc9compiler.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils
>     copying numpy/distutils/npy_pkg_config.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils
>     copying numpy/distutils/misc_util.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils
>     copying numpy/distutils/log.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils
>     copying numpy/distutils/line_endings.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils
>     copying numpy/distutils/lib2def.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils
>     copying numpy/distutils/pathccompiler.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils
>     copying numpy/distutils/system_info.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils
>     copying numpy/distutils/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils
>     copying numpy/distutils/core.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils
>     copying numpy/distutils/exec_command.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils
>     copying numpy/distutils/from_template.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils
>     copying numpy/distutils/mingw32ccompiler.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils
>     copying numpy/distutils/setup.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils
>     copying numpy/distutils/extension.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils
>     copying numpy/distutils/msvccompiler.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils
>     copying numpy/distutils/intelccompiler.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils
>     copying numpy/distutils/_shell_utils.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils
>     copying build/src.macosx-10.14.6-arm64-3.8/numpy/distutils/__config__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/command
>     copying numpy/distutils/command/build.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/command
>     copying numpy/distutils/command/config_compiler.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/command
>     copying numpy/distutils/command/build_ext.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/command
>     copying numpy/distutils/command/config.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/command
>     copying numpy/distutils/command/install_headers.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/command
>     copying numpy/distutils/command/build_py.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/command
>     copying numpy/distutils/command/build_src.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/command
>     copying numpy/distutils/command/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/command
>     copying numpy/distutils/command/sdist.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/command
>     copying numpy/distutils/command/build_scripts.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/command
>     copying numpy/distutils/command/bdist_rpm.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/command
>     copying numpy/distutils/command/install_clib.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/command
>     copying numpy/distutils/command/build_clib.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/command
>     copying numpy/distutils/command/autodist.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/command
>     copying numpy/distutils/command/egg_info.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/command
>     copying numpy/distutils/command/install.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/command
>     copying numpy/distutils/command/develop.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/command
>     copying numpy/distutils/command/install_data.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/command
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/fcompiler
>     copying numpy/distutils/fcompiler/gnu.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/fcompiler
>     copying numpy/distutils/fcompiler/compaq.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/fcompiler
>     copying numpy/distutils/fcompiler/intel.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/fcompiler
>     copying numpy/distutils/fcompiler/none.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/fcompiler
>     copying numpy/distutils/fcompiler/nag.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/fcompiler
>     copying numpy/distutils/fcompiler/pg.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/fcompiler
>     copying numpy/distutils/fcompiler/ibm.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/fcompiler
>     copying numpy/distutils/fcompiler/sun.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/fcompiler
>     copying numpy/distutils/fcompiler/nv.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/fcompiler
>     copying numpy/distutils/fcompiler/lahey.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/fcompiler
>     copying numpy/distutils/fcompiler/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/fcompiler
>     copying numpy/distutils/fcompiler/g95.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/fcompiler
>     copying numpy/distutils/fcompiler/mips.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/fcompiler
>     copying numpy/distutils/fcompiler/hpux.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/fcompiler
>     copying numpy/distutils/fcompiler/environment.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/fcompiler
>     copying numpy/distutils/fcompiler/pathf95.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/fcompiler
>     copying numpy/distutils/fcompiler/absoft.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/fcompiler
>     copying numpy/distutils/fcompiler/vast.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/fcompiler
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/tests
>     copying numpy/distutils/tests/test_system_info.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/tests
>     copying numpy/distutils/tests/test_mingw32ccompiler.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/tests
>     copying numpy/distutils/tests/test_from_template.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/tests
>     copying numpy/distutils/tests/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/tests
>     copying numpy/distutils/tests/test_fcompiler_intel.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/tests
>     copying numpy/distutils/tests/test_misc_util.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/tests
>     copying numpy/distutils/tests/test_fcompiler.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/tests
>     copying numpy/distutils/tests/test_shell_utils.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/tests
>     copying numpy/distutils/tests/test_exec_command.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/tests
>     copying numpy/distutils/tests/test_npy_pkg_config.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/tests
>     copying numpy/distutils/tests/test_fcompiler_nagfor.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/tests
>     copying numpy/distutils/tests/test_fcompiler_gnu.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/distutils/tests
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/doc
>     copying numpy/doc/misc.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/doc
>     copying numpy/doc/internals.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/doc
>     copying numpy/doc/creation.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/doc
>     copying numpy/doc/dispatch.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/doc
>     copying numpy/doc/constants.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/doc
>     copying numpy/doc/ufuncs.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/doc
>     copying numpy/doc/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/doc
>     copying numpy/doc/broadcasting.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/doc
>     copying numpy/doc/basics.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/doc
>     copying numpy/doc/subclassing.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/doc
>     copying numpy/doc/indexing.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/doc
>     copying numpy/doc/byteswapping.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/doc
>     copying numpy/doc/structured_arrays.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/doc
>     copying numpy/doc/glossary.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/doc
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py
>     copying numpy/f2py/cfuncs.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py
>     copying numpy/f2py/common_rules.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py
>     copying numpy/f2py/crackfortran.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py
>     copying numpy/f2py/cb_rules.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py
>     copying numpy/f2py/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py
>     copying numpy/f2py/rules.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py
>     copying numpy/f2py/f2py2e.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py
>     copying numpy/f2py/func2subr.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py
>     copying numpy/f2py/__version__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py
>     copying numpy/f2py/diagnose.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py
>     copying numpy/f2py/setup.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py
>     copying numpy/f2py/capi_maps.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py
>     copying numpy/f2py/f90mod_rules.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py
>     copying numpy/f2py/f2py_testing.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py
>     copying numpy/f2py/use_rules.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py
>     copying numpy/f2py/auxfuncs.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py
>     copying numpy/f2py/__main__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py/tests
>     copying numpy/f2py/tests/test_mixed.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py/tests
>     copying numpy/f2py/tests/test_return_logical.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py/tests
>     copying numpy/f2py/tests/test_assumed_shape.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py/tests
>     copying numpy/f2py/tests/test_common.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py/tests
>     copying numpy/f2py/tests/test_kind.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py/tests
>     copying numpy/f2py/tests/test_array_from_pyobj.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py/tests
>     copying numpy/f2py/tests/test_return_real.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py/tests
>     copying numpy/f2py/tests/util.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py/tests
>     copying numpy/f2py/tests/test_size.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py/tests
>     copying numpy/f2py/tests/test_callback.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py/tests
>     copying numpy/f2py/tests/test_string.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py/tests
>     copying numpy/f2py/tests/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py/tests
>     copying numpy/f2py/tests/test_quoted_character.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py/tests
>     copying numpy/f2py/tests/test_parameter.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py/tests
>     copying numpy/f2py/tests/test_semicolon_split.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py/tests
>     copying numpy/f2py/tests/test_compile_function.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py/tests
>     copying numpy/f2py/tests/test_block_docstring.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py/tests
>     copying numpy/f2py/tests/test_return_integer.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py/tests
>     copying numpy/f2py/tests/test_return_character.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py/tests
>     copying numpy/f2py/tests/test_return_complex.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py/tests
>     copying numpy/f2py/tests/test_crackfortran.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py/tests
>     copying numpy/f2py/tests/test_regression.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/f2py/tests
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/fft
>     copying numpy/fft/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/fft
>     copying numpy/fft/setup.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/fft
>     copying numpy/fft/helper.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/fft
>     copying numpy/fft/_pocketfft.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/fft
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/fft/tests
>     copying numpy/fft/tests/test_pocketfft.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/fft/tests
>     copying numpy/fft/tests/test_helper.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/fft/tests
>     copying numpy/fft/tests/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/fft/tests
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/lib
>     copying numpy/lib/_iotools.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib
>     copying numpy/lib/mixins.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib
>     copying numpy/lib/nanfunctions.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib
>     copying numpy/lib/recfunctions.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib
>     copying numpy/lib/histograms.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib
>     copying numpy/lib/scimath.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib
>     copying numpy/lib/_version.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib
>     copying numpy/lib/user_array.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib
>     copying numpy/lib/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib
>     copying numpy/lib/format.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib
>     copying numpy/lib/twodim_base.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib
>     copying numpy/lib/financial.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib
>     copying numpy/lib/index_tricks.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib
>     copying numpy/lib/npyio.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib
>     copying numpy/lib/shape_base.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib
>     copying numpy/lib/setup.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib
>     copying numpy/lib/stride_tricks.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib
>     copying numpy/lib/utils.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib
>     copying numpy/lib/arrayterator.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib
>     copying numpy/lib/function_base.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib
>     copying numpy/lib/arraysetops.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib
>     copying numpy/lib/arraypad.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib
>     copying numpy/lib/type_check.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib
>     copying numpy/lib/polynomial.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib
>     copying numpy/lib/_datasource.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib
>     copying numpy/lib/ufunclike.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/lib/tests
>     copying numpy/lib/tests/test_type_check.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib/tests
>     copying numpy/lib/tests/test_utils.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib/tests
>     copying numpy/lib/tests/test_twodim_base.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib/tests
>     copying numpy/lib/tests/test_polynomial.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib/tests
>     copying numpy/lib/tests/test__iotools.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib/tests
>     copying numpy/lib/tests/test_shape_base.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib/tests
>     copying numpy/lib/tests/test_ufunclike.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib/tests
>     copying numpy/lib/tests/test_index_tricks.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib/tests
>     copying numpy/lib/tests/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib/tests
>     copying numpy/lib/tests/test_arrayterator.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib/tests
>     copying numpy/lib/tests/test__version.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib/tests
>     copying numpy/lib/tests/test_io.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib/tests
>     copying numpy/lib/tests/test_arraysetops.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib/tests
>     copying numpy/lib/tests/test_function_base.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib/tests
>     copying numpy/lib/tests/test_arraypad.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib/tests
>     copying numpy/lib/tests/test_mixins.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib/tests
>     copying numpy/lib/tests/test_packbits.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib/tests
>     copying numpy/lib/tests/test__datasource.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib/tests
>     copying numpy/lib/tests/test_stride_tricks.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib/tests
>     copying numpy/lib/tests/test_financial.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib/tests
>     copying numpy/lib/tests/test_recfunctions.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib/tests
>     copying numpy/lib/tests/test_nanfunctions.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib/tests
>     copying numpy/lib/tests/test_format.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib/tests
>     copying numpy/lib/tests/test_histograms.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib/tests
>     copying numpy/lib/tests/test_regression.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/lib/tests
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/linalg
>     copying numpy/linalg/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/linalg
>     copying numpy/linalg/setup.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/linalg
>     copying numpy/linalg/linalg.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/linalg
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/linalg/tests
>     copying numpy/linalg/tests/test_linalg.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/linalg/tests
>     copying numpy/linalg/tests/test_deprecations.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/linalg/tests
>     copying numpy/linalg/tests/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/linalg/tests
>     copying numpy/linalg/tests/test_build.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/linalg/tests
>     copying numpy/linalg/tests/test_regression.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/linalg/tests
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/ma
>     copying numpy/ma/extras.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/ma
>     copying numpy/ma/testutils.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/ma
>     copying numpy/ma/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/ma
>     copying numpy/ma/core.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/ma
>     copying numpy/ma/bench.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/ma
>     copying numpy/ma/setup.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/ma
>     copying numpy/ma/timer_comparison.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/ma
>     copying numpy/ma/mrecords.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/ma
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/ma/tests
>     copying numpy/ma/tests/test_old_ma.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/ma/tests
>     copying numpy/ma/tests/test_core.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/ma/tests
>     copying numpy/ma/tests/test_deprecations.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/ma/tests
>     copying numpy/ma/tests/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/ma/tests
>     copying numpy/ma/tests/test_subclassing.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/ma/tests
>     copying numpy/ma/tests/test_extras.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/ma/tests
>     copying numpy/ma/tests/test_mrecords.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/ma/tests
>     copying numpy/ma/tests/test_regression.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/ma/tests
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/matrixlib
>     copying numpy/matrixlib/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/matrixlib
>     copying numpy/matrixlib/setup.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/matrixlib
>     copying numpy/matrixlib/defmatrix.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/matrixlib
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/matrixlib/tests
>     copying numpy/matrixlib/tests/test_matrix_linalg.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/matrixlib/tests
>     copying numpy/matrixlib/tests/test_defmatrix.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/matrixlib/tests
>     copying numpy/matrixlib/tests/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/matrixlib/tests
>     copying numpy/matrixlib/tests/test_interaction.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/matrixlib/tests
>     copying numpy/matrixlib/tests/test_numeric.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/matrixlib/tests
>     copying numpy/matrixlib/tests/test_masked_matrix.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/matrixlib/tests
>     copying numpy/matrixlib/tests/test_multiarray.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/matrixlib/tests
>     copying numpy/matrixlib/tests/test_regression.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/matrixlib/tests
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/polynomial
>     copying numpy/polynomial/laguerre.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/polynomial
>     copying numpy/polynomial/_polybase.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/polynomial
>     copying numpy/polynomial/polyutils.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/polynomial
>     copying numpy/polynomial/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/polynomial
>     copying numpy/polynomial/setup.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/polynomial
>     copying numpy/polynomial/hermite_e.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/polynomial
>     copying numpy/polynomial/chebyshev.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/polynomial
>     copying numpy/polynomial/polynomial.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/polynomial
>     copying numpy/polynomial/legendre.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/polynomial
>     copying numpy/polynomial/hermite.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/polynomial
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/polynomial/tests
>     copying numpy/polynomial/tests/test_chebyshev.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/polynomial/tests
>     copying numpy/polynomial/tests/test_hermite_e.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/polynomial/tests
>     copying numpy/polynomial/tests/test_polynomial.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/polynomial/tests
>     copying numpy/polynomial/tests/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/polynomial/tests
>     copying numpy/polynomial/tests/test_laguerre.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/polynomial/tests
>     copying numpy/polynomial/tests/test_legendre.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/polynomial/tests
>     copying numpy/polynomial/tests/test_printing.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/polynomial/tests
>     copying numpy/polynomial/tests/test_hermite.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/polynomial/tests
>     copying numpy/polynomial/tests/test_classes.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/polynomial/tests
>     copying numpy/polynomial/tests/test_polyutils.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/polynomial/tests
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/random
>     copying numpy/random/_pickle.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/random
>     copying numpy/random/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/random
>     copying numpy/random/setup.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/random
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/random/tests
>     copying numpy/random/tests/test_generator_mt19937.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/random/tests
>     copying numpy/random/tests/test_randomstate.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/random/tests
>     copying numpy/random/tests/test_direct.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/random/tests
>     copying numpy/random/tests/test_extending.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/random/tests
>     copying numpy/random/tests/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/random/tests
>     copying numpy/random/tests/test_smoke.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/random/tests
>     copying numpy/random/tests/test_randomstate_regression.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/random/tests
>     copying numpy/random/tests/test_seed_sequence.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/random/tests
>     copying numpy/random/tests/test_generator_mt19937_regressions.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/random/tests
>     copying numpy/random/tests/test_random.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/random/tests
>     copying numpy/random/tests/test_regression.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/random/tests
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/testing
>     copying numpy/testing/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/testing
>     copying numpy/testing/setup.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/testing
>     copying numpy/testing/utils.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/testing
>     copying numpy/testing/print_coercion_tables.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/testing
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/testing/_private
>     copying numpy/testing/_private/nosetester.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/testing/_private
>     copying numpy/testing/_private/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/testing/_private
>     copying numpy/testing/_private/noseclasses.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/testing/_private
>     copying numpy/testing/_private/utils.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/testing/_private
>     copying numpy/testing/_private/parameterized.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/testing/_private
>     copying numpy/testing/_private/decorators.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/testing/_private
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/testing/tests
>     copying numpy/testing/tests/test_utils.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/testing/tests
>     copying numpy/testing/tests/test_decorators.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/testing/tests
>     copying numpy/testing/tests/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/testing/tests
>     copying numpy/testing/tests/test_doctesting.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/testing/tests
>     creating build/lib.macosx-10.14.6-arm64-3.8/numpy/tests
>     copying numpy/tests/test_warnings.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/tests
>     copying numpy/tests/test_matlib.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/tests
>     copying numpy/tests/test_ctypeslib.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/tests
>     copying numpy/tests/test_numpy_version.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/tests
>     copying numpy/tests/__init__.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/tests
>     copying numpy/tests/test_reloading.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/tests
>     copying numpy/tests/test_public_api.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/tests
>     copying numpy/tests/test_scripts.py -> build/lib.macosx-10.14.6-arm64-3.8/numpy/tests
>     running build_clib
>     customize UnixCCompiler
>     customize UnixCCompiler using new_build_clib
>     building 'npymath' library
>     compiling C sources
>     C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64
>     
>     creating build/temp.macosx-10.14.6-arm64-3.8
>     creating build/temp.macosx-10.14.6-arm64-3.8/numpy
>     creating build/temp.macosx-10.14.6-arm64-3.8/numpy/core
>     creating build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src
>     creating build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath
>     creating build/temp.macosx-10.14.6-arm64-3.8/build
>     creating build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8
>     creating build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy
>     creating build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core
>     creating build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src
>     creating build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath
>     compile options: '-Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -Inumpy/core/include -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/include/python3.8 -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -c'
>     clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath/ieee754.cclang: numpy/core/src/npymath/npy_math.c
>     
>     clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath/npy_math_complex.cclang: numpy/core/src/npymath/halffloat.c
>     
>     ar: adding 4 object files to build/temp.macosx-10.14.6-arm64-3.8/libnpymath.a
>     warning: /Library/Developer/CommandLineTools/usr/bin/ranlib: archive library: build/temp.macosx-10.14.6-arm64-3.8/libnpymath.a will be fat and ar(1) will not be able to operate on it
>     ranlib:@ build/temp.macosx-10.14.6-arm64-3.8/libnpymath.a
>     building 'npysort' library
>     compiling C sources
>     C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64
>     
>     creating build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npysort
>     compile options: '-Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Inumpy/core/include -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/include/python3.8 -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -c'
>     clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npysort/mergesort.c
>     clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npysort/quicksort.cclang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npysort/timsort.c
>     
>     clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npysort/radixsort.c
>     clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npysort/heapsort.c
>     clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npysort/selection.c
>     clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npysort/binsearch.c
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     22 warnings generated.
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
>             npy_intp k;
>             ^~~~~~~~~~~
>     numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
>         else if (0 && kth == num - 1) {
>                  ^
>                  /* DISABLES CODE */ ( )
>     22 warnings generated.
>     ar: adding 7 object files to build/temp.macosx-10.14.6-arm64-3.8/libnpysort.a
>     warning: /Library/Developer/CommandLineTools/usr/bin/ranlib: archive library: build/temp.macosx-10.14.6-arm64-3.8/libnpysort.a will be fat and ar(1) will not be able to operate on it
>     ranlib:@ build/temp.macosx-10.14.6-arm64-3.8/libnpysort.a
>     building 'npyrandom' library
>     compiling C sources
>     C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64
>     
>     creating build/temp.macosx-10.14.6-arm64-3.8/numpy/random
>     creating build/temp.macosx-10.14.6-arm64-3.8/numpy/random/src
>     creating build/temp.macosx-10.14.6-arm64-3.8/numpy/random/src/distributions
>     compile options: '-Inumpy/core/include -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/include/python3.8 -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -c'
>     clang: numpy/random/src/distributions/distributions.cclang: numpy/random/src/distributions/logfactorial.c
>     
>     clang: numpy/random/src/distributions/random_mvhg_count.c
>     clang: numpy/random/src/distributions/random_hypergeometric.c
>     clang: numpy/random/src/distributions/random_mvhg_marginals.c
>     ar: adding 5 object files to build/temp.macosx-10.14.6-arm64-3.8/libnpyrandom.a
>     warning: /Library/Developer/CommandLineTools/usr/bin/ranlib: archive library: build/temp.macosx-10.14.6-arm64-3.8/libnpyrandom.a will be fat and ar(1) will not be able to operate on it
>     ranlib:@ build/temp.macosx-10.14.6-arm64-3.8/libnpyrandom.a
>     running build_ext
>     customize UnixCCompiler
>     customize UnixCCompiler using new_build_ext
>     building 'numpy.core._multiarray_tests' extension
>     compiling C sources
>     C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64
>     
>     creating build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray
>     creating build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/common
>     compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/include/python3.8 -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -c'
>     clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/_multiarray_tests.c
>     clang: numpy/core/src/common/mem_overlap.c
>     clang -bundle -undefined dynamic_lookup -Wl,-headerpad,0x1000 -arch arm64 -arch x86_64 build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/_multiarray_tests.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/common/mem_overlap.o -Lbuild/temp.macosx-10.14.6-arm64-3.8 -lnpymath -o build/lib.macosx-10.14.6-arm64-3.8/numpy/core/_multiarray_tests.cpython-38-darwin.so
>     building 'numpy.core._multiarray_umath' extension
>     compiling C sources
>     C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64
>     
>     creating build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray
>     creating build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/umath
>     creating build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath
>     creating build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common
>     compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -DHAVE_CBLAS -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Inumpy/core/include -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/include/python3.8 -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -c'
>     clang: numpy/core/src/multiarray/alloc.c
>     clang: numpy/core/src/multiarray/array_assign_scalar.c
>     clang: numpy/core/src/multiarray/buffer.c
>     clang: numpy/core/src/multiarray/common.c
>     clang: numpy/core/src/multiarray/datetime_strings.c
>     clang: numpy/core/src/multiarray/conversion_utils.c
>     clang: numpy/core/src/multiarray/descriptor.c
>     clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/einsum.c
>     numpy/core/src/multiarray/einsum.c.src:2158:32: warning: unknown warning group '-Wmaybe-uninitialized', ignored [-Wunknown-warning-option]
>     #pragma GCC diagnostic ignored ""-Wmaybe-uninitialized""
>                                    ^
>     clang: numpy/core/src/multiarray/arrayobject.c
>     clang: numpy/core/src/multiarray/array_assign_array.c
>     clang: numpy/core/src/multiarray/ctors.c
>     clang: numpy/core/src/multiarray/convert.c
>     clang: numpy/core/src/multiarray/datetime_busday.c
>     clang: numpy/core/src/multiarray/arrayfunction_override.c
>     clang: numpy/core/src/multiarray/calculation.c
>     clang: numpy/core/src/multiarray/convert_datatype.c
>     clang: numpy/core/src/multiarray/hashdescr.c
>     clang: numpy/core/src/multiarray/datetime_busdaycal.c
>     clang: numpy/core/src/multiarray/item_selection.c
>     clang: numpy/core/src/multiarray/dragon4.c
>     clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/arraytypes.c
>     clang: numpy/core/src/multiarray/compiled_base.c
>     clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/lowlevel_strided_loops.c
>     clang: numpy/core/src/multiarray/multiarraymodule.c
>     clang: numpy/core/src/multiarray/datetime.c
>     clang: numpy/core/src/multiarray/dtype_transfer.c
>     clang: numpy/core/src/multiarray/nditer_constr.c
>     clang: numpy/core/src/multiarray/iterators.c
>     clang: numpy/core/src/multiarray/refcount.c
>     1 warning generated.
>     numpy/core/src/multiarray/einsum.c.src:2158:32: warning: unknown warning group '-Wmaybe-uninitialized', ignored [-Wunknown-warning-option]
>     #pragma GCC diagnostic ignored ""-Wmaybe-uninitialized""
>                                    ^
>     clang: numpy/core/src/multiarray/sequence.c
>     clang: numpy/core/src/multiarray/shape.c
>     clang: numpy/core/src/multiarray/scalarapi.c
>     clang: numpy/core/src/multiarray/temp_elide.c
>     clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/scalartypes.c
>     clang: numpy/core/src/multiarray/typeinfo.c
>     clang: numpy/core/src/multiarray/vdot.c
>     clang: numpy/core/src/multiarray/usertypes.c
>     clang: numpy/core/src/umath/umathmodule.c
>     clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/nditer_templ.c
>     clang: numpy/core/src/multiarray/nditer_pywrap.c
>     clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/loops.c
>     clang: numpy/core/src/umath/reduction.c
>     clang: numpy/core/src/umath/ufunc_object.c
>     clang: numpy/core/src/multiarray/number.c
>     numpy/core/src/umath/loops.c.src:2611:22: warning: code will never be executed [-Wunreachable-code]
>             npy_intp n = dimensions[0];
>                          ^~~~~~~~~~
>     numpy/core/src/umath/loops.c.src:2610:29: note: silence by adding parentheses to mark code as explicitly dead
>         if (IS_BINARY_REDUCE && 0) {
>                                 ^
>                                 /* DISABLES CODE */ ( )
>     numpy/core/src/umath/loops.c.src:2611:22: warning: code will never be executed [-Wunreachable-code]
>             npy_intp n = dimensions[0];
>                          ^~~~~~~~~~
>     numpy/core/src/umath/loops.c.src:2610:29: note: silence by adding parentheses to mark code as explicitly dead
>         if (IS_BINARY_REDUCE && 0) {
>                                 ^
>                                 /* DISABLES CODE */ ( )
>     numpy/core/src/umath/loops.c.src:2611:22: warning: code will never be executed [-Wunreachable-code]
>             npy_intp n = dimensions[0];
>                          ^~~~~~~~~~
>     numpy/core/src/umath/loops.c.src:2610:29: note: silence by adding parentheses to mark code as explicitly dead
>         if (IS_BINARY_REDUCE && 0) {
>                                 ^
>                                 /* DISABLES CODE */ ( )
>     clang: numpy/core/src/multiarray/strfuncs.c
>     clang: numpy/core/src/umath/ufunc_type_resolution.c
>     clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath/ieee754.c
>     clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath/npy_math_complex.c
>     clang: numpy/core/src/npymath/halffloat.c
>     1 warning generated.
>     clang: numpy/core/src/multiarray/flagsobject.c
>     clang: numpy/core/src/umath/override.c
>     clang: numpy/core/src/common/array_assign.c
>     clang: numpy/core/src/multiarray/nditer_api.c
>     clang: numpy/core/src/multiarray/getset.c
>     clang: numpy/core/src/common/mem_overlap.c
>     clang: numpy/core/src/npymath/npy_math.c
>     clang: numpy/core/src/common/ucsnarrow.c
>     clang: numpy/core/src/common/ufunc_override.c
>     clang: numpy/core/src/umath/extobj.c
>     clang: numpy/core/src/common/npy_longdouble.c
>     clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common/npy_cpu_features.c
>     clang: numpy/core/src/common/numpyos.c
>     clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/scalarmath.c
>     clang: numpy/core/src/common/cblasfuncs.c
>     clang: numpy/core/src/common/python_xerbla.c
>     clang: numpy/core/src/multiarray/mapping.c
>     clang: numpy/core/src/multiarray/methods.c
>     3 warnings generated.
>     numpy/core/src/umath/loops.c.src:2611:22: warning: code will never be executed [-Wunreachable-code]
>             npy_intp n = dimensions[0];
>                          ^~~~~~~~~~
>     numpy/core/src/umath/loops.c.src:2610:29: note: silence by adding parentheses to mark code as explicitly dead
>         if (IS_BINARY_REDUCE && 0) {
>                                 ^
>                                 /* DISABLES CODE */ ( )
>     numpy/core/src/umath/loops.c.src:2611:22: warning: code will never be executed [-Wunreachable-code]
>             npy_intp n = dimensions[0];
>                          ^~~~~~~~~~
>     numpy/core/src/umath/loops.c.src:2610:29: note: silence by adding parentheses to mark code as explicitly dead
>         if (IS_BINARY_REDUCE && 0) {
>                                 ^
>                                 /* DISABLES CODE */ ( )
>     numpy/core/src/umath/loops.c.src:2611:22: warning: code will never be executed [-Wunreachable-code]
>             npy_intp n = dimensions[0];
>                          ^~~~~~~~~~
>     numpy/core/src/umath/loops.c.src:2610:29: note: silence by adding parentheses to mark code as explicitly dead
>         if (IS_BINARY_REDUCE && 0) {
>                                 ^
>                                 /* DISABLES CODE */ ( )
>     3 warnings generated.
>     clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/matmul.c
>     clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/clip.c
>     clang -bundle -undefined dynamic_lookup -Wl,-headerpad,0x1000 -arch arm64 -arch x86_64 build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/alloc.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/arrayobject.o build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/arraytypes.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/array_assign_scalar.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/array_assign_array.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/arrayfunction_override.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/buffer.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/calculation.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/compiled_base.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/common.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/convert.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/convert_datatype.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/conversion_utils.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/ctors.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/datetime.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/datetime_strings.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/datetime_busday.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/datetime_busdaycal.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/descriptor.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/dragon4.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/dtype_transfer.o build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/einsum.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/flagsobject.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/getset.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/hashdescr.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/item_selection.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/iterators.o build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/lowlevel_strided_loops.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/mapping.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/methods.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/multiarraymodule.o build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/nditer_templ.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/nditer_api.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/nditer_constr.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/nditer_pywrap.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/number.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/refcount.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/sequence.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/shape.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/scalarapi.o build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/scalartypes.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/strfuncs.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/temp_elide.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/typeinfo.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/usertypes.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/multiarray/vdot.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/umathmodule.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/reduction.o build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/loops.o build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/matmul.o build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/clip.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/ufunc_object.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/extobj.o build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/scalarmath.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/ufunc_type_resolution.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/override.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath/npy_math.o build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath/ieee754.o build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath/npy_math_complex.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath/halffloat.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/common/array_assign.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/common/mem_overlap.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/common/npy_longdouble.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/common/ucsnarrow.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/common/ufunc_override.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/common/numpyos.o build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common/npy_cpu_features.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/common/cblasfuncs.o build/temp.macosx-10.14.6-arm64-3.8/numpy/core/src/common/python_xerbla.o -L/opt/OpenBLAS/lib -Lbuild/temp.macosx-10.14.6-arm64-3.8 -L/opt/OpenBLAS/lib -lnpymath -lnpysort -lopenblas -lopenblas -o build/lib.macosx-10.14.6-arm64-3.8/numpy/core/_multiarray_umath.cpython-38-darwin.so
>     ld: warning: ignoring file /opt/OpenBLAS/lib/libopenblas.dylib, building for macOS-x86_64 but attempting to link with file built for macOS-arm64
>     building 'numpy.core._umath_tests' extension
>     compiling C sources
>     C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64
>     
>     compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/include/python3.8 -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -c'
>     clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/_umath_tests.c
>     clang -bundle -undefined dynamic_lookup -Wl,-headerpad,0x1000 -arch arm64 -arch x86_64 build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/_umath_tests.o -Lbuild/temp.macosx-10.14.6-arm64-3.8 -o build/lib.macosx-10.14.6-arm64-3.8/numpy/core/_umath_tests.cpython-38-darwin.so
>     building 'numpy.core._rational_tests' extension
>     compiling C sources
>     C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64
>     
>     compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/include/python3.8 -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -c'
>     clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/_rational_tests.c
>     clang -bundle -undefined dynamic_lookup -Wl,-headerpad,0x1000 -arch arm64 -arch x86_64 build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/_rational_tests.o -Lbuild/temp.macosx-10.14.6-arm64-3.8 -o build/lib.macosx-10.14.6-arm64-3.8/numpy/core/_rational_tests.cpython-38-darwin.so
>     building 'numpy.core._struct_ufunc_tests' extension
>     compiling C sources
>     C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64
>     
>     compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/include/python3.8 -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -c'
>     clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/_struct_ufunc_tests.c
>     clang -bundle -undefined dynamic_lookup -Wl,-headerpad,0x1000 -arch arm64 -arch x86_64 build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/_struct_ufunc_tests.o -Lbuild/temp.macosx-10.14.6-arm64-3.8 -o build/lib.macosx-10.14.6-arm64-3.8/numpy/core/_struct_ufunc_tests.cpython-38-darwin.so
>     building 'numpy.core._operand_flag_tests' extension
>     compiling C sources
>     C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64
>     
>     compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/include/python3.8 -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -c'
>     clang: build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/_operand_flag_tests.c
>     clang -bundle -undefined dynamic_lookup -Wl,-headerpad,0x1000 -arch arm64 -arch x86_64 build/temp.macosx-10.14.6-arm64-3.8/build/src.macosx-10.14.6-arm64-3.8/numpy/core/src/umath/_operand_flag_tests.o -Lbuild/temp.macosx-10.14.6-arm64-3.8 -o build/lib.macosx-10.14.6-arm64-3.8/numpy/core/_operand_flag_tests.cpython-38-darwin.so
>     building 'numpy.fft._pocketfft_internal' extension
>     compiling C sources
>     C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64
>     
>     creating build/temp.macosx-10.14.6-arm64-3.8/numpy/fft
>     compile options: '-Inumpy/core/include -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/include/python3.8 -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -c'
>     clang: numpy/fft/_pocketfft.c
>     clang -bundle -undefined dynamic_lookup -Wl,-headerpad,0x1000 -arch arm64 -arch x86_64 build/temp.macosx-10.14.6-arm64-3.8/numpy/fft/_pocketfft.o -Lbuild/temp.macosx-10.14.6-arm64-3.8 -o build/lib.macosx-10.14.6-arm64-3.8/numpy/fft/_pocketfft_internal.cpython-38-darwin.so
>     building 'numpy.linalg.lapack_lite' extension
>     compiling C sources
>     C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64
>     
>     creating build/temp.macosx-10.14.6-arm64-3.8/numpy/linalg
>     creating build/temp.macosx-10.14.6-arm64-3.8/numpy/linalg/lapack_lite
>     compile options: '-DNO_ATLAS_INFO=3 -DHAVE_CBLAS -Inumpy/core/include -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/include/python3.8 -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -c'
>     extra options: '-faltivec -I/System/Library/Frameworks/vecLib.framework/Headers'
>     clang: numpy/linalg/lapack_litemodule.cclang: numpy/linalg/lapack_lite/python_xerbla.c
>     
>     clang: clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly
>     error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly
>     clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly
>     clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly
>     error: Command ""clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/Headers -arch arm64 -arch x86_64 -DNO_ATLAS_INFO=3 -DHAVE_CBLAS -Inumpy/core/include -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/include/python3.8 -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/common -Ibuild/src.macosx-10.14.6-arm64-3.8/numpy/core/src/npymath -c numpy/linalg/lapack_litemodule.c -o build/temp.macosx-10.14.6-arm64-3.8/numpy/linalg/lapack_litemodule.o -MMD -MF build/temp.macosx-10.14.6-arm64-3.8/numpy/linalg/lapack_litemodule.o.d -faltivec -I/System/Library/Frameworks/vecLib.framework/Headers"" failed with exit status 1
>     ----------------------------------------
> ERROR: Command errored out with exit status 1: /Library/Developer/CommandLineTools/usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/pip-req-build-gxz41fjw/setup.py'""'""'; __file__='""'""'/private/var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/pip-req-build-gxz41fjw/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/rr/b7gr0p9d5lzd2g2r5rbkfy180000gn/T/pip-record-_duorvcl/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers /Users/jeff/Library/Python/3.8/include/python3.8/numpy Check the logs for full command output.
```

</details>",2020-11-25T11:52:20Z
736272909,"Just ran into an issue with the inconsistency of the documentation for `PyArray_AsCArray`. I'm on 1.19.

The [documentation](https://numpy.org/doc/stable/reference/c-api/array.html#c.PyArray_AsCArray) says the signature is `int PyArray_AsCArray(PyObject** op, void* ptr, npy_intp* dims, int nd, int typenum, int itemsize)`, but I'm getting a compile error, and looking at [the code on GitHub](https://github.com/numpy/numpy/blob/v1.19.4/numpy/core/src/multiarray/multiarraymodule.c#L230-L231) that doesn't seem to be the case.",2020-12-01T07:14:25Z
736518952,"Note that this latter discussion is off-topic. The bug report is about making bincount work for `uint64`, an unsigned type. If you want it to work with negative numbers, you should perhaps open a new issue.",2020-12-01T12:23:02Z
754299396,"Can you try passing `dtype=object`, and see if the result fits in an int64?",2021-01-05T00:03:49Z
754306454,Will https://numpy.org/devdocs/reference/generated/numpy.lib.stride_tricks.sliding_window_view.html do what you're after (will be part of 1.20.0),2021-01-05T00:23:35Z
754338224,"@rgommers After changing to `A = np.random.rand(200, 100) + 0j`, the perf result remains the same, which means complex `matmul` is twice slower than non-complex.",2021-01-05T02:07:01Z
754351973,"I would guess that the relevant code change is probably array size.  NumPy doesn't have blocked iteration, so if you have out-of-cache sized arrays that makes a massive difference?",2021-01-05T02:47:07Z
754377755,"LGTM, Thanks @seiko2plus !",2021-01-05T04:02:25Z
754380859,whohoo. thanks. ,2021-01-05T04:12:19Z
754396818,"Sorry, I confused myself. This needs to be backported unfortunately. We may have more tests for the casts, but not for promotions (but then the main promotion tests are one huge test in `test_numeric` it is possible float16 ones were just never added.",2021-01-05T04:59:10Z
754441388,"Hello,

As for SLEEF, there are a couple of things to pay attention.

1. Beware of this bug : https://bugs.llvm.org/show_bug.cgi?id=47665  It seems that there are other bugs on clang, but I cannot reliably reproduce the problems. GCC seems okay, but gcc 10.2 cannot compile the quad library with SVE support due to ICE.
2. Arithmetic functions (add, sub, mul, and div) do not give perfectly correctly-rounded results. They have very small amount of error. Please tell me if this is problematic.
",2021-01-05T06:55:03Z
754465288,"@shibatch I didn't see your scalar version quadruple precision implementation, the vectorized version has an error bound like 0.5000000001 ULP, which is unbearable in numpy's precision standard, maybe scalar version can overcame this flaw.",2021-01-05T07:44:38Z
754474032,"> Then I had a thought... why am I trying to build 1.19.4 when I should be trying to build off the current source?

<details>

> ```
> % pip3 install . --no-binary :all: --no-use-pep517
> Defaulting to user installation because normal site-packages is not writeable
> Processing /Users/USERNAME/numpy
> Skipping wheel build for numpy, due to binaries being disabled for it.
> Installing collected packages: numpy
>     Running setup.py install for numpy ... done
> Successfully installed numpy-1.20.0.dev0+24a4704
> % python3
> Python 3.8.2 (default, Oct  2 2020, 10:45:41) 
> [Clang 12.0.0 (clang-1200.0.32.27)] on darwin
> Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
> >>> import numpy
> >>> 
> ```
> 
> So this is a problem that will, ultimately, work itself out. Any chance we could get an expedited release of 1.20?

I show this error in m1
 Running setup.py install for numpy ... error
    ERROR: Command errored out with exit status 1:
     command: /Applications/Xcode.app/Contents/Developer/usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/gy/jzs3xnwd1z3203d75y_31nxc0000gn/T/pip-req-build-6ap9n5ru/setup.py'""'""'; __file__='""'""'/private/var/folders/gy/jzs3xnwd1z3203d75y_31nxc0000gn/T/pip-req-build-6ap9n5ru/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/gy/jzs3xnwd1z3203d75y_31nxc0000gn/T/pip-record-g21yc5wb/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers /Users/sean/Library/Python/3.8/include/python3.8/numpy
         cwd: /private/var/folders/gy/jzs3xnwd1z3203d75y_31nxc0000gn/T/pip-req-build-6ap9n5ru/
    Complete output (41 lines):
    Running from numpy source directory.

    Note: if you need reliable uninstall behavior, then install
    with pip instead of using `setup.py install`:

      - `pip install .`       (from a git repo or downloaded source
                               release)
      - `pip install numpy`   (last NumPy release on PyPi)


    Cythonizing sources
    Processing numpy/random/_bounded_integers.pxd.in
    Processing numpy/random/_philox.pyx
    Traceback (most recent call last):
      File ""/private/var/folders/gy/jzs3xnwd1z3203d75y_31nxc0000gn/T/pip-req-build-6ap9n5ru/tools/cythonize.py"", line 59, in process_pyx
        from Cython.Compiler.Version import version as cython_version
    ModuleNotFoundError: No module named 'Cython'

    During handling of the above exception, another exception occurred:

    Traceback (most recent call last):
      File ""/private/var/folders/gy/jzs3xnwd1z3203d75y_31nxc0000gn/T/pip-req-build-6ap9n5ru/tools/cythonize.py"", line 235, in <module>
        main()
      File ""/private/var/folders/gy/jzs3xnwd1z3203d75y_31nxc0000gn/T/pip-req-build-6ap9n5ru/tools/cythonize.py"", line 231, in main
        find_process_files(root_dir)
      File ""/private/var/folders/gy/jzs3xnwd1z3203d75y_31nxc0000gn/T/pip-req-build-6ap9n5ru/tools/cythonize.py"", line 222, in find_process_files
        process(root_dir, fromfile, tofile, function, hash_db)
      File ""/private/var/folders/gy/jzs3xnwd1z3203d75y_31nxc0000gn/T/pip-req-build-6ap9n5ru/tools/cythonize.py"", line 188, in process
        processor_function(fromfile, tofile)
      File ""/private/var/folders/gy/jzs3xnwd1z3203d75y_31nxc0000gn/T/pip-req-build-6ap9n5ru/tools/cythonize.py"", line 64, in process_pyx
        raise OSError('Cython needs to be installed in Python as a module')
    OSError: Cython needs to be installed in Python as a module
    Traceback (most recent call last):
      File ""<string>"", line 1, in <module>
      File ""/private/var/folders/gy/jzs3xnwd1z3203d75y_31nxc0000gn/T/pip-req-build-6ap9n5ru/setup.py"", line 508, in <module>
        setup_package()
      File ""/private/var/folders/gy/jzs3xnwd1z3203d75y_31nxc0000gn/T/pip-req-build-6ap9n5ru/setup.py"", line 488, in setup_package
        generate_cython()
      File ""/private/var/folders/gy/jzs3xnwd1z3203d75y_31nxc0000gn/T/pip-req-build-6ap9n5ru/setup.py"", line 285, in generate_cython
        raise RuntimeError(""Running cythonize failed!"")
    RuntimeError: Running cythonize failed!
    ----------------------------------------
ERROR: Command errored out with exit status 1: /Applications/Xcode.app/Contents/Developer/usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/gy/jzs3xnwd1z3203d75y_31nxc0000gn/T/pip-req-build-6ap9n5ru/setup.py'""'""'; __file__='""'""'/private/var/folders/gy/jzs3xnwd1z3203d75y_31nxc0000gn/T/pip-req-build-6ap9n5ru/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/gy/jzs3xnwd1z3203d75y_31nxc0000gn/T/pip-record-g21yc5wb/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers /Users/sean/Library/Python/3.8/include/python3.8/numpy Check the logs for full command output.

</details>",2021-01-05T08:02:51Z
754491313,"It is not so hard to just implement a correctly-rounded version of arithmetic functions. The problem is that there is a tradeoff between speed and accuracy. If 0.5000000001 ULP is not bearable, I will add correctly-rounded but slower functions.",2021-01-05T08:37:00Z
754493377,"Note this will try to allocate an array of `2**32` elements of type `object`, which may eventually succeed in systems with 32GB of memory.

Nevertheless, we should ideally respond to ctrl+C here.",2021-01-05T08:41:29Z
754501074,"> Is that actually incorrect with the transpose?

@seberg I think so. The original reproducer seems correct to me. In the extended candidate reproducer https://gist.github.com/ogrisel/efcfca806b39bb51d4ce695bad167b9c, I vary n and k and the results seem correct (I fail to reproduce the bug I observe when calling openblas DGEMM via numpy). I will probably need to use a debugger to double check the exact `cblas_dgemm` arguments when calling from numpy.",2021-01-05T08:57:04Z
754536109,"Perhaps arithmetic functions used for the trigonometric functions do not necessarily have to be correctly-rounded, only the arithmetic functions exposed to the user for IEEE 754 conformity.
In this context, the question arises as to whether GNU libquadmath arithmetic functions are correctly-rounded or not. ",2021-01-05T10:02:30Z
754543515,"Arithmetic functions are implemented within glibc, and they are correctly rounded.",2021-01-05T10:15:32Z
754571149,Thanks @seiko2plus ,2021-01-05T11:11:21Z
754575920,"[Paul Zimmermann (INRIA)](https://members.loria.fr/PZimmermann/) recently compared the accuracy of several math libraries: [accuracy.pdf](https://homepages.loria.fr/PZimmermann/papers/accuracy.pdf). This includes a comparison of quadrupole precision between glibc and icc.  See also his talk [How Slow is Quadruple Precision](https://icerm.brown.edu/materials/Slides/htw-20-vp/How_Slow_is_Quadruple_Precision_]_Paul_Zimmermann,_INRIA,_Nancy,_France.pdf) on [Variable Precision in Mathematical and Scientific Computing](https://icerm.brown.edu/events/htw-20-vp/#workshopschedule).",2021-01-05T11:21:20Z
754583270,"I found a temporary fix for this: It is only necessary to set the `AR` shell variable using

```
export AR=/usr/bin/ar
```
However, this is not really ""the"" solution, and I am confused as to whose fault this is. Someone mentioned that this might be a problem with the python distribution I am using (which is conda-forge).",2021-01-05T11:37:22Z
754583426,"@stereo720712: Have you installed Cython?
  
> Cythonizing sources
> Processing numpy/random/_bounded_integers.pxd.in
> Processing numpy/random/_philox.pyx
> Traceback (most recent call last):
>   File ""/private/var/folders/gy/jzs3xnwd1z3203d75y_31nxc0000gn/T/pip-req-build-6ap9n5ru/tools/cythonize.py"", line 59, in process_pyx
>     from Cython.Compiler.Version import version as cython_version
> ModuleNotFoundError: No module named 'Cython'
",2021-01-05T11:37:45Z
754600626,"Quadruple precision arithmetic functions are slow because packing to/unpacking from IEEE 754 format are pretty complicated. If computation speed is important, double-double is much faster. SLEEF quad library is designed in a way that the frontend and backend are separated. The frontend does packing and unpacking. Computation by the backend is all carried out with triple-double operations. Thus, it is easy to add another frontend for a different format.",2021-01-05T12:16:51Z
754609246,"I want to point out my comment https://github.com/numpy/numpy/issues/14574#issuecomment-753972040  is motivated by my desire to discuss the need for quadrupole precision in Numpy. It seems to me, that adding quadrupole precision to numpy is feasible in forseable future with the help off Sleef and MPLAPACK for a considerable number of target systems.

Personally I would like to have the possibility to combine the efficiency and elegance of numpy with enhanced precision. On the other side I know that this is a small but important niche within computational science. Usually Fortran or C++ are used there.

I'm not a numpy maintainer, so I have no idea if the effort of integration into numpy pays of. This is the question that has to be discussed first IMHO.

",2021-01-05T12:36:07Z
754751458,"This is outside of numpy's control:
```python
x = np.array([ float('nan'), 56.3, 35.2, float('nan'), 5.7, 22., 18.9, 8., 7.7, 47.3, 51.6, 54.1, 47.3, 51.6])
y = np.roll(x, 1)

# numpy is not in the picture any more
x = x.tolist()
y = y.tolist()

# but your tests still fails
print(max(x))
print(max(y))
```
",2021-01-05T16:39:20Z
754757624,">> Could the same magic be used to make datetime64 and timedelta64 available?

> Yes, but let's leave that to a follow up

Trying to follow up on this, added

```
    ctypedef class numpy.datetime64 [object PyObject]:
        pass
    ctypedef class numpy.timedelta64 [object PyObject]:
        pass
```

along with the declarations in this PR, but at import time I'm getting

```
RuntimeWarning: numpy.datetime64 size changed, may indicate binary incompatibility. Expected 16 from C header, got 32 from PyObject
```

which is enough to stop the tests from running.  Did you have something else in mind for how the followup would work?",2021-01-05T16:50:51Z
754775351,"@carlkl I am doing a lot of slow work to allow you to externally write a quadruple precision that integrates seamlessly.  Right now, that is not finished and the parts that are, are not public API yet, but it should fall in places for 1.21 far enough that one can start to use it (experimentally).

There could be some small limitations (at least I assume those won't go away quickly), such as having to use `dtype=quadruple.Quadruple` instead of `dtype=""quadruple""` (i.e. no string).  But it will smoothen out, or at least set the stage, the current issues with externally defined dtypes.  That said, even now `quaternions` work fairly well written outside of NumPy.

My current opinion is to start writing such dtypes outside of NumPy first.  While it probably makes some things work less well, it avoids having to worry about partial implementation or stranger corners in NumPy.
I admit, there is the problem that doing it *right now* with the old API means you need to update it soon, OTOH, I doubt that boilerplate is hard to update.  If you wait a few months, it would be a (very simple) trial baloon for a new API.",2021-01-05T17:18:43Z
754777032,"Is `numpy.datetime64` currently exposed as the full struct?  You can add an ignore flag like for `ndarray` I guess, although I am surprised it is necessary, and I am curious whether this is just a matter of recompiling?  (assuming this is within NumPy itself, if it is outside you should not have to recompile of course.)",2021-01-05T17:21:39Z
754781261,We could add a `PyInterrupt_Occurred()` (if that was the command) check every time we entering another level of sequence unpacking.  That should be easy to do for someone familiar with the Python C-API and not scared of digging into the code.,2021-01-05T17:29:14Z
754782336,"There are uses for quad precision apart from it starting to show up with hardware support. I looked into it because I'd like to use it for a new datetime type. I think astropy uses double double for that, they need the extra precision. As a historical note, VAX had a quad precision type back in the 1980's and I found it useful on occasion. ",2021-01-05T17:31:14Z
754786736,"@jbrockmendel, you need to use `object PyDatetimeScalarObject` as I do in the top PR comment. I suspect you'll need to remove the current `PyDatetimeScalarObject` cython struct two, to prevent them overlapping.",2021-01-05T17:39:05Z
754790553,"I am really not as such opposed with adding things to NumPy, I just feel that comes with a *lot* of baggage (whether real or mental); and also with stronger than typical backcompat guarantees.  Also someone not familiar with NumPy can probably dive into an external implementation much easier (and things tend to work better driven by interested parties).

I agree that having a larger than normal inexact type around does seem like a useful thing (I could imagine even just using it for double checking the accuracy of an algorithm).  Datetimes are a great reason, but also one of those complex beasts that, to me, seem better started outside NumPy.

If someone wants to start these projects, I would be happy to put them on the NumPy organization but outside the NumPy code base.  For DTypes there is the annoyance of private API being much more powerful and less wrinkly, I admit, hopefully that problem will be gone in a few months.",2021-01-05T17:45:30Z
754796988,"Even with a `uint8` dtype, that array seems to  have more than 4GiB of size. I expect you are storing it compressed, and reading it into NumPy will decompress it. Probably just `np.asarray(mask_raster)` is enough to give the error.",2021-01-05T17:55:58Z
754809338,"Thanks for the request, but I will close it, not because it is completely unreasonable, but it is just not a TODO item anyone will be able to work on for at least a few months, and after that it should be done outside of NumPy first (although it could be done in the NumPy organization).  So if anyone wants to start on such a project now, or in a few months, lets talk about it.

Adding it to numpy in the current state is close to impossible and it is not plausible to do it in parallel to the work trying to make it easy.  I would point to developing it outside of NumPy (which in my current opinion is the most likely course of action), but unfortunately that is not possible for such a dtype before NEP 43 is implemented.

At the current point, it would be possible to create such a dtype (or multiple of them), but they would have to be a fixed/small set of scaling factors.  That could be done even outside of NumPy (although the API around it will change soon hopefully and will support creating a single DType with varying scaling factor).",2021-01-05T18:16:07Z
754810465,"So to anwswer your question, yes it is `lzw` compressed but I can open it as an ndarray just fine with : 
```py
with rio.open(mask_raster) as src: 
    data = src.read()
```

at this point the `data` ndarray is uncompressed and in `int16`. I'm well aware that it is 4 GiB which is normal as I'm dealing with a 30m resolution map of RDC. Performing the `np.unique` operation leads to an explosion of the array volume when it tries to allocate a new array in `int64`. In the SO anwser they pointed out that the uint64 array is probably allocated during argsort ([here in the source code](https://github.com/numpy/numpy/blob/v1.19.0/numpy/lib/arraysetops.py#L319)) which is not necessary.

To rephrase my issue :   
`np.unique` allocate a ndarray in a suboptimal data type (int64) which is only a problem when dealing with huge array (like mine)

",2021-01-05T18:17:43Z
754815335,"* Using `int64` is necessary on 64bit systems, since array sizes can be larger than `2**31` (or `2*32`) elements. So the right dtype to return from functions like argsort, or here ""indices"" and ""counts"" is `int64`.
* I am confused because the error message says `uint64` and _not_ int64, and I don't see know NumPy would allocate an *unsigned int*, unless that is already the input. Providing the full traceback would help.
* I thought `src.read()` might return a different object. So that `np.asarray(src.read())` would not make a difference. The full traceback would probably have eliminated that thought (or printing out exact details of what object `mask_raster` is)",2021-01-05T18:25:59Z
754820429,"For your problem, I assume that `find_objects` and then using just boolean logic `w = mask_raster == object_id` and `np.count_nonzero(w)` and `object_values = original_data[w]` is better though. `unique` needs to sort the array, that is a huge waste probably, when all your blobs are likely well defined and in one area.",2021-01-05T18:35:13Z
754843509,"> I think this could be made better looking using stacked parametrizations, the product/nested iteration, for instance, can be done
> 
> ```python
> @pytest.mark.parametrize(""one"", [...])
> @pytest.mark.parametrize(""two"", [...])
> def test_me(one, two):
>     pass
> ```

Done",2021-01-05T19:18:17Z
754844141,"When I disable 
```
    ctypedef struct PyDatetimeScalarObject:
        # PyObject_HEAD
        npy_datetime obval
        PyArray_DatetimeMetaData obmeta

    ctypedef struct PyTimedeltaScalarObject:
        # PyObject_HEAD
        npy_timedelta obval
        PyArray_DatetimeMetaData obmeta
```

and in its place put

```
    ctypedef class numpy.datetime64 [object PyDatetimeScalarObject]:
        cdef npy_datetime obval
        cdef PyArray_DatetimeMetaData obmeta
    
    ctypedef class numpy.timedelta64 [object PyTimedeltaScalarObject]:
        cdef npy_timedelta obval
        cdef PyArray_DatetimeMetaData obmeta
```

I get compile-time errors below where PyDatetimeScalarObject and PyTimedeltaScalarObject are referenced.  Disabling those functions for now numpy compiles, but when running `pytest numpy/core/tests/test_cython.py` i get compile-time errors for checks.pyx complaining about just about everything in the cnp namespace.

 just passing instead of declaring the attributes on datetime64/timedelta64 gives the same errors.",2021-01-05T19:19:25Z
754844612,Thanks @Qiyu8 ,2021-01-05T19:20:14Z
754854756,"So the potential issue with the compiler assuming alignment only applies with the loop unrolling macro?

EDIT: Or is it that it is a *performance* issue with auto vectorization? Which probably does not matter elsewhere.",2021-01-05T19:39:33Z
754952449,"Hello, I am not sure that my issue fits to the example above but I see the following behavior:
I want to do a bitwise or of two arrays with the same shape (and also same unique values). My numpy arrays can be described like this:
shape of ar1: (4194304,) and 
ar2: (4194304,)
unique val of ar1: [0. 1.] and 
ar2: [0. 1.]

## System
System Software Overview:

      System Version: macOS 11.1 (20C69)
      Kernel Version: Darwin 20.2.0

np.__version__: '1.19.2'
Python version: '3.8.5 (default, Sep  4 2020, 02:22:02) \n[Clang 10.0.0 ]'

## Analysis
So I ran something comparable on a smaller scale:
ar = np.array([1,0,1,0])
ar2 = np.array([1,0,0,0])
ar3 = np.array([ar,ar2])
ar3.shape
red_ar = ar3[0,:]
np.bitwise_or(red_ar,ar3[1,:])


This small example produces the result I expect:
array([1, 0, 1, 0])

## Trace
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-180-7d0ef27a4792> in <module>
      1 fma = FlajoletMartin(stream.content, copies=2)
----> 2 res = fma.fm()
      3 res

<ipython-input-179-bfefe9329c43> in fm(self)
    179         # reduce bitmap
    180         print(self.bitmaps.shape)
--> 181         red_bitmap = self.reduce_bitmaps(self.bitmaps)
    182 
    183         R = self.rightmost_zero(red_bitmap)

<ipython-input-179-bfefe9329c43> in reduce_bitmaps(self, bitmap)
    156             print(f'shape of ar1: {red1.shape} and \nar2: {red2.shape}')
    157             print(f'unique val of ar1: {np.unique(red1)} and \nar2: {np.unique(red2)}')
--> 158             reduced_bitmap = np.bitwise_or(red1, red2)
    159 #             for i in range(1,bitmap.shape[0]):
    160 #                 assert bitmap[i,:].shape == reduced_bitmap.shape

TypeError: ufunc 'bitwise_or' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",2021-01-05T22:58:12Z
754955283,"> unique val of ar1: [0. 1.] and
> ar2: [0. 1.]

That shows that `ar1` and `ar2` are arrays of floating point values.  The bitwise operators do not accept floating point inputs.  Convert them to integer arrays (e.g. `ar1.astype(int)`) before passing them to `bitwise_or`.",2021-01-05T23:05:52Z
754955578,"The bitwise operations are only implemented for integers. I assume the original report also was confused by this.  There may be the possibility of discussing changing that, but bitwise operations on floats do seem trickier to understand to me, so right now I doubt we should?  If someone is aware of what bitwise operations on floats will do, they can already use `arr.view(np.uint64)`, etc. to apply bitwise operations on the integer representation.

I will close the issue for now, since I believe there is probably nothing we should change (the error is not ideal, but probably not easy to update). Although it might be nice if it listed the input dtypes.",2021-01-05T23:06:43Z
754961893,Thank you!!,2021-01-05T23:24:30Z
755087498,"I think that's how it has always behaved. `np.any()` is essentially `np.logical_or.reduce()`, and `np.logical_or()` on object arrays follows the behavior of the `or` operator, returning the first of its operands that is truthy, not a `bool`. `np.any()` predates the builtin `any()`, so the difference is unfortunate, but not a bug.",2021-01-06T05:26:34Z
755096139,"@rkern thanks. The behavior surfaced using `pandas.DataFrame.any()` https://github.com/numpy/numpy/issues/18129 and expectations were based on their docs. You mentioned a difference between `np.any()` and built-in `any()` is this the difference between `np.any(arr)` and `arr.any()`? Both seem to give the same result.

```python
arr = np.array(['s', 1], dtype=object)
np.any(arr)
```
also returns `s`.
",2021-01-06T05:49:47Z
755101299,"In addition to this, would it be a good idea to have a function that does what we expect `numpy.fromfunction` to do?
This would call the provided `function` ""no of elements in `shape`"" (`shape[0]*shape[1]`...) times giving it tuples like so `(0,0)` `(0, 1)`...
It would also always return an `np.array` of shape `shape` where the value at `(i, j)`th index is `function(i, j)`",2021-01-06T06:03:19Z
755112270,"ok, then I will use my workaround and close the issue. 
sorry for bothering and thank you for your time",2021-01-06T06:32:16Z
755117409,Could you move the deprecation test to the proper file? Otherwise this looks good to me.,2021-01-06T06:45:07Z
755171534,"@rgommers I see this didn't make it into the 1.19.5 version (https://github.com/Homebrew/homebrew-core/pull/68346), shouldn't this simple patch be backported?",2021-01-06T08:54:42Z
755212503,"I would also find NumPy popcount/countbits useful.

In case it's helpful for anyone: as well as Numba it's also available in JAX, as [`jax.lax.population_count`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.population_count.html#jax.lax.population_count), and here's an implementation in terms of other NumPy functions: https://github.com/google/jax/blob/6c8fc1b031275c85b02cb819c6caa5afa002fa1d/jax/lax_reference.py#L121-L150.",2021-01-06T10:16:02Z
755216577,"Argh yes, that is an oversight. I should have put on a label, then @charris would have seen it when deciding what to backport for 1.19.5. Done now; luckily `1.20.0` is also imminent, so the damage is very limited. I checked the fix is in the `1.20.x` branch.",2021-01-06T10:24:40Z
755293840,"> I should have put on a label

The milestone needed to be 1.19.5.

NumPy 1.20.0 is waiting for pyarrow to release. If they don't get a move on I'll release anyway.",2021-01-06T13:17:49Z
755295179,I'm not planning on another 1.19 release.,2021-01-06T13:20:40Z
755300196,"@charris, @eric-wieser, @javad-doliskani, thank you for the insightful discussion! Given the ambiguity of the problem, let's close this one as-is.",2021-01-06T13:31:37Z
755324181,"> As a comparison of popularity, I did some Google search to see if there are people interested in the functions of these `__builtin` routines. It seems that judging by the relevance of top results and the total number of search results:
> 
> * `popcount` (""python count one bits""): 4070000 results
...
> * `parity` (""python integer parity""): 1040000 results

Note parity is trivial to implement if you have an efficient popcount. `(parity=popcount&1)`",2021-01-06T14:17:50Z
755372460,"@seberg  _I am doing a lot of slow work to allow you to externally write a quadruple precision that integrates seamlessly. Right now, that is not finished and the parts that are, are not public API yet, but it should fall in places for 1.21 far enough that one can start to use it (experimentally)_
Is this https://github.com/seberg/numpy-dispatch? BTW. I don't expect any workable results in the near future. Therefore I do not plan a fixed schedule,  just thinking about the next steps - probably first testing mplapack with sleef as a backend.

I did a quick look at what the developers of https://github.com/moble/quaternion have done, but at a first glance I didn't get the idea how to extend dtypes with quadrupole-precision. Are there tutorials how to extend numpy with new datatypes?

",2021-01-06T15:37:58Z
755385874,"No, I meant the Python builtin `any()`. It came into the language after Numeric (numpy's predecessor) had the function `Numeric.any()` (now `numpy.any()`). If the builtin `any()` had existed earlier (with its `bool`-returning behavior), we probably would have made `Numeric.any()` match.",2021-01-06T15:59:06Z
755387641,"Actually, that timing may not be true. Faulty memory.",2021-01-06T16:01:41Z
755390114,"No, numpy-dispatch is on the array object level. The idea of that is that you can write code that supports both NumPy and e.g. Jax, etc. arrays.  What I am talking about here is the work related to: https://numpy.org/neps/nep-0041-improved-dtype-support.html which will allow to write dtypes outside of NumPy with much fewer issues (also allowing much more complex dtypes than numpy can currently handle reasonably).

There is fairly little information on how to write a dtype currently. And the API has to change pretty much completely. @WarrenWeckesser also has similar tries here: https://github.com/WarrenWeckesser/numtypes which may give a better overview and more explanation.
Its a bit verbose and arcane, but overall it should actually be fairly straight forward.  You have to create a new `dtype` object in C and then register it with NumPy (the main step there is to implement the set of ""ArrFunctions"") after that you can look into casting and ufuncs.  (Pretty much all of that API has to be revamped, but the basic puzzle pieces will remain of course.)",2021-01-06T16:05:27Z
755392477,"I think they came in with `numpy`, which was released a little before Python 2.5, the first release with the `any()` and `all()` builtins. So a closer-run thing than I remember. We might also have had `any()` and `all()` in the Numeric-based `scipy` earlier than that, though it's more effort than I think it's worth to do the code archaeology to figure out.",2021-01-06T16:09:14Z
755397144,Sounds like an OpenBLAS cpu detection problem. @martin-frbg Thoughts?,2021-01-06T16:16:09Z
755400474,"Done, we could consider deprecating even more, but I guess this might be a good (and easy) first step.

Yeah, I think I first wrote that at a time when I was considering to stop relying on that file so much (The original reason, and most of the magic in that file, was to work around python issues with warnings, which were eventually fixed).",2021-01-06T16:20:04Z
755401771,"Note that we do test on aarch64 during the wheel build, so docker may also be at fault. Where did you get it?",2021-01-06T16:20:50Z
755432611,"Great, https://github.com/WarrenWeckesser/numtypes has a link to https://github.com/stefanv/teaching, which includes an  [example](https://github.com/stefanv/teaching/tree/master/2013_scipy_austin_dive_into_numpy/examples/quad_dtype) for extending with quad types.",2021-01-06T17:08:19Z
755438271,"Not sure what the illegal instruction could be - the `getauxval(AT_HWCAP)` in dynamic_arm64.c may be a priviledged call in some circumstances but it has been in the code for two years, the only recent change was my addition of an attempt to read cpuid information from /sys/devices in the event that getauxval did not succeed. (OpenBLAS PRs 2952 and 3004) 
`export OPENBLAS_CORETYPE=ARMV8` (or whatever the actual hardware is) before launching python should hopefully get around this.",2021-01-06T17:18:29Z
755444034,"@rkern  Hey, I was facing the same problem and due to the issue that you mentioned. Thanks :) ",2021-01-06T17:28:35Z
755446663,Thanks @seberg ,2021-01-06T17:33:16Z
755704225,"> here's an implementation in terms of other NumPy functions: https://github.com/google/jax/blob/6c8fc1b031275c85b02cb819c6caa5afa002fa1d/jax/lax_reference.py#L121-L150.

This is an implementation of divide-and-conquer popcount, which is going to be substantially slower than something that uses native popcount instructions. http://github.com/BartMassey/popcount shows 4 slowdown on native code, but the Python implementation will be way slower than that I suspect.",2021-01-06T21:11:51Z
755743899,Haha yes I wasnt claiming it was efficient . Perhaps I should have added an explicit warning.,2021-01-06T22:09:42Z
755762142,"Here's a numpy-only approach I coded up this morning (to fill the need that lead me here)

I have not profiled it, but it made my simulation acceptably fast.

```python
# create a lookup table for set bits per byte
lookuptable = np.array([bin(i).count('1') for i in range(256)]).astype(np.int32)
def count_set_bits(v):
    'returns the count of set bits in each element of v'
    assert v.dtype in (np.uint8, np.uint16, np.uint32, np.uint64), 'must be an unsigned int dtype'
    return lookuptable[np.reshape(v.view(np.uint8), v.shape + (-1, ))].sum(axis=-1)
```",2021-01-06T22:43:41Z
755822314,"Table-driven is probably the best choice for this situation, and this is a nice implementation. Thanks!

That said, still want a `popcount` primitive. :-)",2021-01-07T01:20:36Z
755916689,"For you reference (Macbook M1), 
OPENBLAS=""$(brew --prefix openblas)"" MACOSX_DEPLOYMENT_TARGET=11.1 pip3 install numpy pandas --no-use-pep517

It will install pandas-1.2.0 and numpy-1.19.5 successfully.",2021-01-07T06:32:01Z
755993207,"LGTM. This is a reshuffle to enable changes in the future, with no real code changes.",2021-01-07T09:22:28Z
755994852,"I think we can close this issue and declare the bounty complete. The original bounty says:
> NumPy contains SIMD vectorized code for x86 SSE and AVX. This issue is a feature request to implement native, equivalent enablement for Power VSX, achieving equivalent speedup appropriate for the SIMD vector width of VSX (128 bits).

We have implemented the infrastructure needed to enable the porting of loops via Universal SIMD, and even ported some of the loops, so the enablement is complete.",2021-01-07T09:25:22Z
756003446,"@mattip,

> Could you benchmark the loops to make sure we did not negatively impact x86_64 performance and if possible on ARM64 as well?

same as before nothing changed

> It seems coverage thinks some of the refactored code is not covered. Is there a test we could add to hit that code path?

all cases are covered by tests but the issue here the coverage runs only for the highest supported SIMD target by the local machine and leaves the other paths untested.

Currently, I'm working on a lightweight execution tracer similar to [opencv/7101](https://github.com/opencv/opencv/pull/7101), it will allow us to test the dispatcher paths
and also provides elapsed CPU ticks for each execution region.",2021-01-07T09:40:11Z
756008671,"There is a warning in doc building:
```
/home/circleci/repo/doc/source/release/1.21.0-notes.rst:15: WARNING: Unexpected indentation.
/home/circleci/repo/doc/source/release/1.21.0-notes.rst:16: WARNING: Block quote ends without a blank line; unexpected unindent.
```",2021-01-07T09:49:28Z
756048017,"Thanks @IshanKBG. If we need to change anything, I think copying the content from [the installation docs](https://numpy.org/doc/1.19/user/install.html) would be better. But I am not sure we should add anything. This is the README for the github repo, so it is expected people who see this will know what to do. Did you come across a reference to this page as a resource for how to get started with NumPy?",2021-01-07T11:04:28Z
756098326,"When I read _Hierarchy and abstract classes_ from [NEP 42,](https://numpy.org/neps/nep-0042-new-dtypes.html) I can read out the following. A quadrupole-precision type should be implementing something like this in future:

```text
Abstract Type:  DType -> Numeric -> Floating -> Float128
Concrete Type:  DType -> Float128
```
@seberg with future numpy-1.21 or higher is what I assume is meant?
",2021-01-07T12:52:54Z
756140369,"I can confirm that

```
OPENBLAS_CORETYPE=ARMV8 python3 -c ""import numpy""
```

doesn't segfault, while omitting the OPENBLAS_CORETYPE env var does.",2021-01-07T14:12:36Z
756142087,It may be relevant that `/sys/devices/system/cpu/cpu0/regs` does not exist on this system.,2021-01-07T14:15:19Z
756157883,Can you elaborate on the change you're suggesting? Are you talking about the `parameters` section? The `examples` section?,2021-01-07T14:42:14Z
756163207,"@carlkl I hope 1.21 is what it means, I would have hoped to be closer to having it done, but thats life.  There doesn't need to be an abstract type, Float128 can be a subclass of an abstract Floating, but it doesn't necessarily matter too much to begin with, and those abstract dtypes don't even exist yet.",2021-01-07T14:51:25Z
756172625,"I have implemented the SSE versions, but I notice UT failing locally for float. How float?
```py
>>> import numpy as np
>>> np.__version__
'0.3.0+24365.g2a4437284'
>>> fone = np.array(1.0, dtype=np.float32)
>>> fzer = np.array(0.0, dtype=np.float32)
>>> np.floor_divide(fone, fzer)
<stdin>:1: RuntimeWarning: divide by zero encountered in floor_divide
inf
>>>
```

```py
>>> import numpy as np
>>> np.__version__
'1.19.2'
>>> fone = np.array(1.0, dtype=np.float32)
>>> fzer = np.array(0.0, dtype=np.float32)
>>> np.floor_divide(fone, fzer)
<stdin>:1: RuntimeWarning: invalid value encountered in floor_divide
nan
>>>
```

I gdb'd into the code, specifically `FLOAT_divide`, that in turn goes into `loops_arithm_fp.dispatch.c.src`, which I have not touched and don't see any error being set either. Any pointers here will be very helpful before I proceed to AVX",2021-01-07T15:06:48Z
756180516,"> It may be relevant that `/sys/devices/system/cpu/cpu0/regs` does not exist on this system.

While that certainly does not help with cpu detection, it should not cause a crash (and is actually a scenario that I _did_ test) - this is tried only after an unsuccessful getauxval(), and when the fopen() fails the function returns NULL (which is subsequently interpreted as an unknown/generic ARMV8 core)
I _think_ the only thing that could go SIGILL is the get_cpu_ftr() call which does a ""mrs %0"" to read the processor features register - but this call has been in the code for something like two years and the only related change in recent months was to change the `asm` prefix of the inline assembly to `__asm__` for compatibility with C18. Unless, of course, I messed something up unintentionally...  ",2021-01-07T15:19:32Z
756193514,The code itself looks fine. It seems like we are adding alot of boilerplate but I don't see how to avoid it. There is a conflict and coverage is still complaining that some of the test conditions are never hit. Could you write some c-level tests that abuse the API just to see that the errors are correctly raised with no problems?,2021-01-07T15:40:02Z
756218004,"Well, the boilerplate has a purpose... It adds a layer of separation between code paths that need to be separated.  Ideas welcome, but the only idea I had would be to use a `c.src` and I did not see the advantage of code reduction making that worth it (at least I like to have good editor/static analysis).

I had hoped I can push that off, until that API is actually made public and the errors are more than sanity checks on new additions to NumPy, because that probably makes the testing also nicer (e.g. you had the idea of introducing that test-time compilation infrastructure), but yes, I could do that.",2021-01-07T16:18:32Z
756218686,I restarted Azure since it hit gh-18119 on the 64-bit run (failed to find nm.exe),2021-01-07T16:19:38Z
756220811,"Since it is internal we can put off adding tests for now, but should somehow mark it with a comment `TODO: test this` or so.",2021-01-07T16:22:57Z
756230802,"GDB says yes:
```
=> 0x0000ffffb6106f54 <+500>:   mrs     x21, midr_el1
```

With 1.19.4, that instruction never gets executed, as the tbz instruction just before it causes a jump past it:
```
0xffffb6218ac4 <gotoblas_dynamic_init+524>      bl     0xffffb60a0f70 <getauxval@plt>
0xffffb6218ac8 <gotoblas_dynamic_init+528>      tbz    w0, #11, 0xffffb6218c18 <gotoblas_dynamic_init+864>
0xffffb6218acc <gotoblas_dynamic_init+532>      mrs    x1, midr_el1
```

w0 is 255, so bit 11 is zero. So HWCAP_CPUID isn't set.

In the numpy 1.19.5 code, mrs is executed directly after the getauxval call:

```
0xffffb6106f50 <gotoblas_dynamic_init+496>      bl     0xffffb5f86ec0 <getauxval@plt>
0xffffb6106f54 <gotoblas_dynamic_init+500>      mrs    x21, midr_el1
0xffffb6106f58 <gotoblas_dynamic_init+504>      tbz    w0, #11, 0xffffb610709c <gotoblas_dynamic_init+828>
```

The compiler seems to have reordered the instructions. I believe adding the `volatile` keyword to the `__asm__` block should prevent this.",2021-01-07T16:40:04Z
756262654,"Thank you. I had actually looked at this (in the context of the PR that changed the keyword to `__asm__`) and wondered why this particular instance had not been labeled as volatile, but fooled myself with the argument that it ""appeared"" to have worked just fine for so long. (Could be the version included in 1.19.5 was built with a newer gcc or just subtly different options than before) OTOH, now that I have stumbled upon this thread in the gcc help ML, https://gcc.gnu.org/legacy-ml/gcc-help/2017-10/msg00061.html I am no longer sure if `volatile` will be sufficient. :(",2021-01-07T17:29:56Z
756284418,"I think the restructuring of the code you did caused the optimiser to make different decisions.

> OTOH, now that I have stumbled upon this thread in the gcc help ML, https://gcc.gnu.org/legacy-ml/gcc-help/2017-10/msg00061.html I am no longer sure if volatile will be sufficient. :(

That's not good. The only thing I can think of that would definitely work is to pass the result of getauxval() into the asm, and do some branching within the asm.",2021-01-07T18:07:24Z
756299738,"Wasn't there a fix for this? See #18100.

EDIT: That fix was backported after rc2.",2021-01-07T18:34:07Z
756302337,So it'll be fixed in rc3?,2021-01-07T18:38:37Z
756308357,"> So it'll be fixed in rc3
 
Hopefully. We are currently waiting on pyarrow, so I might put one out in a week or two, especially if the OpenBLAS compiler problems currently reported on AARCH64 get addressed.",2021-01-07T18:49:20Z
756309124,"Ok, I'll keep an eye out.",2021-01-07T18:50:42Z
756312171,"We got around this in NumPy in [npy_get_floatstatus_barrier](https://github.com/numpy/numpy/blob/5cae51e794d69dd553104099305e9f92db237c53/numpy/core/src/npymath/ieee754.c.src#L594) by passing a dummy parameter into the function, which prevented reordering even with `-O3`",2021-01-07T18:56:20Z
756319560,Kindly ask if anyone can review this PR? Any response will be appreciated!,2021-01-07T19:09:24Z
756328776,"> For you reference (Macbook M1),
> OPENBLAS=""$(brew --prefix openblas)"" MACOSX_DEPLOYMENT_TARGET=11.1 pip3 install numpy pandas --no-use-pep517
> 
> It will install pandas-1.2.0 and numpy-1.19.5 successfully.

This worked! I had to jump through a few hoops to get some packages installed and I'm not sure how many times the OPENBLAS argument was actually needed, but this got me through the installation of all the packages I wanted installed:

$ brew install python@3.9
$ brew install openblas
$ OPENBLAS=""$(brew --prefix openblas)"" MACOSX_DEPLOYMENT_TARGET=11.1 python3 -m pip install cython --no-use-pep517
$ OPENBLAS=""$(brew --prefix openblas)"" MACOSX_DEPLOYMENT_TARGET=11.1 python3 -m pip install numpy --no-use-pep517
$ OPENBLAS=""$(brew --prefix openblas)"" MACOSX_DEPLOYMENT_TARGET=11.1 python3 -m pip install pandas --no-use-pep517
$ OPENBLAS=""$(brew --prefix openblas)"" MACOSX_DEPLOYMENT_TARGET=11.1 python3 -m pip install pybind11 --no-use-pep517
$ OPENBLAS=""$(brew --prefix openblas)"" MACOSX_DEPLOYMENT_TARGET=11.1 python3 -m pip install scipy --no-use-pep517
$ brew install libjpeg zlib
$ python3 -m pip install pillow
$ python3 -m pip install matplotlib

all the other packages listed in the requirements.txt file were then able to be installed.
[requirements.txt](https://github.com/numpy/numpy/files/5783558/requirements.txt)
",2021-01-07T19:27:23Z
756375549,"I see, thanks for clarifying, didn't know Python's built-in s`.any()` and `.all()`. Since this isn't a bug, I think it makes sense to close issue. ",2021-01-07T20:49:12Z
756454343,"Question for folks following this issue that's slightly off topic. I've installed scipy as above and am getting a segfault when I import scipy.integrate. I've filed a bug report [here](https://github.com/scipy/scipy/issues/13364#issue-781683689). Any one here able to import scipy.integrate after installing per above on M1? I figured I'd ask here as a bunch of folks here probably already have the setup in place. If so, maybe hop over to [that issue](https://github.com/scipy/scipy/issues/13364#issue-781683689) and leave a note. Thanks!",2021-01-07T23:43:50Z
756455422,"> Question for folks following this issue that's slightly off topic. I've installed scipy as above and am getting a segfault when I import scipy.integrate. I've filed a bug report [here](https://github.com/scipy/scipy/issues/13364#issue-781683689). Any one here able to import scipy.integrate after installing per above on M1? I figured I'd ask here as a bunch of folks here probably already have the setup in place. If so, maybe hop over to [that issue](https://github.com/scipy/scipy/issues/13364#issue-781683689) and leave a note. Thanks!

segfault here too.",2021-01-07T23:47:27Z
756460058,"Well, I added such a comment to the private methods and the internal fromspec (in the header). Also squashed and the doctest flag switched (which means codecov will probably be useless now).",2021-01-08T00:01:08Z
756485337,"I completely agree.  Sayed and the NumPy core developers have done an amazing job!

Do you want me to close the issue? I have been deferring to the NumPy developers.",2021-01-08T01:17:09Z
756571176,Thanks @edelsohn. Agreed that the solution exceeded expectations. I will close it then.,2021-01-08T06:14:47Z
756573673,"Build is failing, typo?:
```
numpy/core/src/multiarray/array_method.c:753:43: error: boundarraymethod__resolve_descripors undeclared here
```",2021-01-08T06:22:11Z
756576062,"This is really low on our priority list. If you wish to move forward with it, please relate to the [last comment on the issue](https://github.com/numpy/numpy/issues/15986#issuecomment-705120053):
> We would like to suggest that contributors show the output from the error. This will require you think about the error: is the code path actually hit? Does it make sense to chain the exception?",2021-01-08T06:28:41Z
756648188,"Awesome, thanks everyone for the excellent work!",2021-01-08T09:24:54Z
756688023,"I'm talking about 
`def stack(arrays, axis=0, out=None): `

the definition is not consistent with other numpy functions like concatenate. It took me a few minutes to realise the use was the same. not a big deal of course, if I looked at the examples I would have realised it, but I think it would be better if it were consistent over all functions",2021-01-08T10:46:41Z
756689367,What are you suggesting that line should be instead?,2021-01-08T10:49:56Z
756695210,"`numpy.concatenate((a1, a2, ...), axis=0, out=None)`
like this one",2021-01-08T11:02:59Z
756695977,"Sorry, but we have to use legal python syntax:
```pycon
>>> def stack((a1, a2, ...), axis=0, out=None): pass
  File ""<ipython-input-62-524e6e6c5d59>"", line 1
    def stack((a1, a2, ...), axis=0, out=None): pass
              ^
SyntaxError: invalid syntax
```",2021-01-08T11:04:30Z
756707487,"```
>>> np.lcm(219060189739591200, 43, dtype=object)
9419588158802421600
>>> x = np.int64(np.lcm(219060189739591200, 43, dtype=object))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
OverflowError: Python int too large to convert to C long

```
It produced the following outputs @eric-wieser .",2021-01-08T11:30:59Z
756711449,Then the behavior is as expected. It is impossible for your original example to produce a correct result because the correct result cannot be stored in the return type.,2021-01-08T11:40:35Z
756767690,"What happens if you replace
```
$ OPENBLAS=$OPENBLAS_ROOT/lib64 pip install ...
```

with
```
$ OPENBLAS=$OPENBLAS_ROOT/lib64 python -m pip install ...
```

?",2021-01-08T13:56:18Z
756768319,(also note OpenBLAS has released version 0.3.13 ...),2021-01-08T13:57:41Z
756783248,"Same issue with a newer OpenBLAS version (0.3.7) and ""python -m pip""

```
[sfux@eu-login-20 ~]$ module list

Currently Loaded Modules:
  1) StdEnv   2) gcc/6.3.0   3) python/3.7.4   4) cuda/9.0.176   5) cudnn/7.3   6) hdf5/1.10.1   7) nccl/2.7.8-1   8) openblas/0.3.7


[sfux@eu-login-20 ~]$ source $HOME/pytorch_100_cupy90_540/bin/activate
(pytorch_100_cupy90_540) [sfux@eu-login-20 ~]$ python
Python 3.7.4 (default, Oct 16 2019, 13:45:57)
[GCC 6.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import Cython
>>> Cython.__version__
'0.29.21'
>>> quit()
(pytorch_100_cupy90_540) [sfux@eu-login-20 ~]$ OPENBLAS=$OPENBLAS_ROOT/lib64 python -m pip install --no-binary :all: numpy==1.19.5
Collecting numpy==1.19.5
  Using cached https://files.pythonhosted.org/packages/51/60/3f0fe5b7675a461d96b9d6729beecd3532565743278a9c3fe6dd09697fa7/numpy-1.19.5.zip
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
    Preparing wheel metadata ... error
    Complete output from command /cluster/home/sfux/pytorch_100_cupy90_540/bin/python /cluster/home/sfux/pytorch_100_cupy90_540/lib64/python3.7/site-packages/pip/_vendor/pep517/_in_process.py prepare_metadata_for_build_wheel /tmp/tmpx_wvk44c:
    Processing numpy/random/_bounded_integers.pxd.in
    Processing numpy/random/_mt19937.pyx
    Traceback (most recent call last):
      File ""/tmp/pip-install-_5e8ppld/numpy/tools/cythonize.py"", line 235, in <module>
        main()
      File ""/tmp/pip-install-_5e8ppld/numpy/tools/cythonize.py"", line 231, in main
        find_process_files(root_dir)
      File ""/tmp/pip-install-_5e8ppld/numpy/tools/cythonize.py"", line 222, in find_process_files
        process(root_dir, fromfile, tofile, function, hash_db)
      File ""/tmp/pip-install-_5e8ppld/numpy/tools/cythonize.py"", line 188, in process
        processor_function(fromfile, tofile)
      File ""/tmp/pip-install-_5e8ppld/numpy/tools/cythonize.py"", line 76, in process_pyx
        raise RuntimeError(f'Building {VENDOR} requires Cython >= {required_version}')
    RuntimeError: Building NumPy requires Cython >= 0.29.21
    Running from numpy source directory.
    setup.py:480: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates
      run_build = parse_setuppy_commands()
    Cythonizing sources
    Traceback (most recent call last):
      File ""/cluster/home/sfux/pytorch_100_cupy90_540/lib64/python3.7/site-packages/pip/_vendor/pep517/_in_process.py"", line 207, in <module>
        main()
      File ""/cluster/home/sfux/pytorch_100_cupy90_540/lib64/python3.7/site-packages/pip/_vendor/pep517/_in_process.py"", line 197, in main
        json_out['return_val'] = hook(**hook_input['kwargs'])
      File ""/cluster/home/sfux/pytorch_100_cupy90_540/lib64/python3.7/site-packages/pip/_vendor/pep517/_in_process.py"", line 69, in prepare_metadata_for_build_wheel
        return hook(metadata_directory, config_settings)
      File ""/cluster/apps/nss/gcc-6.3.0/python/3.7.4/x86_64/lib64/python3.7/site-packages/setuptools/build_meta.py"", line 156, in prepare_metadata_for_build_wheel
        self.run_setup()
      File ""/cluster/apps/nss/gcc-6.3.0/python/3.7.4/x86_64/lib64/python3.7/site-packages/setuptools/build_meta.py"", line 237, in run_setup
        self).run_setup(setup_script=setup_script)
      File ""/cluster/apps/nss/gcc-6.3.0/python/3.7.4/x86_64/lib64/python3.7/site-packages/setuptools/build_meta.py"", line 142, in run_setup
        exec(compile(code, __file__, 'exec'), locals())
      File ""setup.py"", line 508, in <module>
        setup_package()
      File ""setup.py"", line 488, in setup_package
        generate_cython()
      File ""setup.py"", line 285, in generate_cython
        raise RuntimeError(""Running cythonize failed!"")
    RuntimeError: Running cythonize failed!

    ----------------------------------------
Command ""/cluster/home/sfux/pytorch_100_cupy90_540/bin/python /cluster/home/sfux/pytorch_100_cupy90_540/lib64/python3.7/site-packages/pip/_vendor/pep517/_in_process.py prepare_metadata_for_build_wheel /tmp/tmpx_wvk44c"" failed with error code 1 in /tmp/pip-install-_5e8ppld/numpy
You are using pip version 19.0.3, however version 20.3.3 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
(pytorch_100_cupy90_540) [sfux@eu-login-20 ~]$
```",2021-01-08T14:28:15Z
756797245,"There was only one significant comment, on the mailing list from @mattip. I addressed that here with the same answer I gave on the mailing list. I'd like to merge this PR when CI is happy and then follow up on the mailing list to propose this NEP moves from Draft to Accepted.",2021-01-08T14:55:07Z
756813378,"Oh man, should have just run the tests even for a comment change I guess...",2021-01-08T15:23:18Z
756816696,"The concatenate docstring is a bit weird, because its signature line comes from C and is just  a string, but actually does include the invalid syntax. I admit it is probably nicer to read, so I am not sure we have to change it.",2021-01-08T15:29:17Z
756823948,"Apologies, this is a duplicate issue #17784 ",2021-01-08T15:42:04Z
756830704,"I'm using python3.9 on a Big Sur 11.1 and I can confirm that the following has worked for me:

```
wget https://files.pythonhosted.org/packages/e1/c5/65b2f257a154c7fabc1895b435e7863a1f0bb1769d3c28f1500976e090ee/numpy-1.19.5-cp39-cp39-macosx_10_9_x86_64.whl
pip3 install numpy-1.19.5-cp39-cp39-macosx_10_9_x86_64.whl 
```

EDIT: I'm using a 2019 mac with the intel processor",2021-01-08T15:53:27Z
756846438,"The problem has been fixed in Homebrew, so with the latest `python` installed a simple `pip install numpy` works.",2021-01-08T16:16:46Z
756912296,"Thanks, but I am going to close, this. I could imagine adding a link to the NumPy website at https://numpy.org/install/, but I don't think we need an installation section in the readme probably. I expect those actually interested in a source install will find the docs from there or by searching?",2021-01-08T18:05:06Z
756931917,"@kennyfrc The wheel is for intel chips. It doesn't work for M1 chips. I couldn't find a whl for M1 chips (arm64).
@cbrnr Didn't solve the issue. I just updated brew, brew install python and pip3 install numpy didn't work.
Big Sur 11.0.1
Apple M1 chip
python 3.9.1
pip3 20.3.3  ",2021-01-08T18:45:56Z
756936573,"You are right, the problem was only solved for x86.",2021-01-08T18:56:15Z
757056246,"> There are some tests in `numpy/tests/test_reloading.py` that call an external process the right way. That said, I am not sure this is worth that much trouble, can you even be sure that the interrupt comes at the right time?
> 
> I like to have a test, but if it is tricky or weird, I am personally OK without one here. Just curious, but I suspect this can't be measured even if you try something like `np.array([[0]] * 10000)`?

I'm afraid I might be out of my element trying to answer this. I ran some basic tests and there's still some deviation in the time it takes to execute `np.array([[0]] * 10000)`. I don't know how this deviation might differ accross different platforms, or if its neglidgeble ",2021-01-09T00:08:00Z
757064324,"Don't worry about it, I would be extremely surprised if it is measurable in this place. We could add something like that to the benchmarks (I don't think we have a ""many sequences"" one), but really no need. ",2021-01-09T00:43:32Z
757073968,"Ok, I will go ahead and just remove the test for now then",2021-01-09T01:38:35Z
757099973,Thanks! @seberg @eric-wieser I'll give a thought about it. @mattip,2021-01-09T05:30:36Z
757106135,Thank you for everything <3,2021-01-09T06:41:38Z
757109839,"@mattip, please be aware that `__cpu_features__` affected by the environment variable `NPY_DISABLE_CPU_FEATURES` which allows the user to disable certain features in runtime.",2021-01-09T07:21:16Z
757119822,"Thank you for your report.
This bug has been reported by #18092, and already resolved by #18100 as @charris mentioned.",2021-01-09T09:04:06Z
757123004,I had the same issue. Had to Reinstall **NumPy version 1.19.4** manually to fix the error.,2021-01-09T09:33:46Z
757131548,"You are using pip version 19.0.3, however version 20.3.3 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.",2021-01-09T10:52:05Z
757137765,"This seems to be caused by `Generator.shuffle()` calling `ndim()` in order to support the `axis` parameter:
https://github.com/numpy/numpy/blob/e4feb7027e397925d220a10dd58b581b87ca1fec/numpy/random/_generator.pyx#L4393

It seems like this step can be skipped when `axis >= 0`. Not only because of this warning, but also because of the overhead of `asarray`.",2021-01-09T11:47:10Z
757199994,"OpenBLAS is more memory greedy by default. The short history of this difficulty is:

- 1.19.3 uses newer OpenBLAS to work around fmod problem, uses more memory by default https://github.com/xianyi/OpenBLAS/issues/2970
- 1.19.4 uses older OpenBLAS, has fmod problem
- 1.19.5 uses even newer OpenBLAS to work around fmod problem, tries to use less memory

These memory problems seem docker specific, I expect the environment isn't providing accurate information about resources. The best solution going forward may be to using OPENBLAS_NUM_THREADS to limit the number of threads.

@mattip @martin-frbg Here we go again.",2021-01-09T13:39:52Z
757220992,"Hm, I thought numpy had settled for building OpenBLAS with `BUFFERSIZE=22` to keep the ""old"" memory footprint with 32MB GEMM buffers (at the expense of risking segfaults with large matrix sizes) ?",2021-01-09T14:01:53Z
757243417,"@martin-frbg It is actually `BUFFERSIZE=20`. This may be a different problem, but it looks the same at first glance.",2021-01-09T14:28:37Z
757331040,"Since the function is documented to support sequences, I agree that this was an oversight. Technically a regression in 1.18 (the API was new in 1.17 though).",2021-01-09T16:29:07Z
757335661,"@seiko2plus - while sharing worries about duplication, your solution does seem substantially nicer than using jinja2 - we wrapped a C library using ufuncs and the resulting jinja [template file](https://github.com/liberfa/pyerfa/blob/master/erfa/ufunc.c.templ) is not pretty. Much of the work also ends up being done in python anyway, as we base the template on c function comments.
That said, @eric-wieser is also right to warn about problems for newcomers - I never really understood what is happening in the `*.c.src` files...",2021-01-09T17:02:25Z
757350003,Thanks @seberg,2021-01-09T18:45:51Z
757350211,"Thanks @oneiro-naut and @eric-wieser
Is this true then:

In Python 3, ""int"" is an arbitrary precision type but numpy still uses ""int"" to represent the C type ""long""?

For reference:
>>> x = np.lcm(219060189739591200, 43)
>>> print (type(x))
<class 'numpy.int64'>

while in sympy
>>> x = sympy.lcm(219060189739591200, 43)
>>> print (type(x))
<class 'sympy.core.numbers.Integer'>",2021-01-09T18:47:39Z
757351184,@whitty what is the lower bound of the `softlimit` for 1.19.3 (and if it is not too much trouble 1.19.2)?,2021-01-09T18:55:00Z
757351473,Thanks @seiko2plus ,2021-01-09T18:57:24Z
757354244,"> The only sizable suggestion is that we should consider employment a type of in-kind donation like the others (i.e., move it to where you currently have the amounts listed). That way, there is no special category, and it falls in with the rest of the guidelines.

This is connected to several review comments @stefanv made about merging the Sponsors and Institutional Partners concepts.

@InessaPawson replied to that suggestion: _""From the community building perspective, I quite like the concept of institutional partnerships. It might be just a matter of semantics in some cases, but psychologically Id imagine, maybe naively, that orgs and ICs will have a greater sense of belonging and ownership if we assign them as partners instead of sponsors. Using the Jupyter Lab metaphor, its a difference between fostering a puppy from an ASPCA shelter vs simply donating money to them.""_

My thoughts:
I did consider doing that merge when writing, but rejected the idea. On the one hand it makes things simple, and we wouldn't have to potentially make decisions in the future about the fuzzy boundary between the two. It hasn't been a problem so far though, and Inessa's reasoning makes a lot of sense - in my mind it works like that indeed. 

Let's see how this has worked so far in practice:
- We currently have two Institutional Partners, UC Berkeley and Quansight.
- In both cases, there's a strong ongoing relationship with multiple maintainers being employed by those partners and contributions spanning multiple years.
- Those connections are headed currently by @stefanv for Berkeley, and by me for Quansight. However, the bus factor isn't one - if Stefan or me would disappear, other people employed by those partner institutes would take over; contributions to NumPy would be impacted of course, but not immediately grind to a halt.
- We have four sponsors listed on https://numpy.org/about/: Sloan, Moore, CZI and Tidelift. Each of those has provided $$, not employee time.
- So right now there's a clear divide, and it would be valuable to keep the partner and sponsor concepts separate.
- Amazon was contributing employee time for a while. They pulled out again, but if they had stayed they'd be a Sponsor now. I think letting an entity move to Sponsor level first, and once the contributions have gone on for longer and the commitment grew move them to the Institutional Partner level would make sense to me.

So I'd be in favour of keeping both concepts.

Also note that in case we did want to merge them, it'd require a significant update to the governance document, so I'd want to do that separate from this PR.",2021-01-09T19:19:42Z
757354641,"> In Python 3, ""int"" is an arbitrary precision type

Yes

> but numpy still uses ""int"" to represent the C type ""long""?

`np.dtype(int)` is the C long type - but `np.array(2**62)` will always pick np.int64 which is sometimes ""long long"".
",2021-01-09T19:23:40Z
757365054,"Don't worry about the warning.

No issue to fix in NumPy, Matti already gave the correct workaround, so I'll close this now.

",2021-01-09T20:48:27Z
757367743,"Thanks @ronaldoussoren, we hope to do so when it's feasible (in addition to thin `arm64` wheels, which seem preferable long-term). One blocker is that we build all our wheels on CI services, and none of them support MacOS ARM64 yet. And none of us have the hardware at the moment.

We also still have some issues, in particular this build issue: gh-17807. And this performance issue: gh-17989. 

I'll link https://github.com/joerick/cibuildwheel/issues/473 here as probably the most relevant issue for `universal2` wheel building.",2021-01-09T21:10:57Z
757368305,This should be picking up a `1.19.x` wheel though - maybe `pip` too old?,2021-01-09T21:15:03Z
757372400,"> So I'd be in favour of keeping both concepts.

Thanks for considering that proposal, Ralf.  I think we can also clarify the document without merging the concepts (in fact, the lists you provided above are quite convincing that those are two very different scenarios).  Perhaps a sentence after the list of dollar amounts that forward reference the discussions on institutional partners.

Our governance document currently states:

> An Institutional Contributor is any individual Project Contributor who contributes to the project as part of their official duties at an Institutional Partner. Likewise, an Institutional Council Member is any Project Steering Council Member who contributes to the project as part of their official duties at an Institutional Partner.

> With these definitions, an Institutional Partner is any recognized legal entity in the United States or elsewhere that employs at least 1 Institutional Contributor of Institutional Council Member. Institutional Partners can be for-profit or non-profit entities.

> Institutions become eligible to become an Institutional Partner by employing individuals who actively contribute to The Project as part of their official duties. To state this another way, the only way for a Partner to influence the project is by actively contributing to the open development of the project, in equal terms to any other member of the community of Contributors and Council Members. Merely using Project Software in institutional context does not allow an entity to become an Institutional Partner. Financial gifts do not enable an entity to become an Institutional Partner. Once an institution becomes eligible for Institutional Partnership, the Steering Council must nominate and approve the Partnership.

Perhaps we can retire that portion of the governance document and have it be superseded by this NEP, and then have this NEP specify more clearly what defines those relationships and how those relationships work.  The first two paragraphs above make it very unclear whether Amazon would, in fact, have been a sponsor or an institutional partner.",2021-01-09T21:49:17Z
757377383,@eric-wieser  has there been any progress with this?,2021-01-09T22:32:48Z
757384294,"> @whitty what is the lower bound of the `softlimit` for 1.19.3 (and if it is not too much trouble 1.19.2)?

Updated table in original:

| version | approx minimum import limit |
| ------- | -----------------|
| 1.19.4  |  `softlimit -a 400000000 python3` |
| 1.19.5  |  `softlimit -a 1200000000 python3` |
| 1.20.0rc2  |  `softlimit -a 1200000000 python3` |
| 1.19.3  |  `softlimit -a 1200000000 python3` |
| 1.19.2 |  `softlimit -a 400000000 python3` |",2021-01-09T23:37:26Z
757387500,"> The best solution going forward may be to using OPENBLAS_NUM_THREADS to limit the number of threads.

For reference I get get numpy==1.19.5 loaded in a limit of 400000000 (some as 1.19.4) using `OPENBLAS_NUM_THREADS=3`.

Relating this to number of threads changes my view of this somewhat - `softlimit -a` is restricting all process _mappings_ - not actual memory usage.  It sounds like the culprit here is threads reserving their stacks, which doesn't seem like a risk to us for limiting concurrent memory usage.

That said I'm not sure I can find a satisfactory alternative rlimit that suits our requirements.

> These memory problems seem docker specific, I expect the environment isn't providing accurate information about resources.

Do you have any specifics here? is OpenBLAS trying to detect resources for scaling?  All of our builders are docker-based (to validate multiple OS versions) and I observe the failure isn't 100% reliable across all builds.  Perhaps some combination of host OS and container OS is implicated?",2021-01-10T00:07:56Z
757393322,"> Do you have any specifics here?

No :) I'm just spitballing. Someone who knows more about how OpenBLAS preallocates memory will need to address that. I find it curious that the memory usage didn't change between 1.19.3 and 1.19.5, my understanding is that it should have gone down, although I suppose that it is possible that the number of threads allocated increased in 1.19.5.",2021-01-10T01:02:53Z
757395097,I'm running into this on an NVIDIA Jetson (aarch64) - took me a while to isolate it. Funny thing is that it fails in a virtualenv but seems to be working if you install it at the system level. The NVIDIA NGC containers from https://github.com/dusty-nv/jetson-containers install `numpy` directly into the system libraries and they don't have this issue.,2021-01-10T01:19:53Z
757396917,Does this need a backport?,2021-01-10T01:33:31Z
757397032,"I tagged it for backport, if it doesn't need it, please remove the tag.",2021-01-10T01:34:51Z
757406071,"Just curious about something since I've been following the work to add typing support for dtypes: won't making a mutable container type covariant cause issues? For the usual reasons that, e.g. if `my_func` accepts `ndarray[Any, np.dtype[np.float64])` it could assign values of float64 to the array, but calling `my_func` with `ndarray[Any, np.dtype[np.float32])` would no longer raise typecheck errors despite being wrong.",2021-01-10T03:15:59Z
757415875,There's open issue already #4352,2021-01-10T04:55:24Z
757433729,"Yes, I should have marked it for backport since otherwise compilation with visual studio 2019 fails.",2021-01-10T07:47:10Z
757443637,"I ran into gh-17807 myself, my crude workaround was to remove the code adding ""-faltivec"" from setup.py. That worked for me, but is not a proper solution.  I'm a very light user of numpy through pandas at best, I'm basically only using pandas to create graphs from a couple of CSV files with some light calculations.

That said, I can't promise quick responses but feel free to let me know if there's something I can do or test. 

BTW. What's needed to build NumPy in a way that matches the official wheels?  Looking at the azure-pipelines.yml in the repo I'd say gfortran and OpenBlas as the only non-system dependencies. Is that correct?  ",2021-01-10T09:17:10Z
757447665,"Accelerate should be avoided for NumPy as it has bugs. NumPy doesn't use gfortran, SciPy does. But there currently no arm64-darwin OpenBLAS build on https://anaconda.org/multibuild-wheels-staging/openblas-libs/files. I am not sure how we would build that without support for arm64-darwin from a CI system.",2021-01-10T09:50:55Z
757448101,I opened MacPython/openblas-libs#49 about building OpenBLAS for ~Allple~ Apple M1 silicon.,2021-01-10T09:54:07Z
757451297,"> I am not sure how we would build that without support for arm64-darwin from a CI system.

For *building* you only need a CI system that provides macOS 11 hosts, or with some effort macOS 10.15 hosts with Xcode 12.2.   For *testing* you actually need M1 systems in the CI system, which will likely be a blocker for you and could take some time (I have no idea how constrained the supply of M1 systems is, but I expect that a CI provider like Azure Pipelines will require a lot of M1 systems).
",2021-01-10T10:20:56Z
757451600,"> Accelerate should be avoided for NumPy as it has bugs. 

Have you (the numpy project) files bugs about this in Apple's tracker? Or are these bugs in numpy's use of Accelerate?

",2021-01-10T10:22:54Z
757454298,"> Have you (the numpy project) files bugs about this in Apple's tracker? Or are these bugs in numpy's use of Accelerate?

Yes, multiple. Also other scientific projects - Apple basically doesn't care and hasn't been serious about maintaining Accelerate in a long time. Not enough value in it for them I guess.

SciPy has more linear algebra functionality than NumPy, and dropped Accelerate earlier than NumPy.

We have two good options for BLAS/LAPACK - OpenBLAS (our default for wheels) and Intel MKL. So we'd rather spend our effort on OpenBLAS rather than work around Apple's poor support.",2021-01-10T10:43:43Z
757454936,"> For testing you actually need M1 systems in the CI system, which will likely be a blocker for you and could take some time

Yes, I'd be very nervous about releasing something that we can't test. Our stance for other platforms (e.g. Linux on ARM64) is that CI services need to be available; in the absence of that we merge fixes but don't release wheels.

> That said, I can't promise quick responses but feel free to let me know if there's something I can do or test.

Thanks!

> BTW. What's needed to build NumPy in a way that matches the official wheels? Looking at the azure-pipelines.yml in the repo I'd say gfortran and OpenBlas as the only non-system dependencies. Is that correct?

In addition to what @mattip said, here is the actual wheel build machinery: https://github.com/MacPython/numpy-wheels",2021-01-10T10:47:36Z
757455123,"I feel your pain, I'm hearing similar experiences from app developers and most issues I've filed with them have gone completely unanswered :-( ",2021-01-10T10:48:56Z
757459007,Could you add the actual module path as well (`Cython.__file__` or so)?,2021-01-10T11:19:24Z
757465313,"Nope, OpenBLAS does not currently do resource detection beyond counting the number of available cores.Is that number constant for all entries in your table of minimum softrlimits above ? (In a way it would make me happy if the problem turned out to stem from something entirely different than the GEMM buffer, but I do not remember anybody creating huge sinkholes elsewhere in the code)",2021-01-10T12:05:48Z
757476401,"@ganesh-k13,

> I gdb'd into the code, specifically FLOAT_divide, that in turn goes into loops_arithm_fp.dispatch.c.src

Nothing to do with `loops_arithm_fp.dispatch.c.src`, FLOAT_divide only performs true divide.
However, this issue has been fixed by #16161 since if non-zero divided by zero should return inf. ",2021-01-10T13:28:31Z
757480157,"> For information, as a short-term stop gap for numpy users on the new Apple machines, it's possible to get the full stack (Python, numpy, scipy, openblas, gfortran compiler, pandas, scikit-learn...) working natively on Apple Silicon M1 using the [miniforge installer](https://github.com/conda-forge/miniforge#download) from the conda-forge community project: choose the `osx-arm64` platform and then install the packages with conda or mamba instead of pip to grab everything from conda-forge instead of PyPI.
> 
> It would be good to get native arm64 macos wheels on PyPI but that requires upgrading the full wheel building infrastructure to setup cross-compiling on the intel based macos workers on Azure Pipelines and co (this is how conda-forge did it, using https://pypi.org/project/crossenv/).
> 
> For those interested, I also patched numpy master to build against Big Sur's Accelerate, just to see the speed difference with OpenBLAS on this hardware. SGEMM is 1.7x faster with Accelerate but LAPACK operations (e.g. `np.linalg.svd`) can be slower because of better LLVM OpenMP-based multithreading of the conda-forge OpenBLAS. More details on this twitter thread: https://twitter.com/ogrisel/status/1335260559172841474

Works flawlessly. Data science full stack is possible without too much trouble with Apple Silicon. Much appreciate for all the hard work.",2021-01-10T13:54:37Z
757488155,"While the error comes from a potentially unexpected place on linux, I can't reproduce the crash. Just to be sure, you are not running on a apple silicon M1?",2021-01-10T14:45:24Z
757490636,"Nevermind, as the traceback reads, its a stack issue so just can be hard to reproduce.",2021-01-10T15:01:46Z
757512877,"@seberg see the linked issues

this casts python objects to strings which is a huge breaking change",2021-01-10T17:30:59Z
757515028,"Hmmm, thanks for the note. I can't reproduce it immediately, so let me dig in. It can't really be this PR, more likely and an error in  gh-18115 or even gh-13578",2021-01-10T17:46:27Z
757524621,"It appears that I simplified the test case too far on my local machine. In the Linux environment where I encountered this first (`1.19.2 3.8.6 | packaged by conda-forge | (default, Oct  7 2020, 19:08:05)
[GCC 7.5.0])`, the 33-element tuple raises an exception, as it is probably supposed to:

```
Traceback (most recent call last):
  File ""<string>"", line 6, in <module>
ValueError: Cannot construct an iterator with more than 32 operands (33 were requested)
```

If I increase the length of the tuple to 34, however, I get a segfault with the following backtrace (truncating at `PyEval_EvalFrameEx`): 

```
#0  0x00007f72283d149e in arraymapiter_dealloc ()
   from /scratch/jvs/miniconda3/envs/ampel-v0.7/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so
#1  0x00007f72283d4dba in PyArray_MapIterNew ()
   from /scratch/jvs/miniconda3/envs/ampel-v0.7/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so
#2  0x00007f72283d5b8f in array_subscript ()
   from /scratch/jvs/miniconda3/envs/ampel-v0.7/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so
#3  0x000055d3c76add58 in PyObject_GetItem (o=<numpy.ndarray at remote 0x7f722fc4ce90>, key=<optimized out>)
    at /home/conda/feedstock_root/build_artifacts/python-split_1602094424782/work/Objects/abstract.c:154
#4  0x000055d3c77082ca in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>)
    at /home/conda/feedstock_root/build_artifacts/python-split_1602094424782/work/Python/ceval.c:1584
#5  0x000055d3c76f3490 in PyEval_EvalFrameEx (throwflag=0,
    f=Frame 0x7f722fc1e800, for file <string>, line 6, in <module> ())
    at /home/conda/feedstock_root/build_artifacts/python-split_1602094424782/work/Python/ceval.c:741
```",2021-01-10T18:50:44Z
757542575,"Thanks!  Increasing the number makes it reproducible easily (anything between 32 and 64 probably is technically broken), I already found the underlying issue and know the fix. I will fix it in the next few days, unless someone has interest in digging into it. (It is likely that this was unreachable code for a good time  although even then it has been around for many years.)",2021-01-10T20:54:41Z
757554152,Thanks @DWesl .,2021-01-10T22:20:56Z
757667501,"I could find another solution, which does not require numpy 1.19.5. I am therefore closing this issue. Thank you for your help.",2021-01-11T07:11:10Z
757708833,"> s that number constant for all entries in your table of minimum softrlimits above ?

Yes the only difference between the runs are the installed version of numpy",2021-01-11T08:27:36Z
757818736,"> ```python
> TypeError: '<' not supported between instances of 'float' and 'str'
> ```



> Here, no exception is raised, but the results are incorrect:

Both of these are expected behavior, and match the behavior of the builtin `sorted` function on lists.
We should probably update the documentation for `sort` to clarify that nans are sorted to the end _only for arrays of subtypes of `np.inexact`_.",2021-01-11T09:45:59Z
757826959,"> Could you add the actual module path as well (`Cython.__file__` or so)?

Good idea, done.",2021-01-11T09:51:15Z
757853777,"> Perhaps we can retire that portion of the governance document and have it be superseded by this NEP, and then have this NEP specify more clearly what defines those relationships and how those relationships work.

Agreed. There's no compelling reason for that to be in the governance document itself. I'll take that along in my planned update.",2021-01-11T10:17:32Z
757857653,"> Perhaps a sentence after the list of dollar amounts that forward reference the discussions on institutional partners.

agreed that would help, done",2021-01-11T10:24:36Z
757863466,Thanks @rgommers ,2021-01-11T10:35:44Z
757984190,"Hi @seberg , sorry for the long pause in the PR.
I noticed the following TC causing crash in specific environment. I think handling return value for `self_descr` for overflow should take care of it. 
Now to the overflow: https://github.com/numpy/numpy/blob/a190258d4e90f2a17a9469e5dd9fb5f4b045aa90/numpy/core/tests/test_getlimits.py#L68-L70
Is it defined behaviour to do `np.uint(-1)`?",2021-01-11T14:25:05Z
757985496,"@a-reich this is arguably more of a problem with the variancy of `number`, one that I'm not quite about what would be the best solution. Namely, making `number[~NBit]` invariant (instead of covariant) w.r.t. its precision would resolve the issue. On the other hand, it would also make precision-based casting even difficult than it already is, to the point where I'm not convinced it'd be worthwhile. The latter means ditching the limited cases where we can currently deal with precisions, _i.e._ changing the return types to the likes of `number[Any]`.

In any case, besides abovementioned issue the problem with leaving `ndarray` (and `dtype`) invariant is that it leads to some strange situations with the abstract(-ish) part of the `generic` hierarchy. For example, it would mean that a `float64` array would not be acceptable if an `inexact` array is expected.",2021-01-11T14:27:14Z
757997220,"> Which is not something we can easily do - but if we're going to expose an alias in `numpy.typing` as public API, why not make it the thing people should be using for new code when working with integers?

@rgommers If the plan is to expose them then I'd definitely agree with you. 

The current aliases are IMO too useful to remove in their entirety, so instead I'd propose to just give them a more apropiate. Perhaps something along the lines of `_IntLike_co` and `_FloatLike_co` (the same holds for [`_ArrayLike<X>`](https://github.com/numpy/numpy/blob/master/numpy/typing/_array_like.py))?",2021-01-11T14:46:00Z
758002854,"@stefanv There is the [`numpy.typing`](https://numpy.org/devdocs/reference/typing.html) docs page, though nothing specifically for `ndarray[~Shape, ~DType]` yet, as this PR was just aimed at making `ndarray` a parametrized generic. I'm planning on exposing a runtime-subscriptable version of `np.ndarray` soon, something which I'd like to accompany with a documentation update.",2021-01-11T14:55:06Z
758008127,"> Is it defined behaviour to do np.uint(-1)?

I think so, although it may not be formalized. C allows it, but maybe we should make an explicit choice.",2021-01-11T15:03:39Z
758025507,"Lets say that it was once explicitly implemented that `uint(-1)` will work, so for now I will say yes. For comparisons it would be best if this can't happen (the only problematic thing is probably uint64 == int64 using float and losing precision though).

It is something that we could think about changing, but I am not sure it is worth the trouble and how annoying the work around is (since using -1 for maxint is not uncommon, even if technicall undefined by the C standard.)",2021-01-11T15:31:22Z
758032496,`_co` sounds good to me.,2021-01-11T15:42:53Z
758059422,"@BvB93 I am trying to get a full picture for this. For your last example, it seems clear that `inexact` must behave covariant (as do all abstract dtypes).  The concrete dtypes/types must be invariant (in most ways at least), but because of that, my preferred solution is to not allow subclassing at all (and if we ever allow it, we would be very careful to note that it must be extremely limited  whatever that means exactly).

The `number[Any]`, I am a bit confused about how/where this is used (with a bit-width)?
My first thought would be that `number[Any]` would be covariant as well (it is an abstract dtype/type), but it would be ""invariant"" with respect to precision since a higher precision is not a subtype to begin with?",2021-01-11T16:15:08Z
758149393,"> oversight was able to split slip past the tests

Yep, I thought to delay merging, but the tests passed and I was rushing.",2021-01-11T18:48:52Z
758153829,"Luckily our ""extended"" test suits in pandas and astropy both caught it easily ;)",2021-01-11T18:56:20Z
758171894,"It is happening for me, using python 3.9 in a completely clean new virtualenv. It seems, as mentioned, it is because the change from `tp_print` to `tp_vectorcall_offset`. I cannot install numpy. I am using cython version 0.29.21.",2021-01-11T19:28:42Z
758175656,"I am still confused by this. Are you installing from source? In that case, maybe try a `git clean -xdf` (make sure to have no code changes applied or custom `site.cfg`). Also just to be sure `python3.9 -mcython -V` (assuming the python executable you are using is `python3.9`) is the right version and the `pip` you are using is right, by using `python3.9 -mpip` instead of pip directly, just in case.
Maybe @noelmcloughlin you had a solution? (I am not quite sure why removing ""snoflake-connector"" would make a difference)",2021-01-11T19:35:39Z
758187768,"Hmmm, this is an interesting problem. If we go down this road, I am wondering if this should actually go one step further:
Instead of allowing a more complex mini-language, expose a ""transform"" hook/function that returns the shape and axes mapping that we need to set up the iteration.  (I have to carefully think how it fits with [NEP 43](https://numpy.org/neps/nep-0043-extensible-ufuncs.html), but I doubt it will matter much. Signatures will probably be a bit annoying either way with NEP 42 and I just have to pick some decent solution.)

The signature is parsed into a simplified description here: https://github.com/numpy/numpy/blob/c946dbaed8737af68fee8de44cbde7169a8ec75c/numpy/core/src/umath/ufunc_object.c#L617

And this is used to construct the correct iterator (and fill in the additional fixed sizes in this, pretty large, code block (not quite all of this is the core dimensions): https://github.com/numpy/numpy/blob/c946dbaed8737af68fee8de44cbde7169a8ec75c/numpy/core/src/umath/ufunc_object.c#L2559-L2756

Long story short: I think we could do this, but it needs a good proposal (either extending the string syntax, or allowing to ""hook"" with a function) and some thought proposal. A very brief NEP based on the initial one at https://numpy.org/neps/nep-0020-gufunc-signature-enhancement.html may also make sense, if it is not very clear how to do it best.

I expect the main issue is that I doubt anyone from the core team can champion this at the time (happy to help though, @mhvk may have a opinions/thoughts on this), at least unless this is widely important/useful.  But maybe we get lucky :).",2021-01-11T19:58:07Z
758203616,"I like the idea very much! But implementing it directly will not be super easy - as it is, the code is not really set up to deal with relations between dimensions. It would also need some strong use cases (I'm still a bit sad my `n|1` idea for broadcastable core dimensions ended up not accepted, and that was mostly because the use case was considered too weak). 

I do really like @seberg's suggestion of having a possible call-back, as it would open up also completely different ideas (I vaguely feel I've lamented not having it, but am not sure what it was about any more...). Obviously, it would need something with a well-described API, so that packages like numba can use it (a good interface to the present parser would be a nice by-product...).",2021-01-11T20:28:16Z
758215303,"Hmmm, test failure in:
```
          out = subprocess.check_output(['nm.exe', '--help'])
```
because `nm` was not found, restarted the test probably flaky, just noting in case it happens more often.",2021-01-11T20:51:18Z
758222735,I have been seeing the nm.exe failure about once a week or so.,2021-01-11T21:05:21Z
758241818,"Looks safe and good to me, lets put this in. Thanks.  (I suppose another pass is that _filling_ the result array can be slow. But, I guess that will usually not do much more than double the time it already took, so it is less likely to ""trap"" someone in an interactive session.)",2021-01-11T21:38:52Z
758245271,"I am sorry, I was trying to install a previous version of numpy. Current version installs ok with python 3.9. It is ok now.",2021-01-11T21:45:48Z
758247164,"Thanks for the followup, will close the issue then.",2021-01-11T21:49:47Z
758258944,Thanks Sebastian.,2021-01-11T22:15:29Z
758262811,"Thank you, I'm really happy to help, if however small!

Perhaps I can play around with the implementation and propose a more effecient solution in the future (if possible.) For example, perhaps it would be more practical to check for signals every max_dims/2 times. 
",2021-01-11T22:23:15Z
758265456,"Yeah, there may be a better place to check. I was a bit hesitant to just putting it at the entry of the function. Frankly, it is probably so low-overhead, that would be OK as well, it just didn't seem high priority to check often.",2021-01-11T22:28:52Z
758333297,"@BvB93 I see what you mean about the abstract parts of the dtype hierarchy making it difficult. However, your point about invariance of `number` w.r.t. to precision is interesting.
I guess ultimately, there might not be a way to have the whole type system work ""correctly"" here, so it comes down to which way generates more false positives/false negatives and is more annoying for downstream users to patch over? For example, how common actually are operations with different precisions, and if lack of automatic inference by mypy means users have to annotate their casting explicitly how bad is that? Compared to the false negatives for assigning with wrong precision (which admittedly does not raise at runtime, but silently stores truncated results).",2021-01-12T01:31:34Z
758451927,"I only know basics of Python, but can offer some testing under Yocto/OpenEmbedded if it would help.",2021-01-12T07:02:12Z
758453296,"I'm really sorry, but my scheduled got filled up so this set of PRs still has some loose ends. Might be able to do it later this month, I'm still interested in the topic.",2021-01-12T07:05:15Z
758454540,"We've all got our lifes, no problem. I found this series after 12+ hours fighting with SciPy and Yocto and just couldn't pass by. It's great that you haven't abandoned it.

As an additional question - any chance of backporting this to 1.17? That's what's included in Yocto's current LTS and would make life easier for people who do custom work adding SciPy on top of it. I'm asking more in the sense of viability, not anyone's timelines and availability.",2021-01-12T07:08:13Z
758490399,"> > anybody have an eta on the 1.20.0 release?
> 
> We will make the release decision Jan 6. One of the reasons for the slight delay was to allow some downstream projects to work around an expired deprecation.

@charris Has the decision been made regarding the 1.20.0 release?",2021-01-12T08:22:17Z
758497328,"Hmm bit late, not sure if a piecewise function is intended to return more output than input. Could this be done with just pandas by indexing based on your condition  and then using the apply function.

df[condition].apply(function) or using .loc, .iloc",2021-01-12T08:35:36Z
758499362,"> As an additional question - any chance of backporting this to 1.17?

Numpy only maintains two versions at a time - so the only versions this patch could reach are 1.19 onwards, and it's likely that this won't make 1.19 at all and will end up waiting till 1.20 or 1.21.",2021-01-12T08:39:47Z
758552626,"As they say, `apply` is a convenience function & not the one to go for, if performance is important.",2021-01-12T10:12:44Z
758560638,"I am working on this and I have two questions. Firstly, some of the functions in ```numpy.lib.arraysetops``` return an array of unique values. Should the addition of ```return_index```, as an argument, return indices corresponding to any one of the duplicate values in the input array or will the index array contain all the indices of those values?

For example, should ```numpy.setdiff1d``` do the following:
```
    >>> import numpy as np
    >>> a = np.array([1, 2, 3, 2, 4, 1])
    >>> b = np.array([3, 4, 5, 6])
    >>> np.setdiff1d(a, b, return_index=True)
    [array([1, 2]), array([0,1])] # The second array contains the indices
```
Or :
```
    >>> import numpy as np
    >>> a = np.array([1, 2, 3, 2, 4, 1])
    >>> b = np.array([3, 4, 5, 6])
    >>> np.setdiff1d(a, b, return_index=True)
    [array([1, 2]), array([0,1,3,5])] # The second array contains the indices
```
To me, the second implementation choice seems more natural. What do you think? The same kind of question also applies for ```numpy.setxor1d, numpy.intersect1d```.

Secondly, ```return_index``` does not apply for ```numpy.ediff1d```.
",2021-01-12T10:26:47Z
758599066,"@mattip so I was able to sync the fork but now another problem has appeared. I cannot push my local repo to origin.
I've tried ```git push origin master``` and ```git push -u origin master```.
I have also commited the branch",2021-01-12T11:38:02Z
758609525,Im using the binary version of VSC,2021-01-12T11:58:23Z
758622228,"You cannot push to origin. You need to 
- fork this repo (look at the top right corner of this page)
- push your branch to your repo
- issue a pull request",2021-01-12T12:23:58Z
758634557,"I believe the failures are unrelated, but I'm not sure.",2021-01-12T12:49:44Z
758651930,"It looks like this can be closed - if the newest polynomial documentation is still not sufficient, feel free to re-open.",2021-01-12T13:23:24Z
758661236,"Pyarrow release or end of month, whichever comes first.",2021-01-12T13:40:29Z
758700164,Is there anything left to do in this issue or can it be closed? ,2021-01-12T14:40:03Z
758705632,"Unless we have concrete suggestions for improvements to the current documentation, I'm assuming this issue is no longer relevant. Feel free to re-open.",2021-01-12T14:48:42Z
758707183,Closing this as it does seem to be fixed in the [current documentation](https://numpy.org/doc/stable/reference/generated/numpy.gradient.html).,2021-01-12T14:51:09Z
758851445,"@seberg @mhvk Thanks for the fast reply and all the great links. I am very happy to see that there is actually some interest in this idea. I really like the idea of a ""transform"" hock/function, that would for sure address my needs. 

I suspect it indeed might be hard to argue for this though as direct use cases seem limited (I really only can think of the histogram one right now). I am happy to do a bit more research though and help work this out if there is something I can do with my ""limited"" knowledge of the `ufunc` internals.",2021-01-12T18:28:39Z
759057127,"> The number[Any], I am a bit confused about how/where this is used (with a bit-width)?

To give a bit of background: `number` is currently parametrized w.r.t. a set of `npt.NBitBase` subclasses, 
the latter used for representing their precision via an object-based approach. 

``` python
_NBit_co = TypeVar(..., covariant=True)

class floating(inexact[_NBit_co]):
    ...

# Note that the types below are not necasrelly treated as `floating` subclasses,
# it's more along the line of how `Sequence[int]` is a subtype of  plain `Sequence`.
#
# This greatly simplifies the rather complicated typing of precisions, as you can always 
# just hit the panic button when things get too complicated: _e.g._ `return floating[Any]`
float16 = floating[_16Bit]
float32 = floating[_32Bit]
float64 = floating[_64Bit]

```

The key point is that because `NBitBase` subclasses inherit from each other, whenever one of them is 
used as a covariant parameter it would allow mypy to simplify the likes of  `Union[_64Bit, _32Bit]` to
 `_64Bit`, allowing for some basic precision-based casting without adding loads of `overload`s. 

I recall from https://github.com/numpy/numpy/pull/17540 that something like this was actually possible for invariant parameters, but I 
believe there were more immediate issues with it, at least compared to its covariant counterpart. 
I'd have to a bit of digging, as I don't quite remember the details.
",2021-01-12T21:51:04Z
759059107,"The generated documentation:
https://17825-908607-gh.circle-artifacts.com/0/doc/build/html/reference/typing.html#numpy.typing.NestedSequence",2021-01-12T21:52:27Z
759060189,"~As a side note: 
Let's wait with merging until https://github.com/numpy/numpy/pull/18155 is merged, as it would allow for the removal of an overload.~",2021-01-12T21:53:36Z
759088579,"I can see this being nice to have, so I am not opposed. But I am a bit surprised that we want to make nested sequences a common thing for most command? I.e. I would expect it to be mainly a valid input for `np.array` and friends, and pretty much all other occurrences would require you to call `np.asarray` manually to type correctly?",2021-01-12T22:53:40Z
759098565,"> I.e. I would expect it to be mainly a valid input for `np.array` and friends, and pretty much all other occurrences would require you to call `np.asarray` manually to type correctly?

This is true for scripts that utilize `np.array`, but if `np.array` is called under the hood by some function then there is no easy way to reuse its annotations. Barring the case the where its is signatures is absolutely identical, that is (_i.e._ situations where we can abuse `__call__`-based protocols).

``` python
import numpy as np

# No easy way of grabbing the signature of `np.array` and putting it in `func` 
# (besides manually copying it, that is)
def func(a):  # ??? 
    return np.array(a) * 10
```

In practice this means that, unfortunately, we're stuck with quite a bit code duplication and thus the need for the likes of `NestedSequence`. Hence why I feel that making it public would worthwhile, especially for downstream libraries.",2021-01-12T23:18:46Z
759243814,"@vishalsharai-trail how did you install sudo apt-get install libatlas-base-dev in lambda function?


",2021-01-13T06:47:00Z
759329415,"I worked after I installed the latest version numpy in PyCharm. I was using sympy.plot
glad it worked.",2021-01-13T09:40:06Z
759341379,"Thanks. We know, working on it in https://github.com/numpy/numpy.github.com/pull/1",2021-01-13T10:01:12Z
759344126,Oh sorry for the duplicate then,2021-01-13T10:05:43Z
759344715,"no worries, I'll leave this open till it's fixed, or someone else will open a new issue",2021-01-13T10:06:48Z
759345516,"For who needs the docs, they're still accessible here: https://numpy.github.io/devdocs/",2021-01-13T10:08:12Z
759371165,"The new page "" https://numpy.org/devdocs/user/troubleshooting-importerror.html "" returns 404! This helps a lot!",2021-01-13T10:55:15Z
759389439,"@ronaldoussoren will the universal2 python.org installer pick up thin arm64 wheels if they are available and Python is started in native mode?

",2021-01-13T11:31:38Z
759403422,"That's more a pip question than python version, but yes pip will use arm64 wheels when running in natively on M1 Macs. ",2021-01-13T12:00:26Z
759405300,"@ronaldoussoren - sorry - just checking because I was surprised.  You're saying that if I do:

```
python -m pip install numpy
```

on an M1 machine, using the Python.org `universal2` Python,  and PyPI only has an `arm64` wheel for Numpy, it will neverthless install that wheel, even though it does not satisfy the `x86_64` part of `universal2`?",2021-01-13T12:04:28Z
759423151,"That's correct. That's something that surprised me too.

Even worse (IMHO) pip will prefer a native wheel over a universal2 one.  Most users won't care, except for folks like myself that wish to redistribute binaries to other systems (in my case mostly using py2app). I've filed an feature request about that, but the pip team is not convinced yet that preferring universal2 wheels for a universal2 build is a good idea.  I'll probably create a PR to show the impact this change would have on their code base.

Note that it is also possible to install an x86_64 wheel on an M1 system by running python in emulated mode. That is,

This will prefer an arm64 wheel (also without the invocation of the ``arch`` command) :

```
$ arch -arm64 python3 -m pip install numpy
```

This will prefer an x86_64 wheel:

```
$ arch -x86_64 python3 -m pip install numpy
```

P.S. I've not looked into the python3 included in Apple's compiler tools, that version might not support the ``arch`` command in this way.",2021-01-13T12:40:43Z
759431066,"@ronaldoussoren - ouch! - and thanks for working that through, that's very helpful.",2021-01-13T12:56:04Z
759433848,"Interesting. I'm not sure that's a bad thing. There are scientific packages that already exceed the PyPI version limit, so a doubling of wheel size for what (for scientific libraries) is a very niche use case doesn't seem very sustainable.

I didn't have the energy to jump in that packaging discussion, but I suspect scientific libs may prefer thin wheels. We work hard on keeping binary sizes reasonable.",2021-01-13T13:01:27Z
759434296,Should be working now. Please reopen if something is still not working. ,2021-01-13T13:02:20Z
759466278,"Yeah, I noticed that some some packages are very large. Those can always choose to not provide universal2 wheels at all, that will work for most users. 

At a risk of going completely off-topic, I'd like to have more control on what gets installed than pip is currently giving. When I'm doing something on my local machine only I might prefer a thin wheel (smaller, faster install), but when I intend to redistribute I might prefer a universal wheel (and possibly even one that's synthesised from two thin ones on PyPI). 
",2021-01-13T13:59:31Z
759473439,"More control would be nice indeed, if `universal2` stays - perhaps as both a `pip` command line argument and `.rc`-file setting.

I'd actually prefer if it didn't stay, because also for redistribution there isn't much reason to do it as `universal2`. The only good argument I saw was ""it's less effort to introduce"". There is no other situation where we mix OS and hardware platform support. The old `universal` PowerPC/Intel thing was also very annoying, and in my experience didn't work for the scientific stack anyway if you invoked it with the non-native arch.",2021-01-13T14:11:03Z
759475884,"Just superficially, wouldn't it be more simple for Python.org to provide separate M1 and x86_64 installers - as it does for Windows 32 and 64 bit?

It's hard to imagine many people using the `universal2` Python.org Python and always / mostly doing:

```
arch -x86_64 python3
```

And, as Ralf says, I bet that will usually break, when you get to install packages.",2021-01-13T14:15:11Z
759506911,"Separate installers requires users to understand what kind of machine they have.  With a universal2 installer things just work in that regard. The only problem is when projects do no provide wheels with native code for arm64 (either universal wheels or thin arm64 wheels), but that should correct itself in due time (especially once M1 systems are available in cloud CI systems).

I expect that it will be years before we can ignore intel Macs, even after Apple transitions all their hardware to arm64. 

Universal support is more or less required for folks distributing application bundles, having separate application downloads for intel and arm is just not what Mac users expect.   Universal support is not about being able to run x86_64 on arm, but about having a single binary that just works everywhere.",2021-01-13T15:03:58Z
759511015,"I don't think it's much to ask of a user that they know they have an M1 Mac, honestly.  And for now, it would be reasonable to make the x86_64 installer the default, so they have to specifically ask for the M1 build.   I presume an x86_64 build will also work on M1 via Rosetta?

What happens for:

```
arch -x86_64 python3 -m pip install numpy
```

Does this look for an `x86_64` wheel before a `universal2` wheel?",2021-01-13T15:10:16Z
759511567,"I don't understand what problem universal2 pip packages solve. If the package is pure-python, pip will download that. If the package has c-extension modules, pip should choose the proper hardware model.",2021-01-13T15:11:09Z
759517247,"I think the problem they solve is where the user sometimes wants to run Python in M1 mode, and sometimes in x86_64 mode.

They can choose their mode with the `arch -x86_64` prefix.

`universal2` wheels should properly install code for M1 and for x86_64, so `python -m pip install numpy` will mean that both of these will work after that single pip install.

```
python -c ""import numpy""
arch -x86_64 python -c ""import numpy""
```

The other problem is the one Ronald mentioned - a universal binary for Python itself means it will work on either hardware. 

I agree though, that the first use-case doesn't seem all that important, and the second seems a rather minor advantage compared to the cost in terms of packaging confusion.
",2021-01-13T15:19:47Z
759527181,"> Universal support is more or less required for folks distributing application bundles, having separate application downloads for intel and arm is just not what Mac users expect. Universal support is not about being able to run x86_64 on arm, but about having a single binary that just works everywhere.

That's specifically a general ""Mac end user"" problem though, and is unrelated to PyPI and wheels. Having thin wheels plus the right `py2app` tooling to glue two thin wheels together in a single `.pkg`/`.dmg` installer would be much better.

",2021-01-13T15:34:49Z
759527981,"> I expect that it will be years before we can ignore intel Macs, even after Apple transitions all their hardware to arm64.

Agreed, at least 6-7 years I'd think.",2021-01-13T15:36:01Z
759529087,"The shape seems clearly a bug.

`nanpercentile` and `percentile` should ideally return the same thing as well. I don't have much of an opinion on error vs NaN. (Percentile does seem a bit less clear than mean, which naturally goes to `0/0`, but especially with axis, a NaN result may be useful).",2021-01-13T15:37:46Z
759529893,There was a brief outage of most of the website.,2021-01-13T15:38:59Z
759701852,What hardware?,2021-01-13T19:52:19Z
759705470,"<img width=""198"" alt=""Screen Shot 2021-01-13 at 12 58 44 PM"" src=""https://user-images.githubusercontent.com/13019685/104504011-74a80580-55a7-11eb-9c47-86db6246ab94.png"">

Do you need more info?",2021-01-13T19:59:11Z
759707302,"No :0 M1 is not supported, we are working on it but we also need hardware testing support that we do not yet have. There are other issues open for this.",2021-01-13T20:02:29Z
759708936,"Ah, thanks for the info!",2021-01-13T20:05:40Z
759751404,I'm beginning to suspect this is all caused by the BUFFERSIZE=20 workaround not actually getting through to the compiler/preprocessor when building with gmake. (Looks like I dropped a crucial bit of my original patch that would make Makefile.system append the user-supplied BUFFERSIZE declaration to the CCOMMON_OPT). That _is_ a bit embarassing...,2021-01-13T21:30:11Z
759752952,"LGTM, thanks @alexhenrie.",2021-01-13T21:33:16Z
759753459,"In general it would be better to also use `PyMem_Malloc` and friends here instead, but isn't really important.",2021-01-13T21:34:26Z
759774279,Thank you!,2021-01-13T22:18:56Z
759807545,I'm not convinced by this change - isn't it sufficient to mark them const?,2021-01-13T23:00:50Z
759814756,"Sadly, marking them `const` produces compiler warnings because `PyArg_ParseTupleAndKeywords` takes a non-const array even though it doesn't actually modify the array. `git grep -F 'char *kwlist[]'` shows that most of these arrays were already marked `static` in NumPy, and none are marked `const`. I'm just proposing that we be consistent and add `static` to the ones that didn't have it already.",2021-01-13T23:05:58Z
759815730,"I just concluded the same thing. The approach seems odd, but it's even how CPython itself does it. I'll maybe play around with godbolt to see if `const` is ultimately better, but at the end of the day consistency matters more than microoptimization here.",2021-01-13T23:08:42Z
759816667,"> takes a non-const array even though it doesn't actually modify the array

As far as I remember this is due to a shortcoming in C (fixed in C++), where a `T const* const` argument cannot legally be passed a `T*`.",2021-01-13T23:11:17Z
759844651,"Hmm, shouldn't valgrind notice this type of thing?  I am wondering if we want to add a test for it, because I do not recall valgrind ever complaining about it (I run it semi-rgularly). OTOH, the `type_tuple_type_resolver` is a bit obscure and not used much (it is the `dtype=` argument and especially `signature=` to ufuncs).

Maybe just going to merge this as is soon. Just curious are these PRs based on static code analysis?",2021-01-14T00:30:04Z
759853414,I made this pull request and #18156 because of [scan-build](https://clang-analyzer.llvm.org/scan-build.html) warnings. #18161 was just something I noticed while looking through the code.,2021-01-14T00:54:58Z
759957507,"Sorry @seberg that I dropped the ball...

> `result_type` is one problem (the one that is now up for fixing, so if you have any ideas how an API could look like that allows you to insert complex32 into the NumPy type hierarchy, I am all ears).
> 
> For the ufuncs, if your ufunc currently supports `complex64` and `complex128` (but not integers/floats), the current way ufuncs work is a linear search. So an `int8` input, uses `complex64`, but if you add `complex32` that will be what you get (if you define that int8 can cast safely to complex32  or better if you say int8 and complex32 promote to complex32). I want to change that lookup (NEP 43).

Am I understanding it correctly that these are the two facets of the same problem, so if we fix one we fix them all? What if we make `complex32` a bit more restrictive than other dtypes by, say, making it only safe to cast to/from `complex64` but nothing further? Would it help to maintain the current type hierarchy as is? I don't know if it's even possible though, but this would more or less achieve the essence of a storage dtype (+ some very limited computing capability).

> Yes no scalars might eliminate some complexities, but at least to me it would seem a bit strange if `complex32` was the odd one out that behaved differently?

Not sure if I correctly get the different behavior that you referred to, but given that the actual computation performed on a `complex32` array would necessarily involve an upcast before (and downcast after), I think it's worth granting it a special status, by which I mean it mets the minimal requirement of serving as a functional dtype (i.e. storage + minimal compute). Am I making sense? 
",2021-01-14T06:29:39Z
759957542,"xref xianyi/OpenBLAS#3066

I added that change as a patch to openblas-libs and triggered a build in MacPython/openblas-libs#50. Once the tarballs are uploaded, I can try to play with the resulting openblas and see if it fixes this issue. ",2021-01-14T06:29:43Z
760076810,"For me the script
```
LIMIT=XXX; softlimit -a ${LIMIT}00000000 python -c ""import numpy; print(numpy.__version__)""
```

gave these results:
| version | minimum XXX |
| ------- | -----------------|
| 1.19.3  |  34 |
| 1.19.4 |  11 |
| 1.19.5 |  34 |
| with MacPython/openblas-libs#50 | 11 |

so once @martin-frbg makes that fix official we can build new openblas libs.

Aside: building with our openblas in a conda environment with openblas installed is painful: you need to override LDSHARED since otherwise conda python pulls LDSHARED from `sysconfig.get_config_var('LDSHARED')` which adds a `-L` directive to the conda-supplied libopenblas **before** the one added by `site.cfg`.",2021-01-14T09:34:53Z
760085809,"There is  a timeout. Toggling timestamps in the build/test log, it seems ~20 minutes elapse between [this line](https://dev.azure.com/numpy/numpy/_build/results?buildId=14951&view=logs&j=dd90ff3a-8ae6-53a3-f363-d8bacbbc3624&t=af744189-1e05-5b30-3fda-fb7a3b5967e3&l=44) and [this line](https://dev.azure.com/numpy/numpy/_build/results?buildId=14951&view=logs&j=dd90ff3a-8ae6-53a3-f363-d8bacbbc3624&t=af744189-1e05-5b30-3fda-fb7a3b5967e3&l=45)",2021-01-14T09:51:57Z
760170589,"Late reply, but thanks a lot for the quick follow-up and nice deprecation! 

Justed tested this, and this should work for us. There is still a difference compared to < 1.20: now `__array_interface__` is accessed (and so a return value constructed), while before it was never even accessed at all. But that shouldn't be a problem for us (I only noticed it because for a mock in a test the property did `raise NotImplementedError`).",2021-01-14T12:39:03Z
760219383,What are the next steps? Can I do anything to push this forward? I do not want this to become a stale PR.,2021-01-14T14:09:01Z
760248572,"Actually, I spoke too soon. The latest numpy gives a lot of failures in the geopandas test suite ... 
The reason is that Shapely actually *does* raise NotImplementedError in some of the `__array_interface__` properties.

A small example without relying on Shapely, adapted from above:

```
class Polygon:
    def __init__(self, coords):
        self._coords = np.asarray(coords)
    @property
    def __array_interface__(self):
        raise NotImplementedError(""Polygon does not expose array interface"")

poly = Polygon([(0,0), (1,1), (2,1)])
arr = np.empty((1,), dtype=object)
arr[:] = [poly]
```

So it's not anymore about the direct array creation with `np.array([poly])` (which now raises a deprecation warning), but actually about the ""create empty and assign""-workaround we have been using in geopandas to overcome the issues in the `np.array(..)` constructor (and which gets mentioned in the deprecation warning).

So the above raises with rc2:

```
In [2]: poly = Polygon([(0,0), (1,1), (2,1)])
   ...: arr = np.empty((1,), dtype=object)
   ...: arr[:] = [poly]
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
<ipython-input-2-99f5a9597e5d> in <module>
      1 poly = Polygon([(0,0), (1,1), (2,1)])
      2 arr = np.empty((1,), dtype=object)
----> 3 arr[:] = [poly]

<ipython-input-1-1ca53974a957> in __array_interface__(self)
      4     @property
      5     def __array_interface__(self):
----> 6         raise NotImplementedError(""Polygon does not expose array interface"")

NotImplementedError: Polygon does not expose array interface
```

while this worked in rc1. 
(and this is actually a more problematic regression for geopandas as the original one I reported, as we are using the above idiom a lot and cannot be easily fixed (apart from getting Shapely 2.0 out of the door sooner))",2021-01-14T14:56:23Z
760252396,"In one of the tests added in https://github.com/numpy/numpy/pull/17973, there is a note:

>         # NOTE: We actually do still call __array__, etc. but ignore the result
>         #       in the end. For `dtype=object` we could optimize that away.

So this doesn't seem to be true in practice? If the result would actually be ignored (or the error catched and ignored), then the issue would be fixed I think?


",2021-01-14T15:02:06Z
760255872,Thanks @DerWeh .,2021-01-14T15:07:07Z
760295634,"> Am I understanding it correctly that these are the two facets of the same problem, so if we fix one we fix them all?

Yes, you could hack around the ufunc behaviour by not allowing safe casts that are normally around (as well as promotions).  NumPy currently uses ""safe casting"" semantics for promotion (in ufuncs mostly), which in my current opinion mixes two concepts that do not really quite fit.

My hesitation is that `float16` is not very limited or special, even though it also is de-facto in the same category. So this would introduce a slightly ""odd one out"" dtype. Oh the other hand, one problem I just realized is that if you want to do this outside of NumPy getting `arr.real` and `arr.imag` right may require some additional method/protocol.

Also, you could argue that complex32 will be just as odd as `float16` with respect to all of these things. So maybe it just doesn't matter, as it is not like we can fix `float16` soon (so at least we have two dtypes with the same ""odd one out"" behaviour.",2021-01-14T16:09:18Z
760301783,"Heh, there was another issue with shapely, which lead us to ignore less errors when calling `__array_interface__` (gh-17785), etc. That probably is the reason for this change (i.e. gh-17817).

I am wondering now if I should go the 1.19.x route and only catch clearly ""bad"" errors (recursion and memory error), or keep it as is here and catch all errors but we add catch `NotImplementedError` explicitly.  I can see that `NotImplementedError` reads nice. For NumPy an `AttributeError` would be clearer if you want to say: I don't actually implement this.  (for backcompat reasons we should allow it anyway I guess, but in that case, maybe we need to be very lenient for now and maybe even deprecate)",2021-01-14T16:19:06Z
760326857,"Seems like a fixed pandas bug, the line `isinstance(np.bool_, type(np.dtype))` did change behaviour, but doesn't make much sense to begin with (if anything, it should be `isinstance(np.bool, np.generic)`).  Unfortunately, the only solution is probably to upgrade pandas as well (I only checked that 1.0 doesn't have the fix yet).",2021-01-14T16:59:02Z
760330209,"Right, so I was using incompatible version and can fix it by downgrading (which I did) or properly upgrading (which I will do). In the future, I suppose I should check pandas before posting a bug report.
Thanks! Closing.",2021-01-14T17:04:21Z
760334764,"Yeah, frankly, it would be nicer if a year old pandas would work without issues. But yeah, generally it is better if the base package (NumPy) is not (too much) newer than the downstream (pandas).
For NumPy the usual safe bet is that ~1 year newer should be fine (i.e. you may see warnings but everything works), this is an unfortunate exception.",2021-01-14T17:11:39Z
760372847,"I am not too surprised that we still have such errors in NumPy. I couldn't reproduce it immediately using the debug version shipped with debian (maybe it doesn't have full asserts).
If you can do it very quickly, does `2 == np.bool_` and/or `np.array([2]) == np.bool_` have the same problem?",2021-01-14T18:14:13Z
760384505,"Oh, I didn't fully parse your last comment.  So I guess raising the error may be fine for shapely/you if we ensure there is a special case for `object + last dimension reached` (which is not relevant except for the assignment case).  That special case is probably missing in the dtype discovery code right now. (Note: that would not help with `np.array(..., dtype=object)`, however)",2021-01-14T18:25:25Z
760385133,"`2 == np.bool_` is fine.

`np.array([2]) == np.bool_` crashes.

Here's another example:
```
In [3]: class Foo(object):
   ...:     __array_priority__ = ""bar""
   ...:

In [4]: np.array([2]) == Foo()
Assertion failed: (!PyErr_Occurred()), function _PyObject_FastCallDict, file Objects/call.c, line 90.
Abort trap: 6
```

I think the key is that `np.bool_` has an `__array_priority__` member but it isn't a float.

```
In [3]: type(np.bool_.__array_priority__)
Out[3]: getset_descriptor
```",2021-01-14T18:26:35Z
760389943,"Thanks, great debugging/tracking down! That explains where the error originates, I was thinking of the wrong place first and couldn't make sense of it.
The issue is here: https://github.com/numpy/numpy/blob/79d23f0bdc6a5e6bcd280cd93f73fabca8e2469f/numpy/core/src/common/binop_override.h#L148 although it is not quite clear of `GetPriority` might suppress the error (I don't like error supression for no reason, but here it may make sense, unless we first ensure that we look up the priority only on instances.)",2021-01-14T18:35:23Z
760405097,"@jorisvandenbossche to decide how to best fix this:
* `np.array(Polygon(), dtype=object)`   Should not error, or is it allowed to error?
* `np.array([Polygon(), dtype=object])`  Same question

Maybe @eric-wieser will also have a quick opinion.  The question is whether this `NotImplementedError` is expected to never show up, or only show up in certain conditions.  I take it as granted that:
```
arr = np.empty(1)
arr = [Polygon()]
```
must **not** error, but if we ignore the error in the first two examples, that will fall out without any special handling (although technically, it might be better to special case it anyway, due to the `NOTE` comment you found).",2021-01-14T19:03:42Z
760413171,"Thanks, I will just merge this. It doesn't seem to fix an actual bug, but is correct.

Whether or not this causes undefined behaviour seems to not make a difference in this function (because it is used solely for error reporting, and that branch doesn't do nice error reporting).  (This function is a mess, but I hope the solution is to just delete all of it in the next months.)",2021-01-14T19:19:57Z
760494361,"Hi @Gnoblin44 , thanks for your PR. In order to get the right people to review it, it's better to be more explicit in the title and description. Similarly, your commit message [should be descriptive](https://numpy.org/doc/stable/dev/development_workflow.html#writing-the-commit-message) in order to add context to this fix. ",2021-01-14T21:42:07Z
760507699,"If this is still relevant, I'm wondering if the best place to add this discussion on the `@` operator is [this page on linear algebra](https://numpy.org/devdocs/reference/routines.linalg.html). Note that this page is actually a mix of common linear algebra operations and functions in the `np.linalg` module - maybe they should be split up? Finally, the [ndarray](https://numpy.org/devdocs/reference/arrays.ndarray.html) documentation says:

> Matrix operators `@` and `@=` were introduced in Python 3.5 following PEP465. NumPy 1.10.0 has a preliminary implementation of `@` for testing purposes. Further documentation can be found in the matmul documentation.

Can we lose the *preliminary/testing purposes* warning?",2021-01-14T22:10:10Z
760525444,"Is this something we still want to do? I'm guessing since this hasn't been brought up in a while that we don't need the manpage, and this issue can be closed. ",2021-01-14T22:47:58Z
760572480,"i would still like to have a manpage associated with the tool, at least that's what we prefer in Debian to have (but it's definitely not a strong requirement)",2021-01-15T00:53:30Z
760666156,"same problem to me while trying installing pandas under macos big sur 11.1 
python is 3.8 and pip is 20.3.3",2021-01-15T06:00:25Z
760787143,"> If this is still relevant, I'm wondering if the best place to add this discussion on the `@` operator is [this page on linear algebra](https://numpy.org/devdocs/reference/routines.linalg.html). 

I would say yes to mentioning `@` there and a one line to one paragraph summary of what it does. Not all users know what it is, so that would make it more discoverable. For the rest it should refer to the `matmul` docs I'd say.

> Note that this page is actually a mix of common linear algebra operations and functions in the `np.linalg` module - maybe they should be split up?

I don't think so. The docs are arranged by functionality, not by module. And the split between linear algebra functionality in the main namespace and the `linalg` one is mostly an accident of history.",2021-01-15T09:44:09Z
760889970,"oopsie daisy, silly me",2021-01-15T11:34:32Z
760979877,"For the first two cases:

> np.array(Polygon(), dtype=object)  Should not error, or is it allowed to error?
> np.array([Polygon(), dtype=object])  Same question

I think it would be nice if they don't start erroring *now* (since it works in released numpy). But if numpy would prefer to bubble up the error, maybe something similar could be done as for the case that `__array_interface__` doesn't error: raise a warning that in the *future* this object will be coerced as `np.array(obj)`(and thus in this case will effectively raise the error), but for now keep the existing behaviour of not raising. 

Now, on the preferred long-term behaviour: never show such a NotImplementedError or not, I am not fully sure. On the one hand, it seems consistent with the current deprecation warning that objects with one of such protocols will ""be coerced as if it was first converted using `np.array(obj)`"" regardless of wether the protocol raises or not. On the other hand, numpy could treat such an error as an indication of ""I don't want to be converted to an array"" (but the library author could in such a case also simply leave out `__array_interface__` entirely?).
",2021-01-15T14:40:03Z
761059759,"Closing this for now, as further testing has unfortunately revealed a number of detrimental mypy bugs/limitations  :

* `NestedSequence` fails as function annotation if the, respective, passed parameter has previously 
  not been assigned and/or declared.

``` python
from typing import TypeVar, List, overload
import numpy.typing as npt

@overload
def func1(a: npt.NestedSequence[bool]) -> bool: ...
@overload
def func1(a: npt.NestedSequence[int]) -> int: ...

int1 = [[1]]
int2: List[List[int]]

# note: Revealed type is 'builtins.int'
# This is ok
reveal_type(func1(int1))

# note: Revealed type is 'builtins.int'
# This is also ok
reveal_type(func1(int2))

# note: Revealed type is 'builtins.bool'
# This is bad; how did we end up at the `bool` overload all of a sudden????
reveal_type(func1([[1]]))  

```

* The use of `TypeVar`s is just straight-up broken.

``` python

T = TypeVar(""T"")

def func2(a: npt.NestedSequence[T]) -> T: ...

# error: Argument 1 to ""func2"" has incompatible type ""List[List[int]]""; expected ""NestedSequence[<nothing>]""
reveal_type(func2(int1))

# error: Argument 1 to ""func2"" has incompatible type ""List[List[int]]""; expected ""NestedSequence[<nothing>]""
reveal_type(func2(int2))

# note: Revealed type is 'Any'
reveal_type(func2([[1]]))
```",2021-01-15T16:58:23Z
761061078,"Never mind https://github.com/numpy/numpy/pull/18128#issuecomment-759060189, turns out there were, unfortunately, detrimental problems with the therein referenced PR",2021-01-15T17:00:51Z
761099971,"Pinging @seberg and @a-reich.
Continuing from https://github.com/numpy/numpy/pull/18128#issuecomment-759057127, it turns out that the `np.number` variancy could be changed without issue.",2021-01-15T18:12:32Z
761101862,"This is expected. `p=None` triggers a (faster, more accurate) codepath for specifically for unweighted sampling. When you give it an explicit `p`, we use a different algorithm for weighted sampling. There is not a good way to test the explicit `p` that it is equivalent to being unweighted because of floating point inaccuracies.

I can understand the desire that these two give the same answers, but I don't see a good way to reconcile the two except to abandon the faster and more accurate algorithm for unweighted sampling, which I don't think would be a good idea.",2021-01-15T18:16:00Z
761108997,"Thank you for your detailed explanation, I totally understand and agree with you! 
I assumed that they are equivalent and so this caused a bit of a headache when I was writing unit tests.
Would it be possible to highlight this behaviour somehow in the documentation? ",2021-01-15T18:29:16Z
761118952,"This is a duplicate of #18131, seems to be due to the compiler rearranging code.",2021-01-15T18:49:07Z
761243857,@BvB93 needs a rebase.,2021-01-15T23:06:57Z
761245101,@seberg needs rebase.,2021-01-15T23:10:36Z
761248195,"FWIW, if I set `NPY_NUM_BUILD_JOBS=4`, build times are actually slightly slower than if I don't set anything.

Would love to see this issue fixed. In https://github.com/spack/spack/pull/20765, I'm working on making Spack build all Python packages in parallel by default.",2021-01-15T23:19:31Z
761284488,Thanks Bas.,2021-01-16T01:24:34Z
761297213,"> Deprecating `sctypeDict` would be useful too, but I'd rather we first clean up the documentation and remove usage in SciPy first, give a heads up to others, and do it in 1.22 or 1.23.

@rgommers do you feel it would be worthwhile to add a comment in the release note discouraging the use of `sctypeDict`?",2021-01-16T02:32:51Z
761375303,"Thanks for the evaluation, Sebastian. To me it's fine that both `float16` and `complex32` are on the ""odd one out"" side as you coined, as I think a fully functional dtype implies it can be used to carry out all kinds of operations it could possibly be permitted to. If in most occasions they need to be upcast first, this would be enough to disqualify them. (So I actually like when you said ""at least we have two (such) dtypes"" -- `float16` is no longer alone )

As for `.real`/`.imag` for `complex32`, it's unclear to me what would the problem be. Wouldn't they simply be non-contiguous (stride 4) `float16` arrays that we can already handle? Could you elaborate your concern?

Finally, it seems our discussion is converging. What does it take to proceed from here? Should I bomb the mailing list? Does it require a NEP? I might not be able to contribute to the actual code, but I'd be happy to do some logistics (if any).

Thanks.",2021-01-16T04:26:38Z
761543622,"A release note sounds like a good idea, as context with this PR. I'm not sure many people read it though; adding that note to https://numpy.org/devdocs/reference/arrays.dtypes.html is probably more effective long-term.",2021-01-16T10:54:30Z
761594949,"Is this the beginning of more extensive work? It seems so far you have renamed two files. The issue spoke about 

- There needs to be a clear distinction between testing the ufunc object (test_ufunc) and the ufuncs instances (test_umath). Currently the two types of tests are mixed between the files.
- The testing of the various ufuncs is far from complete.",2021-01-16T16:46:50Z
761614247,I get a segfault from _mac_os_check and I'm not even on ARM. This whole macos/brew/compiler/python3 combination keeps biting me everytime whenever I update something.,2021-01-16T18:45:14Z
761661052,"@ganesh-k13, I created a new pr #18178 that adds fast integer division intrinsics for all SIMD extensions, it should be merged before this pr.",2021-01-16T21:12:01Z
761696583,"The QR decomposition is not unique all the way down to the signs. One can flip signs in `Q` as long as you flip the corresponding signs in `R`. Some implementations enforce positive diagonals in `R`, but this is just a convention. Since we defer to LAPACK for these linear algebra operations, we follow its conventions, which do not enforce such a requirement.",2021-01-16T23:22:12Z
761709149,"So this is extreme bikeshedding now that the float issue has been discussed, but would you consider changing `floydselect` into `floydrivestselect`? Or perhaps just `floydrivest` or `frselect`?",2021-01-17T01:08:51Z
761737340,"Thanks, @seiko2plus , I'll rebase once that's merged and add the dispatches  ",2021-01-17T05:31:04Z
761746590,"I think this should have an attribution, something like
```
Based on the VCL library, which is (c) Copyright 2012-2020 Agner Fog and licensed under
the Apache License version 2.0.
```

@rgommers is that correct?",2021-01-17T07:17:32Z
761772578,"> I think this should have an attribution, something like
> 
> ```
> Based on the VCL library, which is (c) Copyright 2012-2020 Agner Fog and licensed under
> the Apache License version 2.0.
> ```
> 
> @rgommers is that correct?

No that sounds wrong. Reminder, we don't do Apache2, see for example: https://github.com/numpy/numpy/issues/13447#issuecomment-488547793.

We could change that at some point, but it's a significant change and needs a strong motivation and decision on the mailing list.

VCL is a little hard to find, so here is a link: https://github.com/vectorclass/version2. If this PR took code from there, that seems like a blocker for accepting it.",2021-01-17T11:07:07Z
761772753,"Now read the PR description, the whole PR is based on that, and it seems valuable. It may be worth considering, especially if there are no good alternatives. We'd be deciding to give up on GPLv2 compatibility.",2021-01-17T11:08:51Z
761779667,"@mattip Hmmm... Its a little vague, not sure what im supposed to do",2021-01-17T11:24:50Z
761802147,"According to the above mention, the NumPy 1.19.5 release uses a workaround for the Windows 2004 bug, instead of waiting for Microsoft to fix it.

> NumPy 1.19.5 is a short bugfix release. Apart from fixing several bugs, the main improvement is the update to OpenBLAS 0.3.13 that works around the windows 2004 bug while not breaking execution on other platforms. This release supports Python 3.6-3.9 and is planned to be the last release in the 1.19.x cycle.",2021-01-17T12:12:13Z
761812595,"@rgommers, we can ask for re-licensing, since we're not taking the same exact code plus the original code itself is based on **T. Granlund** and **P. L. Montgomery** work.",2021-01-17T13:26:33Z
761814845,"@charris, Why do we still support Darwin/PowerPC? this flag `faltivec` should be removed.",2021-01-17T13:44:07Z
761819448,"@mhvk,
> Much of the work also ends up being done in python anyway

That's the whole idea behind this solution. 

> That said, @eric-wieser is also right to warn about problems for newcomers

I see it as more friendly for newcomers if you compare it with web template engines, It still python after all.
",2021-01-17T14:15:56Z
761833505,"> I see it as more friendly for newcomers if you compare it with web template engines, It still python after all.

If one were to chose a single tool, indeed, but there are already a couple... Though perhaps this can already replace something? In particular, `tempita` seems to be used for exactly two files: `numpy/random/_bounded_integers.p{yx,xd}.in`, so perhaps one can just rewrite those two and remove `tempita`? (though looking at #8096, it seems to be there since cython uses it?! And from the discussion at http://numpy-discussion.10968.n7.nabble.com/Vendorize-tempita-td43505.html it may be that scipy uses `npy_tempita`...) 

Also, the main argument for an existing template engine like `jinja2` is that at least some people will have seen it before (and one can easily find documentation/examples/etc), so less to learn.",2021-01-17T15:51:09Z
761836146,"I don't know about you - but I find I forget how to use Jinja2 every time, and have to remind myself. ",2021-01-17T16:08:15Z
761836513,"> I don't know about you - but I find I forget how to use Jinja2 every time, and have to remind myself.

I've had that experience a couple times, but at least it's very easy to remind yourself from the documentation and stackoverflow questions.",2021-01-17T16:10:45Z
761845477,"I ran into this with CI breaking on Jetson NX & TX2 and took me a good while to isolate as well.

Serves me right for using `>=` instead of `==` in versioning. As noted, sticking with `1.19.4` works for now.",2021-01-17T17:10:03Z
761862762,CC: @mattip @melissawm ,2021-01-17T19:03:42Z
761864783,"CentOS7 has glibc 2.17, Ubuntu 18.04 has 2.27. Is one of our builds on Ubuntu 18.04 so we can test this?",2021-01-17T19:18:13Z
761866660,I am on Ubuntu 18.04.,2021-01-17T19:31:51Z
761871065,"I thought one or more of our CI builds use ubuntu 18.04, so how do tests pass?",2021-01-17T20:02:48Z
761873215,"> I thought one or more of our CI builds use ubuntu 18.04, so how do tests pass?

Can you point to a CI build using ubuntu 18.04 that runs f2py tests?",2021-01-17T20:19:22Z
761878808,"@BvB93 Nice! Looks like having `number` invariant is not too bad after all, if that was the only substantial change. I thought e.g. arithmetic operators might need to get messier, but this version is just as clean (~mixed-precision ops returned Unions before anyway so it's just a different Union~ EDIT: whoops I forgot that with covariance mypy simplifies the Union to just one dtype - sorry). 
If this gets merged, then having `ndarray` covariant in dtype makes more sense. (I guess users could still do incorrect assignments on arrays of _abstract_ dtype, but that's fine and will probably be rare). ",2021-01-17T20:57:24Z
761880914,I'll review this PR. Please don't merge yet.,2021-01-17T21:12:41Z
761885851,"Just wanted to add to this old, prescient issue by @charris that a `ShapeError` and `BroadcastError` would be really useful!",2021-01-17T21:47:54Z
761888582,"I confirm, this PR does not fix the underlying problem: when using an assumed shape array argument, the code must be f90, and wrapping the subroutine is required. So, f2py constructs a wrapper subroutine containing f90 code but the constructed code is compiled in f77 mode (notice the `.f` extension) which leads to the compilation failure.

This PR works around the problem by skipping `__user__` module (which might be correct) but the underlying problem still persists. For instance, when the user subroutine would be using some other f90 module, the same compilation failure would be triggered because of the wrong compile mode.",2021-01-17T22:06:39Z
761889974,I think I got away with adding `AxisError` a few years ago - I think a sufficiently motivated contributor could do the same for `BroadcastError`.,2021-01-17T22:17:17Z
761892781,"> My interpretation of this is assert_almost_equal(bools...) should be an alias for assert_equal(bool...), for the purpose of being able to write parametric tests and do something sensible - this is my opinion on what should be done.

My understanding of this recommendation is that we check the types of `actual` and `desired` inside of `assert_almost_equal.` If they booleans then return `assert_equals` instead, as we do for `ndarray`:

```python
   if isinstance(actual, (ndarray, tuple, list)) \
            or isinstance(desired, (ndarray, tuple, list)):
        return assert_array_almost_equal(actual, desired, decimal, err_msg)
```

Is that right?",2021-01-17T22:35:58Z
762016884,"Most of the code was rewritten, while some of the code is very similar to Apache 2.0 based VSL, which is not compatible with current 3-clause BSD License(Apache 2.0 is more restrictive), so we should either rewrite the similar code or ask Agner Fog modify the License.",2021-01-18T06:32:15Z
762056925,Thanks @devnexen ,2021-01-18T07:52:59Z
762058932,"I thought the f2py tests are part of the test suite. We should be running them on at least one of the builds. Since the tests pass without this PR, I guess they are not running. We go through the trouble of installing a gfortran compiler and libraries, so we should run the tests. Maybe as part of this PR you could add whatever is needed to make them run? ",2021-01-18T07:56:56Z
762059827,Now I am paranoid that the tests are not running. Could you point to where this new test is run at least once in CI?,2021-01-18T07:58:34Z
762069526,"See for instance
https://github.com/numpy/numpy/pull/18181/checks?check_run_id=1718020162#step:4:5296
",2021-01-18T08:15:41Z
762077075,Thanks @pearu ,2021-01-18T08:28:26Z
762084322,"The f2py tests are running. There are other differences between local and CI ubuntu boxes that may affect the build/test results:
- local: 18.04.4, CI: 18.04.5
- I using conda environment to build and test numpy.f2py, CI is installing numpy to system
- gcc version - local: 9.3.0, CI: 7.5.0",2021-01-18T08:40:10Z
762105394,"@mattip I found the explanation of why the callback tests pass in CI but not locally: when using ubuntu gcc 7.5.0 compiler (CI), `__STDC_NO_THREADS__` *is defined* but when using conda gcc 9.3.0 compiler (local), `__STDC_NO_THREADS__` *is not defined*. Hence, gcc 7.5.0 does not try to include `threads.h` while gcc 9.3.0 does.
",2021-01-18T09:13:57Z
762117306,"Is the glibc version the standard way to detect `threads.h` ?

[This discussion](https://community.intel.com/t5/Intel-C-Compiler/STDC-NO-THREADS-undefined/td-p/1061575) suggests to explicitly `#include <features.h>` to get the `__STDC_NO_THREADS__` definition. ",2021-01-18T09:33:25Z
762144277,"> Is the glibc version the standard way to detect `threads.h` ?

It is known that `threads.h` was introduced in glibc 2.28.

> [This discussion](https://community.intel.com/t5/Intel-C-Compiler/STDC-NO-THREADS-undefined/td-p/1061575) suggests to explicitly `#include <features.h>` to get the `__STDC_NO_THREADS__` definition.

`__STDC_NO_THREADS__` is specific to C11 only. The conda gcc 9.3 uses `-std=c++17` by default and hence `__STDC_NO_THREADS__` is not defined. The ubuntu gcc 7.5 uses `-std=c++11` by default.
So, we should not include `features.h` just to make C11 specific `__STDC_NO_THREADS__` available when for non-C11 cases it should not be defined anyway.
Notice that the thread is about ICC compiler and from pre glibc 2.28 era and hence not directly applicable here, IMHO.

Another approach would be to use `__STDC_VERSION__ == 201112L` (notice the change from `>=` to `==`) but that may be too restrictive and does not protect us for glibc < 2.28 cases.",2021-01-18T10:16:49Z
762163997,"On the second thought, since both `__STDC_NO_THREADS__` and `threads.h` are C11-only specific, we should use `__STDC_VERSION__ == 201112L` anyway when trying to include `threads.h`.

The `glibc >= 2.28` constraint might be redundant though: when using `gcc -std=c11 ...`, `__STDC_VERSION__` is 201710 when using gcc 9 and 201112 when using gcc7.

What do you think, @mattip ?",2021-01-18T10:49:18Z
762166579,"> we should use `__STDC_VERSION__ == 201112L` anyway when trying to include `threads.h`.

This sounds like the most reasonable choice, without the glibc version check",2021-01-18T10:53:20Z
762184132,"@mattip can you tell me what this means and what i'm supposed to do?:

> There needs to be a clear distinction between testing the ufunc object (test_ufunc) and the ufuncs instances (test_umath). Currently the two types of tests are mixed between the files.

> The testing of the various ufuncs is far from complete.",2021-01-18T11:21:43Z
762222800,@mattip please review.,2021-01-18T12:34:46Z
762223632,"you should compare new implements in `tril_indices` and `triu_indices` with `nonzero`,  Can you write a benchmark in `bench_core.py`?",2021-01-18T12:36:09Z
762229792,"Any thoughts on this @seberg , @rossbar ?",2021-01-18T12:48:10Z
762235284,"> Funny thing is that it fails in a virtualenv but seems to be working if you install it at the system level. The NVIDIA NGC containers from https://github.com/dusty-nv/jetson-containers install `numpy` directly into the system libraries and they don't have this issue.

This has nothing to do with virtualenv nor python. Those images are based on `nvcr.io/nvidia/l4t-base:r32.4.4`, which includes `numpy 1.13.3`, just like the numpy in the system's python for Jetson. Because of this, installing numpy does nothing. If you instead ran `pip install --upgrade numpy` or even `pip install numpy==1.19.5`, then you'll get the same problem.

Confirmed both workarounds
* https://github.com/numpy/numpy/issues/18131#issuecomment-755438271
* pinning to `1.19.4`

avoid the issue, from system, docker, and virtualenv.

",2021-01-18T12:59:30Z
762275267,Don't merge until https://github.com/numpy/numpy/pull/18180#discussion_r559578567 is resolved.,2021-01-18T14:11:59Z
762277900,It's not clear from the pr title - but am I right in thinking this is actually just working around a bug in the Intel compiler where the appropriate macro doesn't end up defined? Or are we seeing this problem with gcc alone?,2021-01-18T14:16:30Z
762285857,"> It's not clear from the pr title - but am I right in thinking this is actually just working around a bug in the Intel compiler where the appropriate macro doesn't end up defined? Or are we seeing this problem with gcc alone?

The problem is with gcc (I don't have icc installed to test this atm).

IIUC, gcc 9 is C17 (`__STDC_VERSION__ == 201710`) and it does not define `__STDC_NO_THREADS__`.",2021-01-18T14:30:16Z
762287890,"My reading of the Intel thread is that glibc itself is responsible for providing `__STDC_NO_THREADS__`, and that it uses a special gcc hook to inject a header into all source files that does so. The Intel issue is that the hook doesn't exist in icc - but if you're using gcc, that suggests that something else is going wrong with the hook.",2021-01-18T14:33:49Z
762340980,As a side note: this PR has some minor (name-related) merge conflicts with https://github.com/numpy/numpy/pull/18128.,2021-01-18T16:05:29Z
762352718,"Hey @seberg , I tried multiple things to reproduce the issue locally, tried `quay.io/pypa/manylinux2010_i686` and used a native 32-bit system as well, but it's not crashing. Any idea why that particular TC is failing? I followed the memory through the code, no double free or invalid dereference either.",2021-01-18T16:26:40Z
762354811,"Also not sure if I am setting `quay.io/pypa/manylinux2010_i686` correctly, for instance, I don't see python being installed in the image, yet it's there. I was curious about how are the packages being installed in the image.",2021-01-18T16:30:46Z
762358131,"@Qiyu8 Added tests to bench_core.py. I don't know why it crashes though, any ideas?

This one seems related: #18183",2021-01-18T16:36:39Z
762443236,"Maybe that overflow `assert` is simply incorrect? It does seem a bit off.  Otherwise, my best idea is to install `gdb` in the CI and the job with that. Hopefully, getting a traceback will be sufficient. I did something for example [here (the command may be useful)](https://github.com/numpy/numpy/pull/17750/files) (but for the travis CI).",2021-01-18T19:52:15Z
762477911,"> Seems harmless, although the PR title should explain what the change actually is

hello, actually the  PR title explains what the change actually is I didn't make any big changes
all I did is a very small change is __inti_.py I think if you will see the code you will get it. :)",2021-01-18T21:27:02Z
762478548,"actually, this was my first PR so sorry if I don't make sense",2021-01-18T21:28:58Z
762512665,Looks good to me. Maybe `cov` instead of `co`? The latter has no obvious meaning.,2021-01-18T23:29:40Z
762525607,"LGTM, just a couple of style suggestions.",2021-01-19T00:19:26Z
762529951,@VisionaryMind The NumPy 1.16.x series was the last to support Python 2.7 and most of the shims have been stripped out. Is there a problem using 1.16?,2021-01-19T00:35:43Z
762533195,"> @VisionaryMind The NumPy 1.16.x series was the last to support Python 2.7 and most of the shims have been stripped out. Is there a problem using 1.16?

Using 1.16 is what caused this issue. We then downgraded to 1.13.2, but the issue remains.",2021-01-19T00:47:08Z
762545123,"The relevant code is
```
#if PY_VERSION_HEX >= 0x03000000
#define NUMPY_IMPORT_ARRAY_RETVAL NULL
#else
#define NUMPY_IMPORT_ARRAY_RETVAL
#endif

#define import_array() {if (_import_array() < 0) {PyErr_Print(); PyErr_SetString(PyExc_ImportError, ""numpy.core.multiarray failed to import""); return NUMPY_IMPORT_ARRAY_RETVAL; } }

#define import_array1(ret) {if (_import_array() < 0) {PyErr_Print(); PyErr_SetString(PyExc_ImportError, ""numpy.core.multiarray failed to import""); return ret; } }

#define import_array2(msg, ret) {if (_import_array() < 0) {PyErr_Print(); PyErr_SetString(PyExc_ImportError, msg); return ret; } }
```
Looks to me like `NUMPY_IMPORT_ARRAY_RETVAL` is not being correctly defined for Python 2.7 for some reason.",2021-01-19T01:31:46Z
762547338,"Yes, this was my original thought, but putting a NULL after RETVAL for Python 2 does not resolve the issue. ",2021-01-19T01:40:56Z
762552435,"It looks like it _is_ returning NULL, and it shouldn't be. It should just be `return;`, i.e., no return value.",2021-01-19T02:00:34Z
762556129,"I don't think #18183 is related, can you add your crash logs here? From the CI perspective, The only failure is `TestMatmul.test_vector_matrix_values`.",2021-01-19T02:14:26Z
762563429,"Oh sorry I didn't mean that the pull request was related to the CI issue. But that the solution in #18183 might make nonzero on par with my solution.


I don't get how TestMatmul.test_vector_matrix_values can crash when I add some performance tests. I'll restart and see if it's just some CI hickup.",2021-01-19T02:39:26Z
762751561,"@Qiyu8 , I have replaced the MIN/MAX Macros, placed the NPY_SIMD checking guard at the proper place, merged the count_nonzero_int16/32/64 functions into a single function and added benchmarks for the 4 int types.",2021-01-19T10:28:59Z
762753689,"Consider the following test program glibc-test.c
<details>

```c
#include <stdio.h>
// #include <features.h>

int main() {
  printf(""__STDC_VERSION__ = %ld\n"", __STDC_VERSION__);
#if defined(__STDC_NO_THREADS__)
  printf(""__STDC_NO_THREADS__ = %d\n"", __STDC_NO_THREADS__);
#else
  printf(""__STDC_NO_THREADS__ not defined\n"");
#endif
#if defined(__GLIBC__)
  printf(""GLIBC version = %d.%d\n"", __GLIBC__, __GLIBC_MINOR__);
#else
  printf(""__GLIBC__ not defined\n"");
#endif
#if defined(__GNUC__)
  printf(""GNUC version %d.%d\n"", __GNUC__, __GNUC_MINOR__);
#else
  printf(""__GNUC__ not defined\n"");
#endif
  return 0;
}
```

</details>

that has the following outputs (including `features.h` does not change the output here) in ubuntu 18.04.4 box:
```bash
$ which gcc
/usr/bin/gcc
$ gcc --version
gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
$ gcc glibc-test.c
$ ./a.out 
__STDC_VERSION__ = 201112
__STDC_NO_THREADS__ = 1
GLIBC version = 2.27
GNUC version 7.5

$ conda activate numpy-dev
$ which gcc
/home/pearu/miniconda3/envs/numpy-dev/bin/gcc
$ gcc --version
gcc (crosstool-NG 1.24.0.133_b0863d8_dirty) 9.3.0
$ gcc glibc-test.c
$ ./a.out 
__STDC_VERSION__ = 201710
__STDC_NO_THREADS__ not defined
GLIBC version = 2.12
GNUC version 9.3

$ gcc glibc-test.c -std=c11
$ ./a.out 
__STDC_VERSION__ = 201112
__STDC_NO_THREADS__ not defined
GLIBC version = 2.12
GNUC version 9.3
```

Notice that the conda gcc 9.3 does not define `__STDC_NO_THREADS__` neither in C17 nor in C11 mode.

A similar issue has been reported in https://stackoverflow.com/questions/61887795/gcc-is-non-conforming for GCC 9.2.0 (Windows 10 x64).

Then according to https://gcc.gnu.org/legacy-ml/gcc-help/2019-06/msg00063.html, `__STDC_NO_THREADS__` is defined in `stdc-predef.h` that gcc should implicitly pre-include.

Digging further... it turns out that `stdc-predef.h` is included in conda-forge package `sysroot_linux-64` version 2.17 but not in `sysroot_linux-64` version 2.12 (that is installed to my numpy-dev environment).

So, the problem boils down to using glibc 2.12 (or older) when `__STDC_NO_THREADS__` is not defined, and it seems to me that checking for GLIBC version is inevitable.

My suggestion is to use:
```c
#elif defined(__STDC_VERSION__) \\
      && (__STDC_VERSION__ >= 201112L) \\
      && !defined(__STDC_NO_THREADS__) \\
      && (!defined(__GLIBC__) || __GLIBC_MINOR__>12)
// see https://github.com/numpy/numpy/pull/18180 discussion for details
#include <threads.h>
#define F2PY_THREAD_LOCAL_DECL thread_local
```

@mattip @eric-wieser what do you think?",2021-01-19T10:33:12Z
762759302,"Are you certain that 2.12 is the last version before `__STDC_NO_THREADS__` was added, or is that simply the last version you tested without it?

Either way, that looks reasonable - though you should be checking `__GLIBC__ > 2 || (__GLIBC__ == 2 && __GLIBC_MINOR__ > 12)` for forwards-compatibility.

I suppose the other option is to just discover if `threads.h` is available as part of `config_test`.",2021-01-19T10:43:41Z
762760800,"I assume the problem goes away if you use `np.append(np.array([], dtype=int), 0)`, and so this is really just a consequence of `[]` being largely treated as a shorthand for `np.asarray([])`.",2021-01-19T10:46:31Z
762766205,"> Are you certain that 2.12 is the last version before `__STDC_NO_THREADS__` was added, or is that simply the last version you tested without it?

2.12 is the version I have tested. However, according to https://lists.gnu.org/archive/html/commit-hurd/2012-07/msg00180.html, `__STDC_NO_THREADS__` was defined in a maintenance release of glibc 2.12 (see commit 6d74dd09d29f77ac2b22410f45687def349ba3da)


> 
> Either way, that looks reasonable - though you should be checking `__GLIBC__ > 2 || (__GLIBC__ == 2 && __GLIBC_MINOR__ > 12)` for forwards-compatibility.

Makes sense.

> I suppose the other option is to just discover if `threads.h` is available as part of `config_test`.

For f2py, I am uncertain that this would be worth it. In fact, the sources of f2py generated extension modules must be self-contained (modulo numpy and Python) so that the result of discovering `threads.h` may be invalid when compiling the (possibly relocated) extension modules.",2021-01-19T10:56:36Z
762768273,"What's the status of this PR? Did all the components end up as separate PRs? If so, can we link them here then close this?",2021-01-19T11:00:45Z
762774566,"Huh, that commit is authored by someone (`@jsm28`) I've seen doing something completely different in another repository - open source is a small world! Referencing the mailing list post in a comment might be a good idea",2021-01-19T11:12:23Z
762795779,"> sorry for forgetting about this for so long!

That's completely fine :)",2021-01-19T11:56:48Z
762828818,"> Looks good to me. Maybe `cov` instead of `co`? The latter has no obvious meaning.

It is used consistently as suffix in both [typeshed](https://github.com/python/typeshed/blob/master/stdlib/3/builtins.pyi#L71) and [typing](https://docs.python.org/3/library/typing.html#typing.Sequence) when referring to covariant typevariables, 
so I'd argue there is very much a precedent for using `co`.",2021-01-19T13:08:05Z
762837673,@eric-wieser Do you want me to squash the commits?,2021-01-19T13:25:58Z
762842102,"> If setting `__all__` is not possible, a workaround might be to ""explicitly reexport"" the objects using something this:
>
> ``` python
> from ._array_like import ArrayLike as ArrayLike
> from ._dtype_like import DTypeLike as DTypeLike
> ```

Nice and simple, I feel that this is approach would neatly fix the entire issue.
Would you be willing to submit a PR?",2021-01-19T13:34:19Z
762855182,"Sure no problem with that. Which style is preferable?

``` diff
- from ._array_like import _SupportsArray, ArrayLike
+ from ._array_like import _SupportsArray, ArrayLike as ArrayLike
 from ._shape import _Shape, _ShapeLike
- from ._dtype_like import _SupportsDType, _VoidDTypeLike, DTypeLike
+ from ._dtype_like import _SupportsDType, _VoidDTypeLike, DTypeLike as DTypeLike
```

vs

``` diff
- from ._array_like import _SupportsArray, ArrayLike
+ from ._array_like import _SupportsArray
+ from ._array_like import ArrayLike as ArrayLike
 from ._shape import _Shape, _ShapeLike
- from ._dtype_like import _SupportsDType, _VoidDTypeLike, DTypeLike
+ from ._dtype_like import _SupportsDType, _VoidDTypeLike
+ from ._dtype_like import DTypeLike as DTypeLike

```",2021-01-19T13:57:39Z
762859701,"Personally I prefer the first one.
Feel free to use a (parenthesized) multi-line import if the statement becomes too long.",2021-01-19T14:04:52Z
762864526,"Thanks @seberg , I'll try to refactor that bit. I had asked the python guys here: https://bugs.python.org/issue42665 , seems like we can use PyLong_AsLong directly and try. The gdb in CI is very useful, thanks.",2021-01-19T14:12:39Z
762877840,"Indeed, I would classify this as correct, it doesn't seem feasible to special case `[]`.  If the list may be empty, you will just have to go the long route unfortunately.

Happy to reopen, especially if anyone has an idea how to handle it better.",2021-01-19T14:32:09Z
762934071,Thanks Matti.,2021-01-19T15:52:40Z
762935043,Thanks Bas.,2021-01-19T15:54:01Z
762935578,Needs rebase.,2021-01-19T15:54:45Z
762939849,Thanks @pmav99 .,2021-01-19T16:00:44Z
762978498,"After more research, the [bug report](https://gcc.gnu.org/bugzilla/show_bug.cgi?id=60035) is still marked as `UNCONFIRMED`.",2021-01-19T16:58:33Z
763020388,"Benchmarks after the changes:
```
       before           after         ratio
     [f5f845b7]       [fe3a8975]
     <master>         <enh_14415_fast_compare>
+         5834ns          6224ns     1.07  bench_scalar.ScalarMath.time_addition('longfloat')
-     3.630.03s      3.450.03s     0.95  bench_scalar.ScalarMath.time_addition_pyint('float32')
-       5840.8ns          5542ns     0.95  bench_scalar.ScalarMath.time_multiplication('complex256')
-         5637ns          5312ns     0.94  bench_scalar.ScalarMath.time_addition('float32')
-         5672ns          5273ns     0.93  bench_scalar.ScalarMath.time_multiplication('int64')
-      23.70.2s      11.50.07s     0.49  bench_scalar.ScalarMath.time_compare('int32')
-      24.10.7s       11.70.2s     0.48  bench_scalar.ScalarMath.time_compare('int16')
-      25.90.3s      12.10.05s     0.47  bench_scalar.ScalarMath.time_compare('complex64')
-      26.20.2s      12.00.07s     0.46  bench_scalar.ScalarMath.time_compare('float32')
-      26.30.5s      11.90.03s     0.45  bench_scalar.ScalarMath.time_compare('float16')
```",2021-01-19T18:03:56Z
763047411,Thanks Bas.,2021-01-19T18:47:45Z
763050400,Thanks @pearu ,2021-01-19T18:53:05Z
763050573,Thanks @jeertmans .,2021-01-19T18:53:23Z
763051878,Needs rebase,2021-01-19T18:55:39Z
763141894,"My first reaction to this enhancement was that this is a bad idea, mostly because of YAGNI and being Pandora's box of bugs.

**OT Warning START**:~~my interpretation of this enhancement includes a possible longer-term wish to maintain numpy, numpy_distutils, f2py, and numpy_testing as separate projects.~~

~~In general, whenever a package is split into many interoperable packages, one increases a maintenance burden. For instance, as noted above, scipy has to support several versions of numpy. Similarly, when numpy is split into numpy_distutils (btw, in past we had scipy_distutils), f2py, and perhaps numpy_testing, then in long term one needs to maintain the support of:~~
- ~~numpy for multiple versions of numpy_distutils and numpy_testing~~
- ~~f2py for multiple versions of numpy_distutils, numpy, numpy_testing~~
- ~~numpy_distutils for multiple versions of f2py~~

~~I think the most difficult combinations would be~~
- ~~f2py dependence on numpy and numpy_distutils, and~~
- ~~numpy dependence on numpy_distutils (the source code generation part)~~

~~On top of the above, add scipy that will need to maintain support for multiple versions of numpy, f2py, numpy_distutils, numpy_testing.~~

~~On top of the above 2, add burden from newly introduced bugs when implementing the package split.~~

~~If the aim is to distribute/package/import/.. numpy without numpy_distutils, f2py, and numpy_testing in resource tight environments, then there is an alternative approach: split numpy into numpy_core and numpy^2 where~~
- ~~numpy_core would contain everything in current numpy except numpy_distutils, f2py, numpy_testing~~
- ~~and numpy^2 would be the current numpy except importing numpy_core so that the numpy backward compatibility is preserved.~~

~~If something needs to be split, then it would be scipy, imho.~~

**OT Warning END**


Since the aim of this issue is to make scipy cross-compilation easier, I am not sure it is worth effort considering the invasive nature of the PR and a high chance of breaking a working system in a nontrivial way with no advantage to the end-users of numpy and scipy.

If it will be agreed to purchase resolving this issue as described, I would recommend implementing it slowly:
- make a long term goal of the split, perhaps for Numpy 2.x?
- introduce new packages and gradually start moving code from numpy to new packages while maintaining backward compatibility
- give time for downstream software (scipy may not be the only affected project) to adjust to the new package structure (numpy will need to trigger appropriate deprecation warnings, etc).

My 2 cents.",2021-01-19T21:07:29Z
763155300,Thanks Pearu.,2021-01-19T21:33:14Z
763172041,"Unfortunately I cannot reproduce this (the underlying problem with OpenBLAS' cpu detection code) on my hardware, so I cannot confirm that the trivial attempt at fixing it with a `volatile` keyword in the current `develop` branch actually works. Are all failure reports from Nvidia Jetson devices ?",2021-01-19T22:06:10Z
763187846,"My initial report was using a Jetson device, I can try it on a different ARM CPU tomorrow",2021-01-19T22:36:48Z
763246983,raspberry pi 4 (armv7l) has no problems.,2021-01-20T00:48:17Z
763254096,@eric-wieser The final part is about to come.,2021-01-20T01:09:19Z
763283289,Can you provide the benchmark result by running `python runtests.py --bench-compare master bench_core.Core`?,2021-01-20T02:24:35Z
763389429,"OpenBLAS does not provide DYNAMIC_ARCH for ARMV7 (no practical difference between the provided cpu targets), but I could not reproduce the problem on a Pi4 in 64bit ARMV8 mode.",2021-01-20T07:10:07Z
763403454,"I can't reproduce the issue on AWS Graviton2 either (ARMv8) - but it does definitely occur in Jetson Nano, Jetson TX2 and Jetson Xavier NX. ",2021-01-20T07:40:27Z
763403571,Superseeded by #18184 which includes the present fix regarding `__user__`.,2021-01-20T07:40:42Z
763429448,"> Well, scipy's setup.py uses numpy's distutils fork for the fortran support (I think). Because numpy.distutils is a subpackage of numpy, if you're working with a cross-compiled version of numpy, it eventually tries to import some .so that wasn't compiled for your host and everything dies -- thus you have to have a native numpy installed to allow numpy.distutils to import correctly.

To avoid importing .so modules, I wonder if using the `builtins.__NUMPY_SETUP__ = True` hook would work around this problem? Or at least use the same idea? For instance, consider
```
$ python
>>> import numpy
>>> numpy.core._multiarray_umath
<module 'numpy.core._multiarray_umath' from '/home/pearu/git/pearu/numpy/numpy/core/_multiarray_umath.cpython-39-x86_64-linux-gnu.so'>
>>> ^D
$ python
>>> import builtins
>>> builtins.__NUMPY_SETUP__ = True
>>> import numpy
Running from numpy source directory.
>>> numpy.core._multiarray_umath
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: module 'numpy' has no attribute 'core'
```
",2021-01-20T08:30:40Z
763436456,"To continue with the above idea, consider
```
$ python
Python 3.9.1 | packaged by conda-forge | (default, Jan 10 2021, 02:55:42) 
[GCC 9.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import builtins
>>> builtins.__NUMPY_SETUP__ = True
>>> import numpy
Running from numpy source directory.
>>> import numpy.distutils
>>> import numpy.f2py
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/pearu/git/pearu/numpy/numpy/f2py/__init__.py"", line 11, in <module>
    from . import f2py2e
  File ""/home/pearu/git/pearu/numpy/numpy/f2py/f2py2e.py"", line 23, in <module>
    from . import rules
  File ""/home/pearu/git/pearu/numpy/numpy/f2py/rules.py"", line 83, in <module>
    from . import f90mod_rules
  File ""/home/pearu/git/pearu/numpy/numpy/f2py/f90mod_rules.py"", line 63, in <module>
    if ((.not.allocated(d)).and.(s(1).ge.1)) then"""""" % np.intp().itemsize
AttributeError: module 'numpy' has no attribute 'intp'
```
which means that while `numpy.distutils` will be readily available for cross-compilation of scipy but numpy.f2py requires some work to remove its dependence on numpy.core and there must be a way to specify the target-specific parameters such as the `intp.itemsize`. I think this approach will be considerably less invasive than the issue proposal.",2021-01-20T08:43:41Z
763482477,My reproduction was not on a Jetson. I'm actually not sure what it is - it's just a (pretty powerful) server I ssh into.,2021-01-20T09:53:04Z
763484857,"@martin-frbg if you can disassemble `gotoblas_dynamic_init` and look for the `mrs` instruction and the `getauxval` `bl` instruction before it, you can check the ordering of the `tbz` instruction relative to `mrs` (see [my previous comment](#issuecomment-756230802)).",2021-01-20T09:56:55Z
763526408,"Edit: I didn't realize OT = offtopic, so this comment is probably meaningless /End edit

> If the aim is to distribute/package/import/.. numpy without numpy_distutils, f2py, and numpy_testing in resource tight environments, then there is an alternative approach: split numpy into numpy_core and numpy^2 where

The aim here is the opposite: to have numpy_distutils without importing the whole of numpy, to make cross-builds (ie OpenEmbedded is my use case) of scipy and similar not depend on native build of numpy. It seems that numpy_distutils can be a pure Python module, greatly simplifying things. As @virtuald wrote, numpy has binary dependencies which can be problematic in such an environment.

To reiterate: this is not about what is happening on the target device, but cross-building the image on a separate host.",2021-01-20T11:05:10Z
763535153,"Thanks, @jaskij for the explanation. Here's a 4-line implementation of numpy_distutils.py module that is pure Python and should provide the required functionality:
```
# numpy_distutils.py
import builtins as _builtins 
_builtins.__NUMPY_SETUP__ = True
from numpy.distutils import *
_builtins.__NUMPY_SETUP__ = False
```

However, as noted above, this will not be sufficient to build scipy: numpy.f2py has to be adjusted to not use numpy.core tools.",2021-01-20T11:20:16Z
763636807,"I just checked and the sever that @amerry reproduced on has a Cavium ThunderX CN8890 CPU (ARMv8.1), notably not a Jetson.",2021-01-20T14:20:53Z
763744543,@mattip If it fixes either of the two problems I'll definitely backport. I will be happy if we can get over the problems that have ensued since Windows 2004.,2021-01-20T16:14:42Z
763806373,"@pearu to be honest, I didn't understood OT meant off-topic the first time and stopped reading after the paragraph I quoted from, my bad.

OpenEmbedded, the system I'm using (and which is the Linux Foundation-backed industry standard), is so good about separating host and  target and setting up correct variables I managed to get it building, with the help of [gpanders/meta-scipy](https://github.com/gpanders/meta-scipy).

There are still some issues which complicate my solution, but at least for this one environment there's a working workaround.

The way OpenEmbedded works, ultimately, the perfect solution would be the ability to _package_ numpy_distutils and f2py without numpy itself. The system works around packages, whether it's build dependencies for the cross host or runtime dependencies.

Taking a closer look at gpanders' [python3-scipy_1.5.3.bb](https://github.com/gpanders/meta-scipy/blob/master/recipes-devtools/python/python3-scipy_1.5.3.bb), which is the instruction for OpenEmbedded how to build scipy, you can see the dependencies list in [line 12](https://github.com/gpanders/meta-scipy/blob/master/recipes-devtools/python/python3-scipy_1.5.3.bb#L12). Without any prior knowledge, having a package depend on both numpy (which is the target numpy) and numpy-native (which is numpy built for host) is a code smell.",2021-01-20T17:23:06Z
763812152,"Interesting, thanks - ThunderX is what the drone.io CI uses, though the OpenBLAS build has been using gcc 7.5 there. (Checked now that the current `develop` branch passes with gcc 10.2 _and_ the tentative patch for this issue)",2021-01-20T17:32:43Z
763849720,"@Qiyu8 I can't figure out how to do it. Do you want to run it yourself or act IT-support?

Anaconda prompt:
```
(base) C:\Users\J.W\Documents\GitHub\numpy>python runtests.py --bench-compare master bench_core.Core
Traceback (most recent call last):
  File ""C:\Users\J.W\Documents\GitHub\numpy\numpy\__init__.py"", line 126, in <module>
    from numpy.__config__ import show as show_config
ModuleNotFoundError: No module named 'numpy.__config__'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""runtests.py"", line 642, in <module>
    main(argv=sys.argv[1:])
  File ""runtests.py"", line 172, in main
    _temp = __import__(PROJECT_MODULE)
  File ""C:\Users\J.W\Documents\GitHub\numpy\numpy\__init__.py"", line 131, in <module>
    raise ImportError(msg) from e
ImportError: Error importing numpy: you should not try to import numpy from
        its source directory; please exit the numpy source tree, and relaunch
        your python interpreter from there.
```

Ok, go back one level then:
```
(base) C:\Users\J.W\Documents\GitHub\numpy\numpy>cd C:\Users\J.W\Documents\GitHub

(base) C:\Users\J.W\Documents\GitHub>python runtests.py --bench-compare master bench_core.Core
python: can't open file 'runtests.py': [Errno 2] No such file or directory
```",2021-01-20T18:36:36Z
763872509,"@jaskij , no problem, I realize my ""this is bad idea"" comment may have sound harsh but I meant it only in a constructive way :)

Another way to make `numpy.{distutils, f2py}` easier for OpenEmbedded to handle is to have two setup.py scripts (actually, one is sufficient but its functionality could be tweaked by some environment variable):
- `setup.py` that packages/installs numpy as it does now,
- `NUMPY_SETUP_NO_CORE=1 setup.py` that packages/installs numpy as, say, `numpy_nocore` that includes only numpy.distutils and numpy.f2py and would be made available to OpenEmbedded like systems. Notice that `NUMPY_SETUP_NO_CORE=1` would change only the parent package name (`numpy` -> `numpy_nocore`) and it would discard subpackages such as `numpy.core` and others when installing/packaging.

This approach could be implemented with minimal changes to numpy and would be an extension of numpy (introduces a new package) rather than a major rewrite of a numpy packaging structure that #17632 is, IMHO.",2021-01-20T19:16:37Z
763888885,"Closing, this is configurable with `np.errstate` and a deliberate choice.",2021-01-20T19:45:29Z
763909860,@whitty could you confirm that this issue should indeed be closed? You should be able to find wheels in the [weekly builds](https://anaconda.org/scipy-wheels-nightly/numpy/files) in a few days,2021-01-20T20:24:14Z
763922085,Thanks Ralf.,2021-01-20T20:47:35Z
763963914,"OK, just opened a small PR to just revert it. I am very happy to add the deprecation (or even the more complex logic), but it just isn't very important, so doing anything for 1.20 already seems not even worth back porting release note additions :).",2021-01-20T21:35:32Z
763966437,"> NUMPY_SETUP_NO_CORE=1 setup.py that packages/installs numpy as, say, numpy_nocore that includes only numpy.distutils and numpy.f2py and would be made available to OpenEmbedded like systems. Notice that NUMPY_SETUP_NO_CORE=1 would change only the parent package name (numpy -> numpy_nocore) and it would discard subpackages such as numpy.core and others when installing/packaging.

This would work in my environment, but I am not sure about other cross-compilation systems and environments (while industry standard in industrial, OpenEmbedded is by far not the only system). The only issue with this approach would be that then scipy's (and other) setup.py would also need to know to import numpy_nocore, but that probably is easily patched downstream (I'm thinking about current and older versions which probably would not include this upstream).

Another issue is numpy distutils being somewhat greedy with paths, as seen [in this patch](https://github.com/gpanders/meta-scipy/blob/master/recipes-devtools/python-numpy/python3-numpy/0001-Disable-runtime_lib_dirs-for-cross-compile.patch) - the default behavior is probably good for normal users, but an option to disable it _without_ patching would be preferred in cross environments.",2021-01-20T21:40:16Z
763970718,#4952 I think is most of your problem here,2021-01-20T21:48:23Z
763973916,"Having this issue as well, it shows up with 1.20.0rc2 while testing. Ended up just watching some random YouTube video and took a risk. `arch -86_64 zsh` will push you to an x86_64 instance of zsh, then running `pip3 install opencv-python` seemed to install numpy-1.19.5-cp39-cp39-maxosx_10_9_x86_64.whl without issue.

The solution above mentioning some openblas commands failed on the third install. Not sure what's going on but I now have like 4 versions of python installed, 2 versions of python3.9, 1 version of python 3.8, and another install of python 2.7.16. that ships with macOS.",2021-01-20T21:54:07Z
763997448,"I am happy to add a module and/or qualname to ufuncs, we just need to decide on how...

Currently, the name is basically passed to the C creation function. We could do it like Python's static types and just assume everything before the first `.` is part of the module, that is not ideal (for one some ufuncs couldrequire a `__qualname__` technically), but likely ""just works"" (have to check if pickle is fine with getting `__module__ + "".""+ __qualname__`, but I expect it is).

The other option, is to make `__module__` and `__qualname__` writeable attribute (or writeable exactly once, with the option of disallowing it when used with future API).

----

`np.memmap` thing seems like the `@set_module('numpy')` decorator can't or doesn't properly set the module of the attributes.   I guess the decorator could try to be smart about methods or is there a better solution?",2021-01-20T22:35:33Z
764006341,The new code improves the accuracy due to the use of FMA too. we will have to dispatch `AVX2&FMA3` and `AVX512F` in runtime.,2021-01-20T22:54:20Z
764128909,Thanks Sebastian. I assume you will issue a FutureWarning at some point in 1.21 development. ,2021-01-21T00:58:44Z
764136996,Thanks Chunlin.,2021-01-21T01:02:37Z
764156076,"The dispatching solution for non-UFunc is not recommend in the past, If it is acceptable now, then we can have a discussion on the mailing list.",2021-01-21T01:23:00Z
764174382,"Mission Accomplished, closing now.",2021-01-21T01:51:35Z
764203736,you can read the benchmark guide [here](https://github.com/numpy/numpy/tree/master/benchmarks). ,2021-01-21T03:11:11Z
764438030,Thanks a lot!,2021-01-21T07:21:19Z
764640903,The branch has been rebased and the tests seem to be passing.,2021-01-21T13:25:46Z
764802091,"RuntimeError: Broken toolchain: cannot link a simple C program
",2021-01-21T17:13:20Z
764802885,"> since the small integers are cached.

I am not sure if that is a language feature or an implementation detail. While PyPy did implement a work around, other implementations may not appreciate being punished for not implementing this.",2021-01-21T17:14:17Z
764810429,"I'd assume it's an implementation detail, and certainly wouldn't recommend having _behavior_ depend on it. Having performance rely on implementation details doesn't strike me as a particularly big problem though, especially since things like function call overhead are influenced by that anyway.",2021-01-21T17:25:40Z
764837712,Hi! I'm new. Can I work on this issue?,2021-01-21T18:09:21Z
764860633,Why are you installing from source?,2021-01-21T18:49:52Z
764861078,"@Qiyu8 It's still not working for me, got a little further though:
* `python runtests.py --bench` -Works
* `python runtests.py --bench-compare master bench_core` - does not work.

```
(base) C:\Users\J.W\Documents\GitHub\numpy>python runtests.py --bench-compare master bench_core
Traceback (most recent call last):
  File ""C:\Users\J.W\Documents\GitHub\numpy\numpy\__init__.py"", line 126, in <module>
    from numpy.__config__ import show as show_config
ModuleNotFoundError: No module named 'numpy.__config__'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""runtests.py"", line 642, in <module>
    main(argv=sys.argv[1:])
  File ""runtests.py"", line 172, in main
    _temp = __import__(PROJECT_MODULE)
  File ""C:\Users\J.W\Documents\GitHub\numpy\numpy\__init__.py"", line 131, in <module>
    raise ImportError(msg) from e
ImportError: Error importing numpy: you should not try to import numpy from
        its source directory; please exit the numpy source tree, and relaunch
        your python interpreter from there.
```

Also virtualenv doesn't seem to be the recommended way anymore:
> Note
> 
> If you are using Python 3.3 or newer, the venv module is the preferred way to create and manage virtual environments. venv is included in the Python standard library and requires no additional installation. If you are using venv, you may skip this section. 
> 
https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/#installing-virtualenv",2021-01-21T18:50:39Z
764870429,"> Why are you installing from source?

Sorry I might not understand this 100%, but we have a requirements.txt file in my repository with numpy being in it. When i run `python -m pip install -r requirements.txt` numpy fails. Is there a better way to install dependencies?",2021-01-21T19:07:03Z
764891893,"Pip is using a cached zip file, e.g., a file of source code. You can force it to use binary wheels
```
python -mpip install --only-binary numpy
```
But the question is where the zip file came from, and if you are on a supported platform. What hardware?
",2021-01-21T19:43:50Z
764892103,"Unless there is a comment that someone is already busy on it (fairly recently), everything is always up for grabs. We don't really assign issues. So of course, go ahead.",2021-01-21T19:44:10Z
765012914,Thanks Bas.,2021-01-21T23:45:11Z
765060943,"I've added some comments on pytorch/pytorch#50880

I suspect that we'll need input from both communities to properly resolve this, so I apologize to all for the bifurcated discussion.",2021-01-22T01:57:27Z
765133668,"> Pip is using a cached zip file, e.g., a file of source code. You can force it to use binary wheels
> 
> ```
> python -mpip install --only-binary numpy
> ```
> 
> But the question is where the zip file came from, and if you are on a supported platform. What hardware?

It can't find the binary it seems. When i use the command `python -m pip install --only-binary=:all: numpy==1.19.5` i get this error: 
`
ERROR: Could not find a version that satisfies the requirement numpy==1.19.5
ERROR: No matching distribution found for numpy==1.19.5
`

Here are my specs:
Numpy version 1.19.5
Python version 3.9.1
pip version 20.3.3
wheel version 0.36.2
macOS Big Sur 11.1 (Apple Silicon/M1)
16gb ram
",2021-01-22T05:26:40Z
765146469,"Sorry to hijack this thread but on a related topic on nonzero(), is there a reason why calling nonzero on a 1D array is orders of magnitude faster than a multi-dimensional array? 

For example, calling it on a Boolean array of shape (1000000,) is taking ~40 s, while it takes 1400 s for an array of shape (1000,1000). Both arrays are identical in values and only differ in shape.

Any idea what's the significant overhead cost here?",2021-01-22T05:53:04Z
765167975,"Duplicate of #18160, #18143. You should have led with the data point that you are using the new Apple Silicon M1 hardware. We don't currently support that hardware until we can run CI on it. You may have more luck with conda, I think they have binary packages for M1. In the meantime you can use the x86_64 Rosetta emulation.",2021-01-22T06:28:47Z
765170261,"The aarch64 builds are failing the admittedly complicated test with errorcode -9, which indicates a SIGKILL. Due to the nature of the test, this is probably due to trying to allocate too much memory and causing an OOM error. Maybe the `requires_memory(free_bytes=7e9)` check is not enough anymore? 

For completeness, I compared the two log files from  the successful run a week ago to the latest run for cpython3.7.9, here is a comparison. (The complex warnings did not show up so they must be present in both runs).

what | old | new
--|--|--
time | 7 days ago | 9 hours ago
travis host kernel | 5.4.0-54-generic | 5.8.0-38-generic
git hash | b91f3c00e | 5c7002926
OpenBLAS | v0.3.13 | v0.3.13-62-gaf2b0d02
hypothesis | 6.0.1 | 6.0.2

Everything else: compiler, SIMD detection, tests (except the failing one and some additional f2py and typing tests) compared equivalently.

The difference in kernel version is troubling. Both machines report Ubuntu bionic, how could the kernel update like that?

In any case, I think it is fine to skip that flaky test on aarch64.",2021-01-22T06:32:41Z
765172219,"@gnool , without further investigation, I am speculating the overhead is coming from the use of an iterator and calls to ```get_multi_index``` function in https://github.com/numpy/numpy/blob/master/numpy/core/src/multiarray/item_selection.c#L2555-L2600 whenever a multidimensional array is passed. In contrast, when a single dimensional array is passed, without an iterator the indices are computed using the ```npy_memchr``` function which is quite fast : https://github.com/numpy/numpy/blob/master/numpy/core/src/multiarray/item_selection.c#L2485-L2520",2021-01-22T06:37:49Z
765278133,"Perhaps a better solution could be to replace line 157:
```
    epsneg_f128 = exp2(ld(-113))
```
by something like:
```
    if( ld(1)!=ld(1)-exp2(ld(-113)) ):
        epsneg_f128 = exp2(ld(-113))
    elif( ld(1)!=ld(1)-exp2(ld(-64)) ):
        epsneg_f128 = exp2(ld(-64))
    elif( ld(1)!=ld(1)-exp2(ld(-53)) ):
        epsneg_f128 = exp2(ld(-53))
    else:
        raise Exception(""Problem with longdouble"")
```",2021-01-22T09:43:33Z
765299943,"This is not a problem localized to import, right? If someone uses numpy and happens to get an overflow error, NumPy (or any well-behaved library) internally detect that and converts the value to a `+float('inf')` or `-float('inf')` without a signal handler. We could consider adding a signal handler of our own, but that just seems like an arms race: whoever wants to detect this situation will just invent a better way to work around our mask. When would a user of scientific python want to trap overflow signals that may naturally occur in mathematical calculations? ",2021-01-22T10:15:25Z
765313821,"I think that what to do with an overflow is user dependant. Some people may want use it as inf, other should stop the code with an error if overflow appears because it must not appear.
The question could be ""Is it an expected behaviour that an overflow occurs ?"" During an application run, I think it is application (user and/or developer) dependant. During a library importation, I would answer no it isn't.",2021-01-22T10:39:53Z
765336275,"Hey @seberg , can you have a look at this PR. Thanks.",2021-01-22T11:26:08Z
765414346,this link: https://docs.scipy.org/doc/numpy/reference/c-api.array.html#importing-the-api has disappeared since.. it would sure nice to find that excellent explanation again from somewhere..,2021-01-22T13:55:32Z
765415651,"As far as I understand, one can sample using the inverse covariance matrix without having to invert it. A colleague of mine has such an implementation:

```python
# Generate samples from a multi-variate normal distribution with provided precision matrix WITHOUT inverting
def _mv_normal_sample(self, mu=0, precision_matrix=None, num_samples=1):

    # Precision matrix must be a square matrix
    assert precision_matrix.shape[0] == precision_matrix.shape[1], 'Precision matrix must be a square matrix'

    dim = precision_matrix.shape[0]

    chol_U = cholesky(precision_matrix, lower=False)

    # Create num_samples iid standard normal vectors
    z_vector_matrix = np.random.normal(loc=0, scale=1, size=[num_samples, dim])

    # Sample from the MV normal with precision matrix by solving the Cholesky decomp for each normal vector
    samples = np.squeeze(np.array(
        [solve_triangular(a=chol_U, b=z_vector_matrix[i, :], unit_diagonal=False) + mu for i in
         range(num_samples)]))

    return (samples)
```

As @david-cortes points out, there are situations in which it would be useful to do this, and having to invert the inverse covariance matrix would be more computationally intensive that sampling using it directly.

Would a PR be welcome?",2021-01-22T13:57:23Z
765429360,"> As far as I understand, one can sample using the inverse covariance matrix without having to invert it. A colleague of mine has such an implementation:
> 
> ```python
> # Generate samples from a multi-variate normal distribution with provided precision matrix WITHOUT inverting
> def _mv_normal_sample(self, mu=0, precision_matrix=None, num_samples=1):
> 
>     # Precision matrix must be a square matrix
>     assert precision_matrix.shape[0] == precision_matrix.shape[1], 'Precision matrix must be a square matrix'
> 
>     dim = precision_matrix.shape[0]
> 
>     chol_U = cholesky(precision_matrix, lower=False)
> 
>     # Create num_samples iid standard normal vectors
>     z_vector_matrix = np.random.normal(loc=0, scale=1, size=[num_samples, dim])
> 
>     # Sample from the MV normal with precision matrix by solving the Cholesky decomp for each normal vector
>     samples = np.squeeze(np.array(
>         [solve_triangular(a=chol_U, b=z_vector_matrix[i, :], unit_diagonal=False) + mu for i in
>          range(num_samples)]))
> 
>     return (samples)
> ```
> 
> As @david-cortes points out, there are situations in which it would be useful to do this, and having to invert the inverse covariance matrix would be more computationally intensive that sampling using it directly.
> 
> Would a PR be welcome?

Isn't `solve_triangular` a scipy function? I'm not sure numpy uses scipy as a dependency. Also wouldn't it be better to solve multiple right hands sides at once instead of using a list comprehension, and then use `ravel` to reshape to a 1d array?

That said, there is a [paper](https://arxiv.org/pdf/1607.04751v2.pdf) (Algorithm 4) that shows a fast way to sample from a multivariate normal using a structured precision matrix that is a sum of an invertible matrix and a low-rank matrix. In some Bayesian algorithms, the precision has to constructed in this way. So this algorithm might be better in such cases where you have the pieces to make the precision. I have an implementation of this [here](https://github.com/zoj613/htnorm).",2021-01-22T14:16:17Z
765439733,"It is but it is just a call to a LAPACK function DTRTRS so not technically impossible.  I still think it is too specialized for inclusion in NumPy.  Moreover, if a package wants really fast sampling for a particular covariance/precision structure, it is much simpler to write a low-level, high-performance implementation that can be shipped to users of the downstream package.  This said, I do still there there is a case for the `""factor""` method.
",2021-01-22T14:29:52Z
765441291,@zoj613 I see you have done exactly that in your implementation. It is nice to see that this feature is being picked up.,2021-01-22T14:31:56Z
765455705,"I forgot about this NEP. Nice timing to comment on it @stefanv. I'll at least link to it from my upcoming NEP about the array API standard. I'd also be interested to push this forward, just a matter of finding the time .....",2021-01-22T14:50:36Z
765457533,"Let me also link https://github.com/numpy/numpy/commits/master/numpy/tests/test_public_api.py, which has all the relevant public/private/in-between parts listed.",2021-01-22T14:52:53Z
765501136,"Still trying to figure out why the azure tests have been failing.
Hopefully https://github.com/numpy/numpy/pull/18204/commits/ef0a3f068b042f676515382e917576234858fa0e will grant a bit of insight.",2021-01-22T15:51:07Z
765510306,"@rgommers and I have discussed this on the pytorch issue and converged on some recommendations:

- Warn if the object is not an `ndarray` or a `MutableSequence`. Most of the new array types that will be problematic to `shuffle()` won't support `MutableSequence`. And those are more likely than custom objects that would work okay to `shuffle()` but, for whatever reason, don't declare that they are `MutableSequence`s. We can evaluate later if this should turn into a deprecation and eventual `raise`.
- Change the type description from `array_like` to something that is more explicit. `array_like` implies that the object is converted to an `ndarray`, which is not the case for `shuffle()`.",2021-01-22T16:02:51Z
765515918,"Thanks @rkern. 

> @charris charris added this to the 1.20.0 release milestone 14 hours ago 

@charris would indeed be nice to take this along, the change should not require an RC3 I'd think. I'm happy to submit a PR this weekend - is Sunday in time?",2021-01-22T16:10:25Z
765555826,"Arrow tagged 3.0.0 Monday, but a new pyarrow isn't up on PyPI yet, so Sunday will be fine. I'm planning on making the 1.20.0 release the weekend after this.",2021-01-22T17:02:50Z
765574521,Thanks @madphysicist .,2021-01-22T17:36:17Z
765577363,"Maybe we should change the assert to not fail for signal 9 and xfail instead. Note that the test didn't raise, so didn't run to completion.",2021-01-22T17:41:39Z
765607733,">  I still think it is too specialized for inclusion in NumPy. 

Sure, no worries

> I have an implementation of this here.

Cool, thanks",2021-01-22T18:34:48Z
765690671,Is this page what you are looking for? https://numpy.org/doc/stable/reference/c-api/array.html#importing-the-api ,2021-01-22T21:21:29Z
766038838,Thanks. Fix in gh-18211.,2021-01-23T13:26:41Z
766088657,"Thank you. I'm working on it! If you give me time I can solve it.

I think the problem is with `npy_divmod` function because `remainder` function use `divmod`. I have see the `core/include/numpy/scalarmath.c` and other files, However  the `warning` is not raise there. I'm trying to locate where is raised.

The warning is raised by the `_error_handler` inside `extobj.c` this function is inside `Handleit` that also is inside `_check_ufunc_fperr`. Which is used by `PyUFunc_GenericFunction_int`, whic is used by `ufunc_generic_call` that it use by `PyTypeObject`. So the work flow is the following:


`_error_handler` -> `HANDLEIT` -> `PyUFunc_handlefperr` -> `_check_ufunc_fperr` -> `PyUFunc_GenericFunction_int` -> `ufunc_generic_call` -> `PyTypeObject`

 Now I'm trying to find the connection between `np.floatxx`, `np.nan` and the `divmod` operator.

Any guidance is accepted! meanwhile I will continue working on it. This task is helping me to understand the library internally!",2021-01-23T14:34:10Z
766090646,"Hello,

I am running into an issue downstream of this typeerror. I am using f2py and f90wrap to python wrap a fortran code, but I am no longer able to run f2py successfully. I encounter the following error, which comes from numpy:

```
    Traceback (most recent call last):
      File ""<string>"", line 1, in <module>
      File ""/private/var/folders/82/x2lgxrz96clgm5p30pp_jxl40000gn/T/pip-req-build-h0n_j19k/setup.py"", line 55, in <module>
        setup(name='f90wrap',
      File ""/usr/local/lib/python3.9/site-packages/numpy/distutils/core.py"", line 169, in setup
        return old_setup(**new_attr)
      File ""/usr/local/lib/python3.9/site-packages/setuptools/__init__.py"", line 153, in setup
        return distutils.core.setup(**attrs)
      File ""/usr/local/Cellar/python@3.9/3.9.1_6/Frameworks/Python.framework/Versions/3.9/lib/python3.9/distutils/core.py"", line 148, in setup
        dist.run_commands()
      File ""/usr/local/Cellar/python@3.9/3.9.1_6/Frameworks/Python.framework/Versions/3.9/lib/python3.9/distutils/dist.py"", line 966, in run_commands
        self.run_command(cmd)
      File ""/usr/local/Cellar/python@3.9/3.9.1_6/Frameworks/Python.framework/Versions/3.9/lib/python3.9/distutils/dist.py"", line 985, in run_command
        cmd_obj.run()
      File ""/usr/local/lib/python3.9/site-packages/numpy/distutils/command/install.py"", line 60, in run
        r = self.setuptools_run()
      File ""/usr/local/lib/python3.9/site-packages/numpy/distutils/command/install.py"", line 34, in setuptools_run
        return distutils_install.run(self)
      File ""/usr/local/Cellar/python@3.9/3.9.1_6/Frameworks/Python.framework/Versions/3.9/lib/python3.9/distutils/command/install.py"", line 546, in run
        self.run_command('build')
      File ""/usr/local/Cellar/python@3.9/3.9.1_6/Frameworks/Python.framework/Versions/3.9/lib/python3.9/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""/usr/local/Cellar/python@3.9/3.9.1_6/Frameworks/Python.framework/Versions/3.9/lib/python3.9/distutils/dist.py"", line 985, in run_command
        cmd_obj.run()
      File ""/usr/local/lib/python3.9/site-packages/numpy/distutils/command/build.py"", line 61, in run
        old_build.run(self)
      File ""/usr/local/Cellar/python@3.9/3.9.1_6/Frameworks/Python.framework/Versions/3.9/lib/python3.9/distutils/command/build.py"", line 135, in run
        self.run_command(cmd_name)
      File ""/usr/local/Cellar/python@3.9/3.9.1_6/Frameworks/Python.framework/Versions/3.9/lib/python3.9/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""/usr/local/Cellar/python@3.9/3.9.1_6/Frameworks/Python.framework/Versions/3.9/lib/python3.9/distutils/dist.py"", line 985, in run_command
        cmd_obj.run()
      File ""/usr/local/lib/python3.9/site-packages/numpy/distutils/command/build_ext.py"", line 269, in run
        fcompiler.customize(self.distribution)
      File ""/usr/local/lib/python3.9/site-packages/numpy/distutils/fcompiler/__init__.py"", line 505, in customize
        get_flags('opt', oflags)
      File ""/usr/local/lib/python3.9/site-packages/numpy/distutils/fcompiler/__init__.py"", line 496, in get_flags
        flags.extend(getattr(self.flag_vars, tag))
      File ""/usr/local/lib/python3.9/site-packages/numpy/distutils/fcompiler/environment.py"", line 40, in __getattr__
        return self._get_var(name, conf_desc)
      File ""/usr/local/lib/python3.9/site-packages/numpy/distutils/fcompiler/environment.py"", line 56, in _get_var
        var = self._hook_handler(name, hook)
      File ""/usr/local/lib/python3.9/site-packages/numpy/distutils/fcompiler/__init__.py"", line 703, in _environment_hook
        return hook()
      File ""/usr/local/lib/python3.9/site-packages/numpy/distutils/fcompiler/gnu.py"", line 222, in get_flags_opt
        v = self.get_version()
      File ""/usr/local/lib/python3.9/site-packages/numpy/distutils/fcompiler/__init__.py"", line 424, in get_version
        version = CCompiler.get_version(self, force=force, ok_status=ok_status)
      File ""/usr/local/lib/python3.9/site-packages/numpy/distutils/ccompiler.py"", line 90, in <lambda>
        m = lambda self, *args, **kw: func(self, *args, **kw)
      File ""/usr/local/lib/python3.9/site-packages/numpy/distutils/ccompiler.py"", line 649, in CCompiler_get_version
        version = matcher(output)
      File ""/usr/local/lib/python3.9/site-packages/numpy/distutils/fcompiler/gnu.py"", line 271, in version_match
        v = self.gnu_version_match(version_string)
      File ""/usr/local/lib/python3.9/site-packages/numpy/distutils/fcompiler/gnu.py"", line 73, in gnu_version_match
        raise ValueError(err + version_string)
    ValueError: A valid Fortran version was not found in this string:
    gfortran-10: warning: couldn't understand version 11
    10.2.0
```


My gcc / gfortran are up to date with the latest Brew versions and I installed numpy 1.20.0rc2.

Thank you in advance for any advice on how to proceed.",2021-01-23T14:48:28Z
766097827,+1 the whole point of this new M1 mac is to do ML and numpy is pretty integral..,2021-01-23T15:34:16Z
766157468,"@arubiales if the warning is actually manually set, it would be something like `npy_set_floatstatus`. But most likely the opposite is the case: The C code (machine code), sets an error flag (the CPU does this, or well, the basic math library) for a certain operation. And our code would have to avoid taking that code path alltogether.",2021-01-23T18:32:45Z
766165770,"Thanks for the reviews. I updated the text for most of them, and replied to the ones I didn't quite agree with.",2021-01-23T19:24:13Z
766175121,Sorry for the clutter :) Github isn't made for line editing documents.,2021-01-23T20:35:10Z
766248652,"The final fix is nearing release and is now in beta and release channels. Hopefully out next month.


**Windows 10 build 19042.782**

- We fixed an issue that causes the 64-bit fmod() and remainder() functions to damage the Floating Point Unit (FPU) stack.
",2021-01-24T00:32:44Z
766297024,Thanks @Carreau ,2021-01-24T05:52:51Z
766297128,Thanks @Carreau ,2021-01-24T05:54:16Z
766298724,Adopted the xfail idea.,2021-01-24T06:12:15Z
766316579,"I disagree that it is ""too specialized to be included in Numpy"". The precision matrix parametrization of any statistical model is as natural as the covariance matrix parametrization. There is no good reason to consider the covariance matrix as ""natural"" and the precision matrix as ""specialized"". Secondly, a key feature of Numpy is speed and (related to this) scalability. Inverting a matrix is neither fast nor scalable.",2021-01-24T09:19:56Z
766323349,Thanks @yunfeim. It would be helpful if you could add a benchmark for `matrix_power` in `benchmarks/benchmarks/bench_linalg.py` that shows the performance benefit.,2021-01-24T10:16:48Z
766323665,"Merged, thanks @Amarnath1904.

Please note that we prefer not to get pure style change PRs, because they're on average not worth the churn, review effort and CI time. If you want to try another PR, tackling a bug or documentation issue would be great.",2021-01-24T10:19:46Z
766323935,@Radhika14soni are you planning to update this PR with the change requested by @mattip?,2021-01-24T10:22:22Z
766324287,"> I should probably also figure out why they're segfaulting at some point. `/proc/cpuinfo` mentions AVX2 and FMA but not FMA3, and that roughly exhausts my knowledge of how to debug the crashes.

@seiko2plus or @mattip any advice here on how to debug?",2021-01-24T10:25:29Z
766331517,"@Qiyu8 I never got `--bench-compare` to work but here are results using `--bench` instead. About the same ~2x improvement as I saw using `timeit`.

Illviljan-faster_tril_indices:
```
(base) C:\Users\J.W\Documents\GitHub\numpy>python runtests.py --bench bench_core.Core
Building, see build.log...
Build OK
 No executable found for python 3.7
 Discovering benchmarks
 Running 28 total benchmarks (1 commits * 1 environments * 28 benchmarks)
[  0.00%]  Benchmarking existing-pyc__users_j.w_anaconda3_python.exe
[  1.79%]  Running (bench_core.Core.time_arange_100--)............................
[ 51.79%]  bench_core.Core.time_arange_100                       1.060.04s
[ 53.57%]  bench_core.Core.time_array_1                             47940ns
[ 55.36%]  bench_core.Core.time_array_empty                       1.550.2s
[ 57.14%]  bench_core.Core.time_array_float64_l1000                 54.72s
[ 58.93%]  bench_core.Core.time_array_float_l1000                   68.35s
[ 60.71%]  bench_core.Core.time_array_float_l1000_dtype             48.63s
[ 62.50%]  bench_core.Core.time_array_int_l1000                     75.35s
[ 64.29%]  bench_core.Core.time_array_l                          3.610.07s
[ 66.07%]  bench_core.Core.time_array_l1                         1.350.06s
[ 67.86%]  bench_core.Core.time_array_l100                        11.40.3s
[ 69.64%]  bench_core.Core.time_array_l_view                      3.690.1s
[ 71.43%]  bench_core.Core.time_diag_l100                         21.10.8s
[ 73.21%]  bench_core.Core.time_diagflat_l100                     32.40.7s
[ 75.00%]  bench_core.Core.time_diagflat_l50_l50                    34.42s
[ 76.79%]  bench_core.Core.time_dstack_l                          11.00.4s
[ 78.57%]  bench_core.Core.time_empty_100                        1.090.03s
[ 80.36%]  bench_core.Core.time_eye_100                           8.070.1s
[ 82.14%]  bench_core.Core.time_eye_3000                          5.390.3ms
[ 83.93%]  bench_core.Core.time_hstack_l                          8.240.5s
[ 85.71%]  bench_core.Core.time_identity_100                      10.10.4s
[ 87.50%]  bench_core.Core.time_identity_3000                     5.490.3ms
[ 89.29%]  bench_core.Core.time_ones_100                          3.930.2s
[ 91.07%]  bench_core.Core.time_tril_indices_500                  1.110.1ms
[ 92.86%]  bench_core.Core.time_tril_l10x10                         21.31s
[ 94.64%]  bench_core.Core.time_triu_indices_500                 1.200.05ms
[ 96.43%]  bench_core.Core.time_triu_l10x10                         20.81s
[ 98.21%]  bench_core.Core.time_vstack_l                          9.830.3s
[100.00%]  bench_core.Core.time_zeros_100                           99040ns
```

master:
```
(base) C:\Users\J.W\Documents\GitHub\numpy>python runtests.py --bench bench_core.Core
Building, see build.log...
Build OK
 No executable found for python 3.7
 Discovering benchmarks
 Running 28 total benchmarks (1 commits * 1 environments * 28 benchmarks)
[  0.00%]  Benchmarking existing-pyc__users_j.w_anaconda3_python.exe
[  1.79%]  Running (bench_core.Core.time_arange_100--)............................
[ 51.79%]  bench_core.Core.time_arange_100                        1.180.1s
[ 53.57%]  bench_core.Core.time_array_1                             44510ns
[ 55.36%]  bench_core.Core.time_array_empty                      1.390.03s
[ 57.14%]  bench_core.Core.time_array_float64_l1000                 68.36s
[ 58.93%]  bench_core.Core.time_array_float_l1000                   86.89s
[ 60.71%]  bench_core.Core.time_array_float_l1000_dtype             43.82s
[ 62.50%]  bench_core.Core.time_array_int_l1000                     73.84s
[ 64.29%]  bench_core.Core.time_array_l                           3.150.1s
[ 66.07%]  bench_core.Core.time_array_l1                         1.530.08s
[ 67.86%]  bench_core.Core.time_array_l100                        11.30.8s
[ 69.64%]  bench_core.Core.time_array_l_view                      3.690.3s
[ 71.43%]  bench_core.Core.time_diag_l100                         21.40.5s
[ 73.21%]  bench_core.Core.time_diagflat_l100                       31.91s
[ 75.00%]  bench_core.Core.time_diagflat_l50_l50                    33.63s
[ 76.79%]  bench_core.Core.time_dstack_l                          10.20.3s
[ 78.57%]  bench_core.Core.time_empty_100                           92310ns
[ 80.36%]  bench_core.Core.time_eye_100                           8.220.3s
[ 82.14%]  bench_core.Core.time_eye_3000                          5.230.2ms
[ 83.93%]  bench_core.Core.time_hstack_l                          8.200.2s
[ 85.71%]  bench_core.Core.time_identity_100                      9.650.3s
[ 87.50%]  bench_core.Core.time_identity_3000                     5.310.2ms
[ 89.29%]  bench_core.Core.time_ones_100                          3.960.2s
[ 91.07%]  bench_core.Core.time_tril_indices_500                  2.240.1ms
[ 92.86%]  bench_core.Core.time_tril_l10x10                         20.21s
[ 94.64%]  bench_core.Core.time_triu_indices_500                  2.640.2ms
[ 96.43%]  bench_core.Core.time_triu_l10x10                       20.50.5s
[ 98.21%]  bench_core.Core.time_vstack_l                          9.740.3s
[100.00%]  bench_core.Core.time_zeros_100                           99520ns
```",2021-01-24T11:23:12Z
766335257,"> Sorry for the clutter :) Github isn't made for line editing documents.

No worries, thanks for the copy edits. All adopted.",2021-01-24T11:52:04Z
766345487,"Yes

On Sun, Jan 24, 2021, 15:52 Ralf Gommers <notifications@github.com> wrote:

> @Radhika14soni <https://github.com/Radhika14soni> are you planning to
> update this PR with the change requested by @mattip
> <https://github.com/mattip>?
>
> 
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/numpy/numpy/pull/18013#issuecomment-766323935>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ANFCBTW2ZONGAK7J7NBAOXLS3PYGTANCNFSM4U7RR7QQ>
> .
>
",2021-01-24T13:09:27Z
766373481,This is a meaningless change. Please do not practice your git skills here: make your own repo for that.,2021-01-24T16:04:16Z
766373607,"This is a meaningless change. Please do not practice your git skills here, use a personal repo.",2021-01-24T16:05:08Z
766394990,"Let's give it a shot. We might want to make the xfail more general, i.e., exitcode anything but 0, but this should cover the immediate problem. ",2021-01-24T17:03:54Z
766395019,Thanks Matti.,2021-01-24T17:04:08Z
766397778,Thanks Bas.,2021-01-24T17:21:01Z
766402826,Did you mean to update the version of the theme in [the requirements file](https://github.com/numpy/numpy/blob/master/doc_requirements.txt#L7)? They have released 0.4.2,2021-01-24T17:55:27Z
766403090,Are you sure this was merged? I see a `WARNING: unsupported theme option 'logo_link' given`,2021-01-24T17:57:16Z
766403491,"Right, that was just a rebase just now. Will push once I confirm the docs in my local.",2021-01-24T17:59:56Z
766406214,"Whoops you're right, the merge wasn't included in this release, gotta wait until they do I guess.",2021-01-24T18:14:42Z
766421548,Thanks Ralf.,2021-01-24T19:55:44Z
766424010,"Hmm, ignoring the warning seems correct. Strange that it doesn't show in most test runs.  Maybe warning tests can still sometimes be flaky :(.
I thought the new python versions (and pytest) were smart enough to avoid flakiness...

Thanks Chuck!",2021-01-24T20:11:23Z
766429209,"@DWesl, we use the term `FMA3` instead of `FMA` to avoid any confusion with `AMD/FMA4`.

@rgommers,
> any advice here on how to debug?

- Testing CPU detecting mechanism -> `python runtests.py -t numpy/core/tests/test_cpu_features.py`
  NOTE: You will have to patch `test_cpu_features.py` to enables `Cygwin`.
- Testing `_SIMD` module itself without any dispatched features:
   *  `python runtests.py --cpu-dispatch=""none"" -t numpy/core/tests/test_simd_module.py`
   *  `python runtests.py --cpu-dispatch=""none"" -t numpy/core/tests/test_simd.py`
- Testing `_SIMD` module with certain dispatched features, e.g. `sse41`
   `python runtests.py --simd-test=""baseline sse41"" -t numpy/core/tests/test_simd.py`
- Checking the build log, and finally GDB or LLDB
",2021-01-24T20:47:06Z
766487394,The performance improvements looks good to me.,2021-01-25T01:44:13Z
766593745,Thanks @Carreau ,2021-01-25T06:59:15Z
766600004,"Only 597 warnings left :(. Any idea why we get this one, which is probably 2/3 of the warnings?
```
WARNING: c:identifier reference target not found: PyObject
```",2021-01-25T07:10:16Z
766623509,"The code change here should not trigger the CI test failure of `numpy.core.tests.test_multiarray.TestMethods.test_no_dgemv`, will restart for verification.",2021-01-25T07:55:49Z
766623512,"It may look confusing to those who have written kLOCs of Python2 code. But every new Pythonista learns that 1 / 60 is a float. And there are 60 minutes in an hour, not 60.0.

Of course I can remove these changes. But that would be my question: would you actually like to get the code to Python3 level entirely, i.e. remove all Python2 relics?
",2021-01-25T07:55:50Z
766623533,Thanks @rgommers ,2021-01-25T07:55:52Z
766636563,"I think this change is fine - but there are subtleties to watch out for - `1 / float(x)` and `1/x` have different types when `x` has type `np.float64`. The two aren't quite interchangeable, but the former is sometimes risky as on longdouble it causes precision loss.",2021-01-25T08:18:30Z
766705174,"This was likely closed by #18097, though it's still missing the acceptance.",2021-01-25T10:10:02Z
766711296,"Thanks @h-vetinari, I forgot about this one. I'll close it through the next PR which changes the status to accepted and adds the mailing list links.",2021-01-25T10:20:20Z
766754039,"A couple of release highlights for mypy 0.800: https://mypy-lang.blogspot.com/2021/01/mypy-0800-released.html

By far the most interesting seems to be the added support for [PEP 604](https://www.python.org/dev/peps/pep-0604/), 
_i.e._ we can now use the ``|`` operator for writing unions:

``` python
def func(a: int | None) -> None:
   ...
```",2021-01-25T11:34:41Z
766782445,"Anyone knows what's up with the `DOWNLOAD_OPENBLAS=1 ATLAS=None` tests on [travis](https://travis-ci.com/github/numpy/numpy/jobs/474911090)?
It seems to be failing quite a bit recently, despite a lack of any clear reason as to why.",2021-01-25T12:28:21Z
766876833,Thanks @Illviljan ,2021-01-25T15:01:30Z
766883369,"That runs on ppc64le (PowerPC), and the hardware has been flaky in the past. ",2021-01-25T15:10:23Z
766884163,Thanks @BvB93 and dependabot. ,2021-01-25T15:11:31Z
766887364,"Comparing an anchor of the new [version of the theme](https://17941-908607-gh.circle-artifacts.com/0/doc/build/html/reference/generated/numpy.geterr.html#numpy.geterr) to the [current version](https://numpy.org/devdocs/reference/generated/numpy.geterr.html#numpy.geterr), they fixed the problem with the header covering the test. (issue #17207)",2021-01-25T15:16:13Z
766888175,Closed by updating the theme version to 0.4.2.,2021-01-25T15:17:18Z
766891406,"Good old dependabot, what a dependable fellow.",2021-01-25T15:21:59Z
766908651,"I don't think this is fixed by 0.4.2 (it's only a tiny bug fix release to fix a xarray display compatibility issue, it was not a release of latest master). It is fixed on master though, for the upcoming 0.5 release (and then you will need to set a `--header-height` variable)",2021-01-25T15:46:19Z
766912054,"Something changed. [This page](https://17941-908607-gh.circle-artifacts.com/0/doc/build/html/reference/generated/numpy.geterr.html#numpy.geterr) (from the CI build of the new theme) no longer overlaps for us. For the next few minutes, until CI finishes, you can see the old page [here](https://numpy.org/devdocs/reference/generated/numpy.geterr.html#numpy.geterr)",2021-01-25T15:51:14Z
766922727,"Hmm, on my browser (firefox, Ubuntu), both links give the incorrect offset ..",2021-01-25T16:05:32Z
766927114,On Firefox I can confirm the title is still overlapping for the CI build of the new theme. ,2021-01-25T16:12:04Z
766943522,"Closing, pyarrow should be fine now (and about to release).",2021-01-25T16:36:02Z
766968094,@kraj  Any progress?,2021-01-25T17:12:12Z
766969826,"@ader1990 Ping.

@zooba Any update on progress getting the ARM infrastructure in Azure?",2021-01-25T17:14:58Z
766972598,This looks ready to go. Comments?,2021-01-25T17:17:21Z
766972624,"> @ader1990 Ping.
> 
> @zooba Any update on progress getting the ARM infrastructure in Azure?

Unfortunately, I cannot access the Azure infra, but the code looks great and it works great IMHO.",2021-01-25T17:17:24Z
766973461,Needs rebase. Is this still relevant?,2021-01-25T17:18:38Z
766974379,What is the status of this?,2021-01-25T17:20:01Z
766975313,@eric-wieser Look good to you?,2021-01-25T17:21:24Z
766980838,"@charris yes, made this PR https://github.com/pandas-dev/pydata-sphinx-theme/pull/246 to close #17211, waiting to be released",2021-01-25T17:29:29Z
767014840,"nop, no clues. Most of the things I fix are due to another unrelated project that does not use sphinx/docutils.

Might be from `numpy/polynomial/*`, some functions seem to have a lot of `` `c` ``, and maybe sphinx don't resolve them properly ? and a few functions like `polyvander` seem to reference `` `c` `` but have no `c` parameters",2021-01-25T18:18:40Z
767015694,"> The note shows up for long double types, mostly in the sorting routines

According to the mail archive of [gcc/msg00634](https://gcc.gnu.org/legacy-ml/gcc-patches/2014-07/msg00634.html), ABI change affects to:

>  passing a struct by value that is
>   * not a float/vector special case, and
>   * has a size of 1, 2, 4, 8, or 16 bytes, and
>   * has an alignment requirement of 16 bytes or more
>  [ Not *all* these cases will see change, but all cases that change
>  will share these properties.  ]

> Anyone know why or if it is significant? The tests usually pass.

We can just suppress the ABI warnings through CLI `-Wno-psabi` or `#pragma`. ABI changes cause incompatible binaries with older versions of GCC e.g. `GCC 4.9`.

",2021-01-25T18:20:07Z
767017247,still present in version 1.19.2,2021-01-25T18:22:48Z
767019270,"Ahh, I see. I chose a page that is quite small. Because of differing zoom betwee the two versions, in one case the whole page fit better and showed the header, on the more-zoomed-in one it did now. ",2021-01-25T18:25:51Z
767025725,"I tried to give some more details about integers in the release notes, and mention the precision version `float64` rather than `float_` in the message.  So hopefully it is a bit more clear, it may be slightly opinionated.",2021-01-25T18:35:48Z
767029488,Needs rebase. I'm inclined to close this unless it is still relevant. @seberg @eric-wieser thoughts?,2021-01-25T18:42:27Z
767031271,Is this still relevant after recent changes to `stride_tricks` and the addition of a rolling window?,2021-01-25T18:45:42Z
767033103,Does the SIMD work supercede this? @Qiyu8 @seiko2plus Thoughts?,2021-01-25T18:48:52Z
767040538,"Since it wold require a rebase, lets close it.  I think there may still be some value in it, so if anyone wants to pick it up, that sounds reasonable to me. But I agree that in the majority of cases the new sliding window function or `broadcast_to` probably will do the trick better. So the need should be much lower, and a keyword argument is probably not as convenient as the alternatives.",2021-01-25T19:01:03Z
767047701,"It is still relevant, there was no change in behaviour with respect to the issue.

I am not sure whether the trailing bytes themselves should or should not be relevant. The main interest would be with respect to getting alignment right in the buffer protocol (this PR does that as far as I can tell).  But the mismatch between buffer protocol itemsize and the `length` that is set, does currently prevent roundtripping `array -> buffer -> array` in certain cases, so I guess there is something wrong.

(I am not sure how easy it is to rebase this PR, it might be annoying, but I expect all the pieces here are still relevant just as much)",2021-01-25T19:13:21Z
767062505,@uchida Thanks for making the PR.,2021-01-25T19:34:37Z
767063935,"Hmmm, I thought I would look into this. But I am honestly not sure where we should be adding this flag, or if we actually should (additionally) even prod Python itself to add this flag.  Am I right to think that most clang specific flags currently are inherited from Python itself?",2021-01-25T19:37:11Z
767080881,Where do we stand with this? Has it been discussed on the mailing list? Does it need to be or is this niche enough we can merge it as is?,2021-01-25T20:07:16Z
767085153,Last night's wheels built cleanly. Can we close this?,2021-01-25T20:15:08Z
767086166,"This has been mentioned multiple times on the list, just not super recently.",2021-01-25T20:16:58Z
767089123,"Yes. This was a problem on aarch64, but it seems fixed now. Triggering rebuilds succeeded in 2 out of 3, and the last after the fix went in (showed xfail).",2021-01-25T20:22:11Z
767183632,"I had the same problem. But I came up with a pretty simple solution.    
Please refer to the link below. 
https://stackoverflow.com/a/65893798/14411354

**Note:** I think enabling/disabling mmap_mode is not really related to this problem. So in this case, feel free to do whatever you think is better for your particular situations.",2021-01-25T23:41:21Z
767257136,"I've validated I can get basic loading under softlimit 400MB with `numpy-1.21.0.dev0+485.gbec2b07db-cp38-cp38-manylinux2010_x86_64.whl`, and a manually invoked build seems to build OK under 1GB softlimit.

Our formal build processes won't have any change until 1.19.6 or later comes out on pip

Thanks for your help",2021-01-26T03:02:43Z
767284249,Thanks @h-vetinari .,2021-01-26T04:11:31Z
767288919,What was the reason to change the 1.20.0 release note?,2021-01-26T04:29:38Z
767291192,"Thanks, have to try how to fix that reference. Will fix (maybe tomorrow morning).  There was some discussion that this release note/deprecation is maybe a bit unclear. I hope this is a bit clearer (and ""recipe style""), so the idea was to change it before the release.",2021-01-26T04:37:45Z
767341664,Thanks @seberg,2021-01-26T06:49:59Z
767366158,"Thanks for the review @seberg , I shall fix the scalar conversion of `other` and then add the tests you mentioned as well.",2021-01-26T07:48:33Z
767373265,"IMO, A `memcpy&memset` method is provided to replace `SIMD/uloop` method for special cases such as readonly masked arrays,  but I don't see enough evidence to proves the benefits here. if It's shows better performance than `SIMD/uloop`, then we should accept it. OTOH, this PR reminds me that #16960 is extending current sse2 functions to universal intrinsics, we can have a bench test after #16960 is merged.",2021-01-26T08:04:29Z
767394663,"No more follow-up, and indeed looks like a duplicate. I've also changed the title, because this seems unrelated to Alpine.",2021-01-26T08:47:29Z
767404482,I don't see a difference in the rendered documents [before](https://numpy.org/devdocs/reference/generated/numpy.take_along_axis.html) and [after](https://17959-908607-gh.circle-artifacts.com/0/doc/build/html/reference/generated/numpy.take_along_axis.html). Am I missing something?,2021-01-26T09:05:47Z
767468332,"I have been bitten several times by this problem. Since the docstring says about `shift`
```
If a tuple, then `axis` must be a tuple of the same size.
```
 it would be great to enforce this in the code and raise an exception if this condition is not fulfilled, rather than carrying out an operation which the documentation clearly declares invalid.",2021-01-26T11:01:29Z
767618955,"<img width=""347"" alt=""Screen Shot 2021-01-26 at 07 21 32"" src=""https://user-images.githubusercontent.com/335567/105864800-28959180-5fa7-11eb-8168-5e392f5ddd24.png"">

Space before the colon, italic for ndarray. Hard to see for this theme, but could be more proeminent in other. 

It's not about the style, it's about the semantic, and it prevent a terminal render I'm working on in IPython to detect that the documented parameters are `arr` and `indices`.
<img width=""428"" alt=""Screen Shot 2021-01-26 at 07 23 36"" src=""https://user-images.githubusercontent.com/335567/105865162-7e6a3980-5fa7-11eb-8bdc-ca95e2164527.png"">

wile a properly spaced items will be detected correctly by numpydoc (here np.where), see how `condition`, `x` and `y` are green so correctly infered.


<img width=""385"" alt=""Screen Shot 2021-01-26 at 07 24 38"" src=""https://user-images.githubusercontent.com/335567/105865257-9641bd80-5fa7-11eb-8c85-bd1c744ece57.png"">


",2021-01-26T15:26:35Z
767651773,Thanks @Carreau ,2021-01-26T16:12:42Z
767663721,"I just hit this through pyupgrade also. The fact that `%.4f`-formatting works but f-strings/`.format` doesn't, seems like a genuine bug.",2021-01-26T16:30:39Z
767667405,"I checked that the latest master, and geopandas tests are passing again now (didn't check with the 1.20.x maint branch, but assuming that that will be OK as well since the fix is backported). Thanks!",2021-01-26T16:36:10Z
767670613,The documentation is correct in its current form. The following paragraphs explain the difference between `__array__` and other methods like `__array_ufunc__` and `__array_function__` in terms of how output types are handled.,2021-01-26T16:40:56Z
767682734,"No opinions from me - note that I originally opened the issue only to track a TODO that was removed when some documentation was updated. I personally don't use the setops often, so I'm not sure what the most common use-cases are.",2021-01-26T16:59:28Z
767684403,"Allow me to insist:

- the `arr` object is of type `DiagonalArray`, not `ndarray`, so the current block is incorrect.
- The following paragraph explains how one can make `np.multiply` return a `DiagonalArray` instead of a `ndarray`:

> How can we pass our custom array type through this function?...


I believe that the current version assumes that `arr` is the output from `np.multiply` above, but it's not.",2021-01-26T17:01:47Z
767695913,"> I believe that the current version assumes that arr is the output from np.multiply above, but it's not.

The output of `np.multiply(arr, 2)` is the object returned by `__array__`, i.e. the result of `self._i * np.eye(self._N)`: `np.eye` creates an `ndarray` object.

Broken into finer steps:

```python
>>> arr = DiagonalArray(5, 1)
>>> type(arr)
__main__.DiagonalArray
>>> b = np.multiply(arr, 2)
>>> type(b)
numpy.ndarray
```",2021-01-26T17:19:50Z
767700021,"@rossbar I understand all that but please read the document. It goes like this:

```py
>>> arr = DiagonalArray(5, 1)
>>> type(arr)
__main__.DiagonalArray
>>> np.multiply(arr, 2)
>>> type(arr)
numpy.ndarray
```

which is wrong. Note how the output of `np.multiply` is **not** `arr` and it's **not** assigned to anything.",2021-01-26T17:26:32Z
767719630,"Just to be clear, `type(np.multiply(arr, 2))` is calling `type()` on the output of `np.multiply(arr, 2)`, which, as you point out, is not `arr`. Thus the original docs are correct.

In your above example, you have

```python
>>> np.multiply(arr, 2)
>>> type(arr)
```

Which would be incorrect, but is different than the original documentation.",2021-01-26T17:56:57Z
767725268,"> Thus the original docs are correct.

Sorry, no

> but is different than the original documentation.

No, it's not different. Are we looking at the same document?
",2021-01-26T18:05:54Z
767725556,"
Here are the commands that are present in the master branch of the docs:

```py
In [1]: import numpy as np

In [2]: class DiagonalArray:
   ...: ...     def __init__(self, N, value):
   ...: ...         self._N = N
   ...: ...         self._i = value
   ...: ...     def __repr__(self):
   ...: ...         return f""{self.__class__.__name__}(N={self._N}, value={self._i})""
   ...: ...     def __array__(self):
   ...: ...         return self._i * np.eye(self._N)
   ...: 

In [3]: arr = DiagonalArray(5, 1)

In [4]: arr
Out[4]: DiagonalArray(N=5, value=1)

In [5]: np.asarray(arr)
Out[5]: 
array([[1., 0., 0., 0., 0.],
       [0., 1., 0., 0., 0.],
       [0., 0., 1., 0., 0.],
       [0., 0., 0., 1., 0.],
       [0., 0., 0., 0., 1.]])

In [6]: np.multiply(arr, 2)
Out[6]: 
array([[2., 0., 0., 0., 0.],
       [0., 2., 0., 0., 0.],
       [0., 0., 2., 0., 0.],
       [0., 0., 0., 2., 0.],
       [0., 0., 0., 0., 2.]])

In [7]: type(arr)
Out[7]: __main__.DiagonalArray. # <------ This is not ndarray unlike the docs claim!!!!!!!!!
```",2021-01-26T18:06:23Z
767727763,"Wow - huge brainfart on my part - I just failed to interpret the diff correctly (I was assuming your change was what was already in the docs). So when I say ""the original docs are correct"", what I meant was ""your original proposed changes are correct!

Sorry for all the noise.",2021-01-26T18:10:10Z
767730276,No wonder I couldn't figure out why we were disagreeing even though I felt like we were saying the same thing :) - thanks for bearing with me @NicolasHug ,2021-01-26T18:13:37Z
767740064,"This looks great, thanks @seberg!",2021-01-26T18:29:26Z
767797166,"Thanks Eric and Ross for figuring out how to correctly link the user guide... applied the changes, so should be good to go (assuming CI passes now).",2021-01-26T20:09:18Z
767803640,"> Any update on progress getting the ARM infrastructure in Azure?

Let me be careful here... there is _progress_, but there is no _update_... yeah I think I can say that much. :)",2021-01-26T20:21:03Z
767828083,"@charris I think the test results above hold, for my purposes its working ok.",2021-01-26T21:07:09Z
767831325,Thanks sebastian.,2021-01-26T21:13:15Z
767896522,Thanks @SnoopJeDi .,2021-01-26T23:26:15Z
767920529,"Yeah, the original issue seems to be about a reference cycle and thousands of opened files, while the memory mapping mode is more relevant to few but large files.  I guess the original issue was largely a bug in the Python garbage collector, but there is likely still a use in cleaning this up, it may well be possible to just resolve the reference cycle (I have not looked into it, but with a weakref or otherwise) and avoid any garbage collection related issues.

Especially the simple `np.load` call that doesn't even load an `npz` should not have these issues.  I will reopen the issue.

EDIT: To be clear, I have not double checked that current master (1.20) still exhibits this issue.",2021-01-27T00:28:51Z
767988251,"I was also recently bitten by this. Given the usual `numpy` semantics, I kinda expected
```python
arr = np.array([[1, 0, 0],
                [0, 2, 0],
                [0, 0, 3],
                [4, 0, 5]])
np.roll(arr, np.arange(3), axis=0)
np.roll(arr, np.arange(4), axis=1)
```
To result in
```python
[[1, 0, 3],
 [0, 0, 5],
 [0, 2, 0],
 [4, 0, 0]]
```
and
```python
[[1, 0, 0],
 [0, 0, 2],
 [0, 3, 0],
 [4, 0, 5]]
```

The intuition here would be that `shift` is broadcasted together with `arr` (sans `axis`) and each ""matching"" 1-D array is rolled a different amount, specified by `shift`. So something along the lines of (but hopefully more efficient):

```python
Ni, M, Nk = a.shape[:axis], a.shape[axis], a.shape[axis+1:]
assert shift.shape == (Ni + Nk)
out = np.empty_like(a)

for ii in np.ndindex(Ni):
    for kk in np.ndindex(Nk):
        match = ii + np.s_[:,] + kk
        out[match] = np.roll(a[match], shift[ii + kk])
```",2021-01-27T03:18:05Z
768003980,"@melissawm and @mattip thanks for the comments/suggestions. I apologize for letting this sit, the Holidays were much more hectic than I imagined. 

I made most of the suggested changes, but then hit `checks were not successful` again. I'm having some trouble building NumPy with `pip install .`. I hit the import error:
```
We have compiled some common reasons and troubleshooting tips at:

    https://numpy.org/devdocs/user/troubleshooting-importerror.html
```

Am I missing a step in the build process? or doing it wrong?",2021-01-27T03:52:09Z
768067439,More information needed. What error exactly are you seeing?,2021-01-27T06:34:07Z
768074889,The documentation for the [`__array__` function](https://numpy.org/devdocs/reference/arrays.classes.html#numpy.class.__array_priority__) is not very descriptive. It describes what the ufunc *will not* do if `__array__` is present. It should describe what the prospective `__array__` user *is expected to do* with `self` and optional `dtype`. That would be nice to expand upon here or in a follow-up PR.,2021-01-27T06:51:52Z
768147317,"There is a stray `""` in `longer build or would give incorrect results.""`",2021-01-27T09:15:08Z
768268072,"> More information needed. What error exactly are you seeing?

My process: 
1. create fresh Python 3.8 conda environment
2. install cython
3. `pip install .` in NumPy source folder

Here is the output from `pip install .`
```
  Created wheel for numpy: filename=numpy-1.21.0.dev0+566.g49cbaf96f-cp38-cp38-linux_x86_64.whl size=17034457 sha256
=11a020022e59157a7507556add9a3f1857508b6290038ac24b86052463664c09                                                   
  Stored in directory: /tmp/pip-ephem-wheel-cache-f2hpe7hs/wheels/2f/4d/d7/a2e907151ccb709c255da24ca6e22caa300bdb551
9815b1ac2                                                                                                           
Successfully built numpy                                                                                            
Installing collected packages: numpy                                                                                
Successfully installed numpy-1.21.0.dev0+566.g49cbaf96f
```

Then, I open a Python interpreter and get the following error with `import numpy`
```
Traceback (most recent call last):                                                                                  
  File ""/home/ryan/.conda/envs/doc-fresh-38/lib/python3.8/site-packages/numpy/core/__init__.py"", line 22, in <module
>                                                                                                                   
    from . import multiarray                                                                                        
  File ""/home/ryan/.conda/envs/doc-fresh-38/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 12, in <modu
le>                                                                                                                 
    from . import overrides                                                                                         
  File ""/home/ryan/.conda/envs/doc-fresh-38/lib/python3.8/site-packages/numpy/core/overrides.py"", line 7, in <module
>                                                                                                                   
    from numpy.core._multiarray_umath import (                                                                      
ImportError: /home/ryan/.conda/envs/doc-fresh-38/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38
-x86_64-linux-gnu.so: undefined symbol: cblas_sgemm                                                                 
                                                                                                                    
During handling of the above exception, another exception occurred:                                                 
                                                                                                                    
Traceback (most recent call last):                                                                                  
  File ""<stdin>"", line 1, in <module>                                                                               
  File ""/home/ryan/.conda/envs/doc-fresh-38/lib/python3.8/site-packages/numpy/__init__.py"", line 150, in <module>   
    from . import core                                                                                              
File ""/home/ryan/.conda/envs/doc-fresh-38/lib/python3.8/site-packages/numpy/core/__init__.py"", line 48, in <module
>
    raise ImportError(msg)
ImportError: 

IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!

Importing the numpy C-extensions failed. This error can happen for
many reasons, often due to issues with your setup or how NumPy was
installed.

We have compiled some common reasons and troubleshooting tips at:

    https://numpy.org/devdocs/user/troubleshooting-importerror.html

Please note and check the following:

  * The Python version is: Python3.8 from ""/home/ryan/.conda/envs/doc-fresh-38/bin/python""
  * The NumPy version is: ""1.21.0.dev0+566.g49cbaf96f""

and make sure that they are the versions you expect.
Please carefully study the documentation linked above for further help.

Original error was: /home/ryan/.conda/envs/doc-fresh-38/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpy
thon-38-x86_64-linux-gnu.so: undefined symbol: cblas_sgemm 
```",2021-01-27T12:58:34Z
768302965,"> `_multiarray_umath.cpython-38
-x86_64-linux-gnu.so: undefined symbol: cblas_sgemm`

Something is off with the BLAS implementation found when building. I assume you are on Linux? If `ldd multiarray_umath.cpython-38-x86_64-linux-gnu.so` is showing a system library, can you update? You could do something like `python tools/openblas_support.py` (which will download and unpack the OpenBLAS used in CI), it should tell you where it put the files, let's say that is `<path-reported>`. Then you can create a `site.cfg` with the content
```
[openblas]
libraries = openblas
library_dirs = <path-reported>/lib
include_dirs = <path-reported>/include
runtime_library_dirs = <path-reported>/lib
```

If I didn't mislead you, now when you build numpy and `ldd` the c-extension module, it should use that openblas.",2021-01-27T13:58:24Z
768317608,"It seems like we now have a (draft) pep for variadic generics, 
_i.e._ generics with not one but an arbitrary number of variables (like `typing.Tuple`): [PEP 646](https://www.python.org/dev/peps/pep-0646/).
",2021-01-27T14:21:24Z
768345314,"> > `_multiarray_umath.cpython-38 -x86_64-linux-gnu.so: undefined symbol: cblas_sgemm`
> 
> Something is off with the BLAS implementation found when building. I assume you are on Linux? If `ldd multiarray_umath.cpython-38-x86_64-linux-gnu.so` is showing a system library, can you update? You could do something like `python tools/openblas_support.py` (which will download and unpack the OpenBLAS used in CI), it should tell you where it put the files, let's say that is `<path-reported>`. Then you can create a `site.cfg` with the content
> 
> ```
> [openblas]
> libraries = openblas
> library_dirs = <path-reported>/lib
> include_dirs = <path-reported>/include
> runtime_library_dirs = <path-reported>/lib
> ```
> 
> If I didn't mislead you, now when you build numpy and `ldd` the c-extension module, it should use that openblas.

The `site.cfg` + `openblas_support.py` installation worked, thanks! no errors with `import numpy` now. ",2021-01-27T15:02:14Z
768393910,"Just noticed this function has been added to version 1.20.0, hooray!

Although writing `np.lib.stride_tricks.sliding_window_view` is a mouthful, would be great if this could be moved to the main module and have its name shortened ",2021-01-27T16:11:05Z
768397281,"Well, you're free to do
```python
from numpy.lib.stride_stricks import sliding_window_view
```

I think @rgommers voiced against putting this in the main namespace on the mailing list, although I may be mistaken.

But yes, `sliding_window_view(arr, window_shape=(2, 2))` should do exactly what this issues asks for, so I think this issue can be closed. If you want to discuss naming, I'd recommend finding and responding to the mailing list thread, or opening a new issue.",2021-01-27T16:16:03Z
768417915,Sounds like we should consider this a bug (although when fixing I would probably just go with a `VisibleDeprecationWarning` just in case).,2021-01-27T16:47:24Z
768422683,"Seems correct to me, in the sense that `max(..., 0)` *can* (and does in the test run) return an integer and it is entirely possible for you to ensure that it does not.  In many cases defining `otypes` may be a good thing to ensure this in any case.",2021-01-27T16:54:16Z
768477669,"On my side things are ok, just pending the indentation issues. Thanks, @cooperrc !",2021-01-27T18:17:51Z
768490168,"> On my side things are ok, just pending the indentation issues. Thanks, @cooperrc !

Thanks @melissawm!

regarding indentation, do you mean the outputs or the code blocks? I think I caught the indentation issues we discussed above. I'll build again and check formatting. ",2021-01-27T18:39:19Z
768515076,Checkout the CI failure on cirecleCI build -> details. It is complaining about indentation and line 34.,2021-01-27T19:17:04Z
768518680,"We had discussed this, and we somewhat agreed that this is desireable, and should have `FutureWarnings` (or `VisibleDeprecationWarning`.

A specific issue may be `np.loadtext` and `genfromtxt` since that might be a more common use case for using 0 and 1 as boolean, so that may need some special consideration.

I hope to look at this, hopefully, `FutureWarnings` (and opting in to future behaviour) will be reasonably possible.",2021-01-27T19:22:51Z
768546192,"> Checkout the CI failure on cirecleCI build -> details. It is complaining about indentation and line 34.

Thanks, I see that. I don't know why its complaining about the indent for the list. I tried making it one line in the last commit since it doesn't like my indents (third time's a charm). ",2021-01-27T20:09:54Z
768551057,"`SeedSequence.spawn(n)` always returns a list of `n` elements, even when `n==1`. Use `child_seq = old_seed_seq.spawn(1)[0]` to make sure that the `child_seq` is the actual desire `SeedSequence`.",2021-01-27T20:19:03Z
768580300,"Agreed @mattip - in fact the documentation is so unclear, that I am not even sure what it is saying! Thus I am afraid that I might not be the one to come up with better wording.",2021-01-27T21:12:16Z
768585641,"If you are up for it, that would be great anyway, we can always iterate together (no pressure though). The only subtlety I can really think of, is that it should be completely fine to ignore `dtype` which would just trigger casting in NumPy itself.",2021-01-27T21:22:07Z
768601498,"I argued against step_size when we designed this function, because you can get that behavior 
already with `sliding_window_view(x, 3)[::2]`.",2021-01-27T21:53:05Z
768610616,"> > Checkout the CI failure on cirecleCI build -> details. It is complaining about indentation and line 34.
> 
> Thanks, I see that. I don't know why its complaining about the indent for the list. I tried making it one line in the last commit since it doesn't like my indents (third time's a charm).

L34 indentation was changed to a single line. It fixed the circleci build test. Looks like we are ready to merge. ",2021-01-27T22:07:21Z
768644629,"On second thought, we should probably just get rid of `make dist` completely, and build all release artifacts via CI. `make dist` is really weird, the `Makefile` for the docs should just build the docs, not jump through weird hoops to build numpy and put it in a nonstandard place.",2021-01-27T23:24:44Z
768649065,Thanks Ralf.,2021-01-27T23:34:25Z
768650619,`runtests` has most of the machinery needed to build docs.,2021-01-27T23:38:22Z
768764235,"> Seems correct to me, in the sense that `max(..., 0)` _can_ (and does in the test run) return an integer and it is entirely possible for you to ensure that it does not. In many cases defining `otypes` may be a good thing to ensure this in any case.

In the output, the columns 'duration' is the np.vectorize implementation, and 'duration2' and 'duration3' are pd.apply and map implementation. I know specifying otypes is good practice, but I doubt most of the users would not do it.

Without specifying otypes, the np implemention gives diff result than the other implementations, which users might not aware, as in the arrears_months columns = [3, 3.1, 4.3, 4.1, 5] is defined to be float, while using float array in an operation returns int array is really unexpected behavior.

Although it's probably not necessary, but align the outcome of one implementation with other implementations (especially map is the default python function), could really be beneficial to the users.",2021-01-28T03:01:28Z
768814989,"OK, let's put this in as-is, it is a good first step.",2021-01-28T05:44:53Z
768815084,Thanks @rsokl ,2021-01-28T05:45:10Z
768817130,Thanks @Carreau ,2021-01-28T05:51:15Z
768820366,Thanks @Carreau ,2021-01-28T06:00:05Z
769150287,"The test `Build_Test / debug (pull_request)` failed but not due to my commits but the container cannot download a package:
```
E: Failed to fetch http://azure.archive.ubuntu.com/ubuntu/pool/main/g/glibc/libc6-dbg_2.31-0ubuntu9.1_amd64.deb  404  Not Found [IP: 52.252.75.106 80]
```
Unfortunately I don't know how to restart the test.",2021-01-28T15:09:59Z
769199368,"You return `max(-0.1, 0)` which *is* an integer for the first value, and that is how it is designed to work; maybe that can be changed, but I doubt it, you always have similar issues if the type is fixed at some point.",2021-01-28T16:19:18Z
769484437,I make heavy use of `numpy.nonzero()` on computational geometry work--will be great to see some potential speedups there.,2021-01-29T00:18:25Z
769492740,"> Unfortunately I don't know how to restart the test.

Don't worry about it. Because this adds a new function it needs to be run by the mailing list first. My own preference would be to start with an `absolute_square` ufunc, which ISTR has been discussed before.
",2021-01-29T00:40:03Z
769495112,"TBH, I'd rather see an attempt to add some abbreviated explanations to the  ""see"" references, and I suspect a stab could be made for the types also.",2021-01-29T00:47:02Z
769548038,"`range()` generate python3 built-in integers that have unlimited precision, while numpy's `arange()` produces numbers are implemented using `int32`/`int64` in C. so there is an implied information that ndarray uses ctypes. If you have a clearer explanation, welcome to submit a document PR.",2021-01-29T03:22:45Z
769582664,"Yeah, i'm expecting other things to be wrong most of those changes were autogneratd.",2021-01-29T05:22:35Z
769599315,"Merged, thanks @Carreau ",2021-01-29T06:07:57Z
769624037,@btel was this eventually fixed?,2021-01-29T07:10:11Z
769637500,Thanks @rgommers ,2021-01-29T07:41:50Z
769646623,"apparently not yet

```
>>> import numpy as np
>>> np.array(['a', 1]).dtype
dtype('<U1')
>>> np.array([1, 'a']).dtype
dtype('<U21')
>>> np.__version__
'1.19.2'
```",2021-01-29T08:03:55Z
769766820,"Merged, thanks!",2021-01-29T12:07:08Z
769773173,"All green, merging. Thanks @mattip ",2021-01-29T12:20:45Z
769874297,"Its fixed *and* deprecated in 1.20, you have to use `np.array([1, ""a""], dtype=""U"")`, which will give you the identical result in both cases to begin with. The result is than always length 1, just because that is the more common variant currently.",2021-01-29T15:30:58Z
769877420,"Sorry, its a bit ""weirder""... Just to be clear: Right now, its deprecated, but you get `U21` *always*.  If you use `dtype=""U""` (which you should and will have to), the result is unchanged and gives you `U1` always.",2021-01-29T15:36:35Z
769977988,"> I'm working on an auto formatter, and a side effect of how it work is that it's difficult to distinguish empty sections from section that do not exists when interacting with numpydoc's Docsscrape, I could add a ""keep empty sections"" if that's desired.

That's a really nice project @Carreau! I just wanted to bring up that the numpydoc extension does have a [validation module](https://numpydoc.readthedocs.io/en/latest/validation.html) that seems like it has a lot of overlap with the types of style/validation checks performed in velin. I realize the goals of the projects are distinct, but it might be nice to consider adding any formatting/validation checks that you are doing for velin to the numpydoc validator as well --- there seems to be some overlap already, e.g. [PR10](https://github.com/numpy/numpydoc/blob/da816b5b4e876dfb24807dfd3c12f0efe6f27cb7/numpydoc/validate.py#L75-L76) covers the ""space before the colon"" check. I know you've already added some of these checks, e.g. numpy/numpydoc#278 - just wanted to point out that the validation module would be a natural place for these types of checks IMO.

As of now the docstring validation from numpydoc is only available via commandline, but there's a [PR](https://github.com/numpy/numpydoc/pull/302) to add the docstring validation to the sphinx build process (with configuration option so that users can specify which checks they want). ",2021-01-29T18:38:55Z
770020488,@seberg Can this be closed?,2021-01-29T20:03:36Z
770022373,What is the status of this?,2021-01-29T20:07:16Z
770024043,"I was having bad errors trying to install numpy from a pyenv install of python3 on my M1 until I used this:

```OPENBLAS=""$(brew --prefix openblas)"" pip install --force-reinstall --no-cache-dir numpy```

and numpy installed correctly 
(I had previously done ```brew install openblas```).

Also was still having problems installing pandas until I **installed cython**.",2021-01-29T20:10:27Z
770025831,"I think this is important, but I am not very familiar with distutils, and am not sure where this flag should be inserted correctly. (It probably is also correct for SciPy, but do we want to set it by default for scipy as well?).
I am happy to check for the related code cleanups, but not too fond of digging through distutils to find the right place.

It shouldn't matter too much to push it off to 1.21 all known issues due to this are fixed after all. While the flag change is something we could do, the (probably) related code cleanup we do not want to backport anyway probably.",2021-01-29T20:14:15Z
770033988,I kicked this off to the 1.20.1 release as I expect the Windows update will be out before then.,2021-01-29T20:31:52Z
770052523,Thanks Sebastian.,2021-01-29T21:11:58Z
770055961,"Changes are harmless, so in it goes. Thanks @Carreau .",2021-01-29T21:19:42Z
770090457,"
> TBH, I'd rather see an attempt to add some abbreviated explanations to the ""see"" references, and I suspect a stab could be made for the types also.

Can you elaborate ? I'm not sure I understand ? 




> That's a really nice project @Carreau! I just wanted to bring up that the numpydoc extension does have a [validation module](https://numpydoc.readthedocs.io/en/latest/validation.html) that seems like it has a lot of overlap with the types of style/validation checks performed in velin. I realize the goals of the projects are distinct, but it might be nice to consider adding any formatting/validation checks that you are doing for velin to the numpydoc validator as well --- there seems to be some overlap already, e.g. [PR10](https://github.com/numpy/numpydoc/blob/da816b5b4e876dfb24807dfd3c12f0efe6f27cb7/numpydoc/validate.py#L75-L76) covers the ""space before the colon"" check. I know you've already added some of these checks, e.g. [numpy/numpydoc#278](https://github.com/numpy/numpydoc/pull/278) - just wanted to point out that the validation module would be a natural place for these types of checks IMO.
> 
> As of now the docstring validation from numpydoc is only available via command line, but there's a [PR](https://github.com/numpy/numpydoc/pull/302) to add the docstring validation to the sphinx build process (with configuration option so that users can specify which checks they want).

Thanks, and yes I had a look and need to re-check as well; I just found that the steps to take for validation and formatting are currently quite different. I tend to throw away all formatting operations and recreate the docstring from scratch from the raw data, which lead me to also checking (and trying to) autofix the names of the parameters, and to reflow RST, which led me to my terminal rendered who also have different requirement; but I think I'm getting slowly close to end end of the stack and it will likely wind up back to numpydoc at some point. 
",2021-01-29T22:47:33Z
770129319,"> > For you reference (Macbook M1),
> > OPENBLAS=""$(brew --prefix openblas)"" MACOSX_DEPLOYMENT_TARGET=11.1 pip3 install numpy pandas --no-use-pep517
> > It will install pandas-1.2.0 and numpy-1.19.5 successfully.
> 
> This worked! I had to jump through a few hoops to get some packages installed and I'm not sure how many times the OPENBLAS argument was actually needed, but this got me through the installation of all the packages I wanted installed:
> 
> $ brew install python@3.9
> $ brew install openblas
> $ OPENBLAS=""$(brew --prefix openblas)"" MACOSX_DEPLOYMENT_TARGET=11.1 python3 -m pip install cython --no-use-pep517
> $ OPENBLAS=""$(brew --prefix openblas)"" MACOSX_DEPLOYMENT_TARGET=11.1 python3 -m pip install numpy --no-use-pep517
> $ OPENBLAS=""$(brew --prefix openblas)"" MACOSX_DEPLOYMENT_TARGET=11.1 python3 -m pip install pandas --no-use-pep517
> $ OPENBLAS=""$(brew --prefix openblas)"" MACOSX_DEPLOYMENT_TARGET=11.1 python3 -m pip install pybind11 --no-use-pep517
> $ OPENBLAS=""$(brew --prefix openblas)"" MACOSX_DEPLOYMENT_TARGET=11.1 python3 -m pip install scipy --no-use-pep517
> $ brew install libjpeg zlib
> $ python3 -m pip install pillow
> $ python3 -m pip install matplotlib
> 
> all the other packages listed in the requirements.txt file were then able to be installed.
> [requirements.txt](https://github.com/numpy/numpy/files/5783558/requirements.txt)

The same approach worked on my machine (MBP M1) for `scikit-learn` and `statsmodels`:

```
OPENBLAS=""$(brew --prefix openblas)"" MACOSX_DEPLOYMENT_TARGET=11.1 python3 -m pip install scikit-learn --no-use-pep517
OPENBLAS=""$(brew --prefix openblas)"" MACOSX_DEPLOYMENT_TARGET=11.1 python3 -m pip install statsmodels --no-use-pep517
```",2021-01-30T01:15:48Z
770198448,"Any news about that stuff? It's a while ago, but the issue remains, I guess.",2021-01-30T11:34:17Z
770234413,Should we close this?,2021-01-30T16:06:53Z
770234678,@eric-wieser @seberg Just thought I'd ping this to check the status.,2021-01-30T16:08:48Z
770244252,"I'm not sure that I find the motivation for adding a separate function for this convincing. The accuracy issue in the example given is `2 * eps`, which is negligible:
```
>>> x = np.array([[1, 2], [3, 4]])
>>> y = np.array([[2, 2], [4, 4]])
>>> (np.linalg.norm(x - y)**2 - 2)/ np.finfo(np.float64).eps
2.0
```

The performance improvement amounts to avoiding one square root of a scalar - on the order of 1 us.

Neither seems worth going through a lot of trouble for.",2021-01-30T17:01:45Z
770247115,"Your code example doesn't actually use numpy in any meaningful way. You would see exactly the same behavior by defining `nan = float(""nan"")` and using `nan` instead of `np.nan` - so while I agree this is annoying, I don't think numpy can do anything about it.",2021-01-30T17:23:33Z
770266416,`data[39] == 27` which is the maximum value in this list.,2021-01-30T19:15:00Z
770266765,"If I clip the data such that there are multiple maximum values of 20, we get the `argmax` of 12 as expected:

```
[~]
|9> clipped = np.clip(data, -100, 20)

[~]
|10> np.argmax(clipped)
12
```

I don't think there is a bug here.",2021-01-30T19:17:23Z
770290216,"@charris thanks. 
Is the doc update on the TODO list?:
- copy the 1.20 docs to the repo https://github.com/numpy/doc
- move the `stable` symlink to the 1.20 directory",2021-01-30T22:26:59Z
770292331,I posted an updated version of this issue in the python bug tracker and got back a response that explains why this is expected behavior. Linking it here - in case anyone else runs into the same or a similar issue: https://bugs.python.org/issue43078,2021-01-30T22:46:53Z
770292590,"> Is the doc update on the TODO list?:

It is, but `make dist` fails in the 1.20.x branch with `cythonize` problems, so I've put it on hold until I can deal with it. Not sure why `runtests` works and `make dist` fails though. I'm hoping that a backport of #18144 will fix things.",2021-01-30T22:49:47Z
770294608,"Problem seems to have been due to two installs of Python3.9 after I upgraded the system Jan 5, one in `/usr/lib` the other in `/usr/local/lib`.",2021-01-30T23:07:42Z
770299964,Lot of oddball fonts needed that will take a while to track down and install.,2021-01-30T23:54:44Z
770311409,Documentation up.,2021-01-31T01:49:49Z
770334297,Thanks @charris ,2021-01-31T06:20:26Z
770334465,PR welcome,2021-01-31T06:22:42Z
770352505,This is presumably an artefact of a bad rst to Markdown converter?,2021-01-31T09:25:06Z
770353798,"Ah very odd. I didn't even consider that this stuff was not present in `source/release/1.20.0-notes.rst`. Seems to be a new GitHub rendering bug.

Thanks for pointing that out. Closing.",2021-01-31T09:34:23Z
770375887,"`cibuildwheel` has full support now it looks like. Doesn't help without hardware or fixing the critical bugs we still have for M1 (see my first comment on this issue), but it's a good point of reference.",2021-01-31T12:36:49Z
770380108,"Bah! I'm an idiot, fixed it. Was a local installation issue.",2021-01-31T13:08:13Z
770380615,I think multibuild has full support too - thanks to @isuruf.,2021-01-31T13:12:35Z
770382222,"> > > For you reference (Macbook M1),
> > > OPENBLAS=""$(brew --prefix openblas)"" MACOSX_DEPLOYMENT_TARGET=11.1 pip3 install numpy pandas --no-use-pep517
> > > It will install pandas-1.2.0 and numpy-1.19.5 successfully.
> > 
> > 
> > This worked! I had to jump through a few hoops to get some packages installed and I'm not sure how many times the OPENBLAS argument was actually needed, but this got me through the installation of all the packages I wanted installed:
> > $ brew install python@3.9
> > $ brew install openblas
> > $ OPENBLAS=""$(brew --prefix openblas)"" MACOSX_DEPLOYMENT_TARGET=11.1 python3 -m pip install cython --no-use-pep517
> > $ OPENBLAS=""$(brew --prefix openblas)"" MACOSX_DEPLOYMENT_TARGET=11.1 python3 -m pip install numpy --no-use-pep517
> > $ OPENBLAS=""$(brew --prefix openblas)"" MACOSX_DEPLOYMENT_TARGET=11.1 python3 -m pip install pandas --no-use-pep517
> > $ OPENBLAS=""$(brew --prefix openblas)"" MACOSX_DEPLOYMENT_TARGET=11.1 python3 -m pip install pybind11 --no-use-pep517
> > $ OPENBLAS=""$(brew --prefix openblas)"" MACOSX_DEPLOYMENT_TARGET=11.1 python3 -m pip install scipy --no-use-pep517
> > $ brew install libjpeg zlib
> > $ python3 -m pip install pillow
> > $ python3 -m pip install matplotlib
> > all the other packages listed in the requirements.txt file were then able to be installed.
> > [requirements.txt](https://github.com/numpy/numpy/files/5783558/requirements.txt)
> 
> The same approach worked on my machine (MBP M1) for `scikit-learn` and `statsmodels`:
> 
> ```
> OPENBLAS=""$(brew --prefix openblas)"" MACOSX_DEPLOYMENT_TARGET=11.1 python3 -m pip install scikit-learn --no-use-pep517
> OPENBLAS=""$(brew --prefix openblas)"" MACOSX_DEPLOYMENT_TARGET=11.1 python3 -m pip install statsmodels --no-use-pep517
> ```

You would think so, and yeah, `pip` manages to compile sklearn correctly, but after *actually trying to use sklearn*, you'll get hit with a weird `[1]    94284 bus error  python3`:

```python3
 python3
Python 3.9.1 (default, Jan 27 2021, 22:29:40)
[Clang 12.0.0 (clang-1200.0.32.28)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import sklearn
[1]    94284 bus error  python3
```
The current downstream issue on sklearn [scikit-learn/scikit-learn#19097](https://github.com/scikit-learn/scikit-learn/issues/19097) is waiting for this one to be resolved (amongst other things), but thanks to the above instructions I already managed to install NumPy on the M1.

It's the end of the month and pyarrow still hasn't released a fix",2021-01-31T13:24:52Z
770383740,"Restarting CI, something weird happened.",2021-01-31T13:36:40Z
770385252,This looks like a mistake during doc build uploading for 1.20.0. @charris did you do that upload? ,2021-01-31T13:48:03Z
770385457,That makes very little sense. `__array__` is supposed to simply return an `ndarray` with either a view or a copy of the data.,2021-01-31T13:49:43Z
770386747,"> It's the end of the month and pyarrow still hasn't released a fix

`pyarrow` has been released beginning this week and NumPy 1.20 has also been released.",2021-01-31T13:59:34Z
770389008,"By ""that"" I assume you mean the current documentation that states:
```

class.__array__([dtype])

    If a class (ndarray subclass or not) having the __array__ method is used as the output object of an 
ufunc, results will not be written to the object returned by __array__. This practice will return TypeError.

```",2021-01-31T14:15:59Z
770389671,I convert the notes from rst to md for github using pandoc. The conversion often needs a little fixing up and I missed that. The big fixup needed for 1.20.0 was a table. It would be nice if the github window accepted rst files.,2021-01-31T14:20:26Z
770389723,I think all the pieces are in place to build a wheel. The question is if we wish to upload a wheel without testing it.,2021-01-31T14:20:42Z
770389950,"Maybe we can do a short delay, while we wait for some set of nominated volunteers to test the wheel, for now.",2021-01-31T14:22:35Z
770390962,"That explains it, thanks.",2021-01-31T14:29:38Z
770391077,Indeed.,2021-01-31T14:30:14Z
770392483,"> +1 the whole point of this new M1 mac is to do ML and numpy is pretty integral..

@vincentwi and others, if you want to use the numpy / scipy / scikit-learn stack in native mode on Apple M1 hardware, the only, semi-supported way at the moment, is to install the conda-forge packages using the miniforge installer as explained here:

https://scikit-learn.org/stable/install.html#installing-on-apple-silicon-m1-hardware

Alternatively, you should be able to use pip to install the existing macos-10_9 wheels from pypi.org in x86_64 environment (via the Rosetta 2 emulation layer of macOS 11) as explained here: https://github.com/numpy/numpy/issues/17807#issuecomment-763973916 . This should work, although the par

Generating native M1 wheels on pypi.org requires to setup a gfortran + C / C++ cross-compiling infrastructure (e.g. to build [openblas](https://github.com/MacPython/openblas-libs/pull/53) which is the tricky dependency for numpy / scipy) and coordination between several open source projects maintainers. Please be patient or just use conda-forge. It just works great and you get native M1 speed.

When I said ""semi-supported"", it's because we have no way to run the automated test suite on public continuous integration on M1 hardware at the moment  (github actions / azure pipelines / travis and co). I manually launch the scikit-learn tests on a regular basis on my M1 laptop and everything seems to work as expected so far but. But we have no way to automate the tests on that platform... \\_()_/",2021-01-31T14:39:59Z
770393442,"The main translation problem is folks using back ticks for links in the release notes. That works fine when building the docs, not so well with standalone release notes.",2021-01-31T14:47:11Z
770395723,"> did you do that upload?

Yes. The problem was that I was unable to build the docs before updating the release branch for further development. I thought that might be a problem but didn't see it after a quick check. I think the fix is to just check out the proper commit before the build and that was already on my list for today, the problem wasn't unexpected :)",2021-01-31T15:01:47Z
770401537,"The problem is that the documentation was built against my installed NumPy. I thought it was supposed to be built in its own environment. I can fix that, but it either needs fixing or to be in the notes.

EDIT: That is one reason I'd like to see documentation builds made part of runtests, it already sets up an appropriate environment.",2021-01-31T15:40:51Z
770403613,"> EDIT: That is one reason I'd like to see documentation builds made part of runtests, it already sets up an appropriate environment.

I'd like to build them in CI together with wheels, much more controlled. And same for the sdist, for SciPy we recently had an issue with that (https://github.com/scipy/scipy/issues/13369), that could easily happen for NumPy as well.",2021-01-31T15:55:57Z
770403897,"> The problem is that the documentation was built against my installed NumPy. I thought it was supposed to be built in its own environment. I can fix that, but it either needs fixing or to be in the notes.

`make dist` is indeed supposed to do that. Maybe you had `PYTHONPATH` defined that was overriding it or something.",2021-01-31T15:57:54Z
770404945,"> `make dist` is indeed supposed to do that. Maybe you had `PYTHONPATH` defined

IIRC, `make dist` has never worked that way for me. I don't have `PYTHONPATH` in my environment. OTOH, NumPy is installed in `~/.local/lib/python3.9/site-packages`, which might be a problem for some reason.",2021-01-31T16:04:59Z
770407493,"OK, sadly this doesn't seem to be working very well.
The Sphinx build on CircleCI timed out.

I guess I'll have to find a way to make fewer `git` calls.

For now, I'll close this PR.",2021-01-31T16:21:39Z
770408913,"This extension looks like a great idea. I wonder if by using GitPython you can avoid some overhead from repeated subprocess calls - I get the impression it might work directly with the git filesystem and/or cache commit information, although I may be mistaken.",2021-01-31T16:30:46Z
770410348,"Thanks for the suggestion!

AFAICT, GitPython also simply calls `git`:

https://github.com/gitpython-developers/GitPython/blob/3c19a6e1004bb8c116bfc7823477118490a2eef6/git/cmd.py#L173

https://github.com/gitpython-developers/GitPython/blob/3c19a6e1004bb8c116bfc7823477118490a2eef6/git/cmd.py#L998

https://github.com/gitpython-developers/GitPython/blob/3c19a6e1004bb8c116bfc7823477118490a2eef6/git/cmd.py#L724-L736

I don't know about caching though ...",2021-01-31T16:40:12Z
770410844,my understanding was that most of `GitPython` does not use the `git.cmd` module. There seems to be a lot of code that walks the git filesystem in other files in that repo.,2021-01-31T16:43:12Z
770412219,"Appears to have been fixed in master, 

https://github.com/numpy/numpy/blob/de8bb00dbf007b65095e858ff2490b8df39b9047/numpy/__init__.pyi#L316

but sadly not backported to the maintenance 1.20 branch. Okay, closing.

",2021-01-31T16:52:04Z
770421204,I was looking around to learn more about BLAS (still new to looking at low-level issues with numpy tbh). Found out that Apple has their own BLAS framework that might be usable. https://developer.apple.com/documentation/accelerate/blas,2021-01-31T17:50:51Z
770424103,"@zeuschops: The functions numpy and scipy use from Acclerate are buggy and Apple has not shown an interest engaging with us to fix them, so we blocklisted using it for numpy and scipy. ",2021-01-31T18:09:03Z
770425200,"Thanks for the hint!

I think I would want to use something like [Repo.iter_commits()](https://gitpython.readthedocs.io/en/stable/reference.html#git.repo.base.Repo.iter_commits) ...?

But this seems to call `repo.git.rev_list(rev, args, as_process=True, **kwargs)` (via `Commit.iter_items()`), which itself runs a new `subprocess`.

It looks like they obtain a list of SHAs from the `git` command, but then ""parse the actual commit information directly from our lighting fast object database"", see https://github.com/gitpython-developers/GitPython/blob/0c6e67013fd22840d6cd6cb1a22fcf52eecab530/git/objects/commit.py#L248-L277

AFAIU, this would still need a call to `git` for each file ...

I think a possible approach would be to collect a whole list of files in the Sphinx extension and then call `git log` once to get a customized list of commits and then manually parse the ""last changed"" information from this list of commits for the whole list of files in one go.",2021-01-31T18:15:10Z
770425788,"On that topic - I suppose that there may be a much larger performance difference for some BLAS routines in Accelerate, on M1, compared to openblas.   Has anyone checked for such differences?  Is there any way we can call into the Accelerate routines that are not buggy, and give good performance?",2021-01-31T18:18:18Z
770427477,"If a commercial vendor wants to get their software integrated into an open source project, I think it is incumbent upon that vendor to a) show an interest b) promise long term support with a channel for interacting with the community and c) do most of the work themselves or pay us to maintain it. I think that reflects our current relationship with Intel and MKL, why should Apple and Accelerate be any different?",2021-01-31T18:30:10Z
770429099,"Two reasons - Accelerate is more or less an operating system library - and I think that is different from something like MKL, where we have to ship the library ourselves.

And - I suspect the performance differences for MKL are in the order of 10s of percents, almost all the time, but for Accelerate on M1, in particular, the differences may be much larger.",2021-01-31T18:42:32Z
770431888,Thanks @Carreau ,2021-01-31T19:01:38Z
770432809,"As a supporting argument for Apple's BLAS, it could have potential to avoid this issue, though I'm not sure if the functions that are causing problems are present or not. Otherwise it may be worth looking into accelerate more on my end strictly for the purpose of Machine Learning - which is most of the reason that brings people with an M1 to numpy currently, afaik.

Also, accelerate got updated last year just before the M1s shipped because of ""desktop support"" for ARM64. From a quick glance, Microsoft once had a BLAS library - which would be the equivalent here - but I'd be willing to say that they probably switched to OpenBLAS.",2021-01-31T19:07:10Z
770436229,"@matthew-brett / @zeuschops - 

I'm sure the community would be supportive of you guys putting in the effort and energy to explore which numpy functions are compatible with Accelerate, then submitting a pull request that makes the appropriate adjustments to numpy code. 

As someone who has an M1 MBA, I'm looking forward to seeing your contributions in the coming weeks and months!
",2021-01-31T19:26:33Z
770436288,"@zeuschops If Accelerate had fixed the bugs, this issue (and the others about avoiding Accelerate when building from source on M1) would not be open., it simply would have worked out of the box. So if you are using Accelerate for machine learning, I hope you have a very good test suite to verify your results.",2021-01-31T19:26:56Z
770436955,"Well, once Apple fixes the issues that caused us to block them, the actual work should be small.  But it is not like we probably even *can* work around those issues reasonably (I did not review them, but they are here: https://github.com/scipy/scipy/wiki/Dropping-support-for-Accelerate).  We cannot sacrifice correct and stable executation for speed in these cases.

NumPy and SciPy have not taken the step of blocking Accelerate lightly, it only happened after years of unresolved issues IIRC.  If you want to use Accelerate, you can patch NumPy to not check for it, if you want to upstream some `NPY_IGNORE_THAT_ACCELERATE_IS_BUGGY` variable, I might be OK with that. But as Matti said, you better be very sure that your code is extra well tested on the M1 hardware, since it may give incorrect results for certain incarnations (e.g. float32 matrix multiplications).",2021-01-31T19:31:16Z
770437300,"Note that [in 1.10 at least](https://docs.scipy.org/doc/numpy-1.10.1/reference/arrays.classes.html), the description is different:
```
If a class (ndarray subclass or not) having the __array__ method is used as the output object of an ufunc, results will be written to the object returned by __array__. Similar conversion is done on input arrays.
```

Checking with `git blame`, the change was made last May, in #16130.",2021-01-31T19:33:08Z
770438184,"I should say that there is some interest in getting this working, so hopefully these problems will soon not be relevant anymore.",2021-01-31T19:38:49Z
770438755,"@bstaletic Yes, NumPy leaks memory on import, but only once.  Can you explain if this is a problem for you? I do check numpy with valgrind, but there are certain things during Python and NumPy import that simply are designed in a way that they must ""leak"" (exactly once).",2021-01-31T19:43:03Z
770439514,"I'm not sure if the change in gh-16130 is correct or not, but either way that's a very niche detail. The docstring doesn't explain at all what `__array__` is actually for.

It is one of the protocols with which other array-like object can achieve interoperability with numpy. When numpy functions encounter a foreign object, it will try (in order):

1. the buffer protocol
2. `__array_interface__`
3. `__array__`

in order to turn the object into an `ndarray`. For (1) and (2) the object describes its memory layout and numpy does everything else (zero-copy if possible). For (3) the object itself is responsible for returning an `ndarray` from `__array__()`.",2021-01-31T19:48:12Z
770440980,"It's not exactly a problem, as pybind11 can keep suppressing the valgrind-reported errors. I assumed you weren't aware of these and thus opened the bug report.
What *actually* lead me to open the bug report was https://github.com/pybind/pybind11/pull/2837. By switching to the latest version of NumPy, pybind11 ran into a new leak. This one isn't shown in the gist, but the details are all in that PR. I'm not sure where is it coming from, but this is the new stack trace that valgrind reports.

```
==12516== 80 bytes in 1 blocks are indirectly lost in loss record 2,861 of 4,401
==12516==    at 0x4C31ECB: malloc (vg_replace_malloc.c:307)
==12516==    by 0x467566: _PyMem_RawMalloc (obmalloc.c:99)
==12516==    by 0x468168: PyObject_Malloc (obmalloc.c:685)
==12516==    by 0x52CE7B: _PyObject_GC_Alloc (gcmodule.c:2225)
==12516==    by 0x52CE47: _PyObject_GC_Malloc (gcmodule.c:2252)
==12516==    by 0x52CFB3: _PyObject_GC_NewVar (gcmodule.c:2281)
==12516==    by 0x473BD1: tuple_alloc (tupleobject.c:92)
==12516==    by 0x473CA4: _PyTuple_FromArray (tupleobject.c:420)
==12516==    by 0x42DE4D: _PyObject_MakeTpCall (call.c:165)
==12516==    by 0x42EDBC: _PyObject_VectorcallTstate (abstract.h:116)
==12516==    by 0x42EDBC: _PyObject_CallFunctionVa (call.c:542)
==12516==    by 0x42FA4F: PyObject_CallFunction (call.c:564)
==12516==    by 0x4F648C: PyImport_Import (import.c:2073)
```

To be clear, if you say this is fine and expected, I'm fine with that.",2021-01-31T19:57:50Z
770442748,"There were always some leaks that seemed fine, but now I notice that all of these are in Cython code generated for numpy random (and not in NumPy proper).  To be honest, I stopped checking import time leaks, but I suppose there are fewer nowadays (at least few ""definite"" ones that valgrind notices, there are definitely more leaks :)).

I think you are right, we actually should see if Cython generates incorrect reference counting or so.

That last one could be a serious refcounting bug somewhere.  I had run valgrind over NumPy 1.20 a while ago, but there were a few changes after that, and the refcount check was a bit longer, I guess.  Should plan to do more full runs again soon before 1.20.1, just to be sure nothing happened in the mean time.

Can you give more information on that leak (e.g. what code is run when it is triggered?).  Tracking an incorrectly refcounted tuple seems very hard otherwise (unless I get lucky and the NumPy test suit finds it as well).",2021-01-31T20:09:55Z
770443627,"@bstaletic one small thing, if you this inside one of the many tests. Just in case it helps my little pytest plugin: https://pypistats.org/packages/pytest-valgrind could help with finding which test is failing exactly. (The github repo is in a slightly newer state, but it shouldn't matter much.)",2021-01-31T20:16:25Z
770446630,"Thanks for looking into this, @seberg!

To add to @bstaletic's story: we managed to run Valgrind on pybind11 and clean pybind11 of all reported leaks, a month ago, and suppressed the leaks reported on `import numpy` as ""these aren't ours"". But now our tests failed after updating NumPy (because of the one extra leak), and we though we should finally take the time to report upstream.

Here's how I reproduced the new leak that @bstaletic reported: `PYTHONMALLOC=malloc valgrind --leak-check=full --show-leak-kinds=definite,indirect --suppressions=pybind11/tests/valgrind-python.supp --gen-suppressions=all python3.9-dbg -c ""import numpy, sys; print(numpy.__version__); print(sys.version); print(sys.abiflags)""` 
Full output here: [numpy-leaks-by-valgrind.txt](https://github.com/numpy/numpy/files/5900585/numpy-leaks-by-valgrind.txt)

Suppressing the error with [our current suppression file](https://github.com/pybind/pybind11/blob/932769b03855d1b18694e7a867bbd1c24ff6170e/tests/valgrind-numpy-scipy.supp), this one remains:

```
$ PYTHONMALLOC=malloc valgrind --leak-check=full --show-leak-kinds=definite,indirect --suppressions=pybind11/tests/valgrind-python.supp --suppressions=pybind11/tests/valgrind-numpy-scipy.supp --gen-suppressions=all python3.9-dbg -c ""import numpy, sys; print(numpy.__version__); print(sys.version); print(sys.abiflags)""
==12620== Memcheck, a memory error detector
==12620== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==12620== Using Valgrind-3.16.1 and LibVEX; rerun with -h for copyright info
==12620== Command: python3.9-dbg -c import\ numpy,\ sys;\ print(numpy.__version__);\ print(sys.version);\ print(sys.abiflags)
==12620== 
1.20.0
3.9.1 (default, Dec  8 2020, 03:24:52) 
[GCC 7.5.0]
d
==12620== 
==12620== HEAP SUMMARY:
==12620==     in use at exit: 3,340,248 bytes in 20,455 blocks
==12620==   total heap usage: 316,728 allocs, 296,273 frees, 47,691,463 bytes allocated
==12620== 
==12620== 80 bytes in 1 blocks are indirectly lost in loss record 2,861 of 4,401
==12620==    at 0x4C31ECB: malloc (vg_replace_malloc.c:307)
==12620==    by 0x467566: _PyMem_RawMalloc (obmalloc.c:99)
==12620==    by 0x468168: PyObject_Malloc (obmalloc.c:685)
==12620==    by 0x52CE7B: _PyObject_GC_Alloc (gcmodule.c:2225)
==12620==    by 0x52CE47: _PyObject_GC_Malloc (gcmodule.c:2252)
==12620==    by 0x52CFB3: _PyObject_GC_NewVar (gcmodule.c:2281)
==12620==    by 0x473BD1: tuple_alloc (tupleobject.c:92)
==12620==    by 0x473CA4: _PyTuple_FromArray (tupleobject.c:420)
==12620==    by 0x42DE4D: _PyObject_MakeTpCall (call.c:165)
==12620==    by 0x42EDBC: _PyObject_VectorcallTstate (abstract.h:116)
==12620==    by 0x42EDBC: _PyObject_CallFunctionVa (call.c:542)
==12620==    by 0x42FA4F: PyObject_CallFunction (call.c:564)
==12620==    by 0x4F648C: PyImport_Import (import.c:2073)
==12620== 
{
   <insert_a_suppression_name_here>
   Memcheck:Leak
   match-leak-kinds: indirect
   fun:malloc
   fun:_PyMem_RawMalloc
   fun:PyObject_Malloc
   fun:_PyObject_GC_Alloc
   fun:_PyObject_GC_Malloc
   fun:_PyObject_GC_NewVar
   fun:tuple_alloc
   fun:_PyTuple_FromArray
   fun:_PyObject_MakeTpCall
   fun:_PyObject_VectorcallTstate
   fun:_PyObject_CallFunctionVa
   fun:PyObject_CallFunction
   fun:PyImport_Import
}
==12620== LEAK SUMMARY:
==12620==    definitely lost: 0 bytes in 0 blocks
==12620==    indirectly lost: 80 bytes in 1 blocks
==12620==      possibly lost: 2,854,181 bytes in 17,746 blocks
==12620==    still reachable: 377,931 bytes in 1,131 blocks
==12620==         suppressed: 108,056 bytes in 1,577 blocks
==12620== Reachable blocks (those to which a pointer was found) are not shown.
==12620== To see them, rerun with: --leak-check=full --show-leak-kinds=all
==12620== 
==12620== For lists of detected and suppressed errors, rerun with: -s
==12620== ERROR SUMMARY: 4007 errors from 4007 contexts (suppressed: 189 from 189)
```",2021-01-31T20:37:33Z
770447258,"Oh, I should specify one more thing, perhaps: we were holding back on having more general rules (e.g., just the stack up to `fun:tuple_alloc`), because there doesn't seem to be a reference to NumPy in there, and we didn't want to potentially mask leaks not caused by NumPy.

Apart from that, we're perfectly happy. The main goal is to know what to suppress such that we can have Valgrind run in CI for pybind11.",2021-01-31T20:41:45Z
770447931,"Honestly, I don't use any suppression files at all, so :). The only reason we don't have a CI run in NumPy is that there are some tests that always have to leak, and running most of the test suit with `pytest-valgrind` at least (its slower, because it checks for leaks after every tests and not just at exist), takes about 24 hours...

Its great that you guys do these checks!  I am a little stumped, working of master, but:
```
sebastian@seberg-x1 ~/forks/numpy-master
 % PYTHONMALLOC=malloc valgrind --leak-check=full --show-leak-kinds=definite python3.9-dbg runtests.py --python
==14334== Memcheck, a memory error detector
==14334== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==14334== Using Valgrind-3.16.1 and LibVEX; rerun with -h for copyright info
==14334== Command: python3.9-dbg runtests.py --python
==14334== 
Building, see build.log...
==14335== Warning: invalid file descriptor 1024 in syscall close()
==14335== Warning: invalid file descriptor 1025 in syscall close()
==14335== Warning: invalid file descriptor 1026 in syscall close()
==14335== Warning: invalid file descriptor 1027 in syscall close()
==14335==    Use --log-fd=<number> to select an alternative log fd.
==14335== Warning: invalid file descriptor 1028 in syscall close()
==14335== Warning: invalid file descriptor 1029 in syscall close()
Build OK
Enabling display of all warnings
Python 3.9.1+ (default, Jan 20 2021, 14:49:22) 
[GCC 10.2.1 20210110] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
(InteractiveConsole)
>>> import numpy, sys; print(numpy.__version__); print(sys.version); print(sys.abiflags)
1.21.0.dev0+586.gf55f5d455
3.9.1+ (default, Jan 20 2021, 14:49:22) 
[GCC 10.2.1 20210110]
d
>>> 
now exiting InteractiveConsole...
==14334== 
==14334== HEAP SUMMARY:
==14334==     in use at exit: 3,570,926 bytes in 20,808 blocks
==14334==   total heap usage: 374,075 allocs, 353,267 frees, 55,757,282 bytes allocated
==14334== 
==14334== LEAK SUMMARY:
==14334==    definitely lost: 0 bytes in 0 blocks
==14334==    indirectly lost: 0 bytes in 0 blocks
==14334==      possibly lost: 2,974,833 bytes in 19,418 blocks
==14334==    still reachable: 596,093 bytes in 1,390 blocks
==14334==         suppressed: 0 bytes in 0 blocks
==14334== Reachable blocks (those to which a pointer was found) are not shown.
==14334== To see them, rerun with: --leak-check=full --show-leak-kinds=all
==14334== 
==14334== For lists of detected and suppressed errors, rerun with: -s
==14334== ERROR SUMMARY: 4228 errors from 4228 contexts (suppressed: 0 from 0)
```
Although let me try without the interactive shell... Maybe that does something weird.",2021-01-31T20:45:45Z
770448257,"Yeah, same running from the shell directly; no reported leaks with just import (and version printing). Maybe the cython version matters?
```
>>> import cython
>>> cython.__version__
'0.29.21'
```",2021-01-31T20:48:01Z
770448433,"> one small thing, if you this inside one of the many tests. Just in case it helps my little pytest plugin: https://pypistats.org/packages/pytest-valgrind could help with finding which test is failing exactly. 

We have actually stumbled across that plugin when we were first setting valgrind up with pybind11. It looked interesting, but so far we decided not to rely on it, because of the inability to report errors that happen at start up or shut down. It would have had certainly come in handy a few times by now, but habits, I guess. I'm used to just commenting out a bunch of tests until I pin point where the error is coming from, which is the caveman method.",2021-01-31T20:49:27Z
770448965,"I did that first, when cleaning out NumPy: manual bisecting (and in the end, once you hit the individual test level, it is often necessary in any case). Then I wrote that, thinking it is a bit better ;).
The plugin just flushes all initial errors right now, I happy to change that if it helps you. Just an offer if you think it might be useful. Aside from narrowing down tests, it doesn't actually do much.",2021-01-31T20:53:03Z
770449489,"> Yeah, same running from the shell directly; no reported leaks with just import (and version printing). Maybe the cython version matters?
> 
> ```
> >>> import cython
> >>> cython.__version__
> '0.29.21'
> ```

```
$ python3.9-dbg
Python 3.9.1 (default, Dec  8 2020, 03:24:52) 
[GCC 7.5.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import cython
>>> cython.__version__
'0.26.1'
```

That seems to be the same?

Besides, I believe I really just installed the wheel from PyPI. I'm also using deadsnakes' `python3.9-dbg` (which I suppose has a bunch of Debian patches), and so is our CI. Can that be a factor playing into this?",2021-01-31T20:56:53Z
770449871,"> The plugin just flushes all initial errors right now, I happy to change that if it helps you. Just an offer if you think it might be useful. Aside from narrowing down tests, it doesn't actually do much.

I must admit I was intrigued by the plugin :-) Given that we're now running on CI, it should be easy enough to spot the test that changed and (probably) causes the leak, but I'd be happy to give that a try!

But yes, as @bstaletic says, pybind11 has a bunch of internals so we should definitely check the first import of a pybind11 module (outside of a `pytest` test function, I'm afraid) and the final cleanup on interpreter shutdown.",2021-01-31T20:59:48Z
770450704,"> Besides, I believe I really just installed the wheel from PyPI. I'm also using deadsnakes' python3.9-dbg (which I suppose has a bunch of Debian patches), and so is our CI. Can that be a factor playing into this?

I can repro with my custom built, non-patched, CPython.

1. `python -m venv v`
2. `. ./v/bin/activate`
3. `which pip` - just to confirm everything's as expected. Virtual environments make me paranoid, after staring at CI so much.
4. `PYTHONMALLOC=malloc valgrind --leak-check=full --show-leak-kinds=definite,indirect python -c import\ numpy`
5. `deactivate`

Here's the summary:

```
==6236== LEAK SUMMARY:
==6236==    definitely lost: 1,936 bytes in 19 blocks
==6236==    indirectly lost: 520 bytes in 8 blocks
==6236==      possibly lost: 2,958,818 bytes in 19,291 blocks
==6236==    still reachable: 404,409 bytes in 1,201 blocks
==6236==         suppressed: 0 bytes in 0 blocks
==6236== Reachable blocks (those to which a pointer was found) are not shown.
```

That includes the new `PyImport_Import` one.",2021-01-31T21:04:56Z
770451160,"Thanks @bstaletic, I can reproduce it with the wheel, did not yet try to check the 1.20.x branch, but master did not diverge a whole lot.
@mattip what cython version did we use for the wheel build? I still feel the cython modules are the most likely cause.",2021-01-31T21:08:19Z
770453011,"Nevermind, I *can* reproduce the leak on the 1.20 branch, just not on master branch (I am super surprised, because I would think the cython generation is pretty much unchanged; so maybe have to triple check).

Note sure what relevant change happened in between, but at least I can reproduce it now. I also doubt it is a big issue (since it is on import only probably). But I would like it if you guys wouldn't even need a numpy suppression file :).",2021-01-31T21:21:17Z
770453336,"> But I would like it if you guys wouldn't even need a numpy suppression file :).

That would be amazing! But if you just tell us ""these are the leaks you need to live with and you don't need to report similar ones in the future"", we're also totally OK with that :-)",2021-01-31T21:23:35Z
770454533,"@YannickJadoul, OK: I am very certain, there is no need to worry about them. But I suspect they will go away again (whether due to a cython upgrade or otherwise).

For now, I have figured out that if I compile NumPy with a debug version (which sets some debug paths also in NumPy itself), it doesn't show up. But if I compile with plain `python3.9` it does show up.
So the ""difference"" between master and 1.20, was actually just me compiling with `python3.9-dbg runtests.py` before. I have tried with the Cython 3 pre-release, and it doesn't make a difference.

For today, I will stop looking into this. I hope there is some small, simple fix in Cython that would make this go away, so will look into it a bit more in the next days and hopefully find it.  As this appears in module init, I am not worried that it is problematic.",2021-01-31T21:33:14Z
770454775,"> 
> 
> @DWesl, we use the term `FMA3` instead of `FMA` to avoid any confusion with `AMD/FMA4`.
> 
> @rgommers,
> 
> > any advice here on how to debug?
> 
>     * Testing CPU detecting mechanism -> `python runtests.py -t numpy/core/tests/test_cpu_features.py`
>       NOTE: You will have to patch `test_cpu_features.py` to enables `Cygwin`.
> 
```bash
python runtests.py -t numpy/core/tests/test_cpu_features.py
Building, see build.log...
Build OK
NumPy version 1.21.0.dev0+393.g0386777c3
NumPy relaxed strides checking option: True
NumPy CPU features: SSE SSE2 SSE3 SSSE3* SSE41* POPCNT* SSE42* AVX* F16C* FMA3* AVX2* AVX512F? AVX512CD? AVX512_KNL? AVX512_KNM? AVX512_SKX? AVX512_CLX? AVX512_CNL? AVX512_ICL?
.ss                                                            [100%]
1 passed, 2 skipped in 0.08s
```

>     * Testing `_SIMD` module itself without any dispatched features:
>       
>       * `python runtests.py --cpu-dispatch=""none"" -t numpy/core/tests/test_simd_module.py`

```bash
python runtests.py --cpu-dispatch=""none"" -t numpy/core/tests/test_simd_module.py
Building, see build.log...
    ... build in progress
Build OK
NumPy version 1.21.0.dev0+393.g0386777c3
NumPy relaxed strides checking option: True
NumPy CPU features: SSE SSE2 SSE3
.........................s..........                                   [100%]
35 passed, 1 skipped in 0.43s
```

>       * `python runtests.py --cpu-dispatch=""none"" -t numpy/core/tests/test_simd.py`

```bash
python runtests.py --cpu-dispatch=""none"" -t numpy/core/tests/test_simd.py
Building, see build.log...
Build OK
NumPy version 1.21.0.dev0+393.g0386777c3
NumPy relaxed strides checking option: True
NumPy CPU features: SSE SSE2 SSE3
.......................................................................................................................................................................................................... [ 84%]
....................................                                                                                                                                                                       [100%]
238 passed in 1.89s
```
> 
>     * Testing `_SIMD` module with certain dispatched features, e.g. `sse41`
>       `python runtests.py --simd-test=""baseline sse41"" -t numpy/core/tests/test_simd.py`

```bash
python runtests.py --simd-test=""baseline sse41"" -t numpy/core/tests/test_simd.py
Building, see build.log...
    ... build in progress
    ... build in progress
Build OK
NumPy version 1.21.0.dev0+393.g0386777c3
NumPy relaxed strides checking option: True
NumPy CPU features: SSE SSE2 SSE3 SSSE3* SSE41* POPCNT* SSE42* AVX* F16C* FMA3* AVX2* AVX512F? AVX512CD? AVX512_KNL? AVX512_KNM? AVX512_SKX? AVX512_CLX? AVX512_CNL? AVX512_ICL?
........................................................................ [ 15%]
........................................................................ [ 30%]
........................................................................ [ 45%]
........................................................................ [ 60%]
........................................................................ [ 75%]
........................................................................ [ 90%]
............................................                             [100%]
476 passed in 7.31s

$ python runtests.py --simd-test=""baseline sse41 sse42 avx avx2 fma"" -t numpy/core/tests/test_simd.py
Building, see build.log...
    ... build in progress
    ... build in progress
Build OK
NumPy version 1.21.0.dev0+393.g0386777c3
NumPy relaxed strides checking option: True
NumPy CPU features: SSE SSE2 SSE3 SSSE3* SSE41* POPCNT* SSE42* AVX* F16C* FMA3* AVX2* AVX512F? AVX512CD? AVX512_KNL? AVX512_KNM? AVX512_SKX? AVX512_CLX? AVX512_CNL? AVX512_ICL?
........................................................................ [ 15%]
........................................................................ [ 30%]
........................................................................ [ 45%]
........................................................................ [ 60%]
........................................................................ [ 75%]
........................................................................ [ 90%]
............................................                             [100%]
476 passed in 7.40s

$ python runtests.py -t numpy/core/tests/test_simd.py
Building, see build.log...
Build OK
NumPy version 1.21.0.dev0+393.g0386777c3
NumPy relaxed strides checking option: True
NumPy CPU features: SSE SSE2 SSE3 SSSE3* SSE41* POPCNT* SSE42* AVX* F16C* FMA3* AVX2* AVX512F? AVX512CD? AVX512_KNL? AVX512_KNM? AVX512_SKX? AVX512_CLX? AVX512_CNL? AVX512_ICL?
........................................................................ [ 15%]
........................................................................ [ 30%]
........................................................................ [ 45%]
........................................................................ [ 60%]
........................................................................ [ 75%]
........................................................................ [ 90%]
............................................                             [100%]
476 passed in 3.55s
```
> 
>     * Checking the build log, and finally GDB or LLDB

After following the above steps, I can run the whole testsuite with no segfaults.  If I run `git clean -xdf` first, I get segfaults.  I have no idea why, but at least I have a workaround now.",2021-01-31T21:35:05Z
770455398,"Thanks for confirming. We'll stick to the current suppression files, then, and hopefully in some future, we try without and find a beautiful world without suppression files ;-)",2021-01-31T21:39:56Z
770455431,"Thanks for all your effort, @seberg!!",2021-01-31T21:40:13Z
770459019,"Can you update the annotations in [arrayprint.pyi](https://github.com/numpy/numpy/blob/master/numpy/core/arrayprint.pyi) as well?

In total two changes will be necessary:

1. Import the `SupportsIndex` protocol right [here](https://github.com/numpy/numpy/blob/f55f5d45518c2f83271330041b982e55dcd6bbf9/numpy/core/arrayprint.pyi#L26-L29):
``` python
if sys.version_info > (3, 8):
    from typing import SupportsIndex
else:
    from typing_extensions import SupportsIndex
```

2. Replace `int` with `SupportsIndex` (for the precision parameter) in the following functions:
* [`set_printoptions`](https://github.com/numpy/numpy/blob/f55f5d45518c2f83271330041b982e55dcd6bbf9/numpy/core/arrayprint.pyi#L65)
* [`array2string`](https://github.com/numpy/numpy/blob/f55f5d45518c2f83271330041b982e55dcd6bbf9/numpy/core/arrayprint.pyi#L82)
* [`array_repr`](https://github.com/numpy/numpy/blob/f55f5d45518c2f83271330041b982e55dcd6bbf9/numpy/core/arrayprint.pyi#L120)
* [`array_str`](https://github.com/numpy/numpy/blob/f55f5d45518c2f83271330041b982e55dcd6bbf9/numpy/core/arrayprint.pyi#L126)
* [`printoptions`](https://github.com/numpy/numpy/blob/f55f5d45518c2f83271330041b982e55dcd6bbf9/numpy/core/arrayprint.pyi#L133)",2021-01-31T22:05:22Z
770459500,"According to my quick tests, OpenBLAS with the VORTEX target is still quite competitive vs Accelerate BLAS.

I have been benchmarking an experimental combo of Netlib's LAPACK + Accelerate's BLAS that @isuruf is working on for conda-forge. This using  [isuruf/vecLibFort](https://github.com/isuruf/vecLibFort).

https://twitter.com/ogrisel/status/1355998987107487748

When I use threadpoolctl to limit the number of threads for OpenBLAS to 4 (instead of 8 by default) then a big DGEMM with OpenBLAS is barely slower than with Accelerate.

Note that with 8 threads, OpenBLAS is suffering from a bit of over-subscription because the 8 cores of the Apple M1 chip are not homogeneous (4 performance cores + 4 efficiency cores).

",2021-01-31T22:09:05Z
770476832,"I did some more benchmarks, and for float32, SGEMM from Accelerate is really impressive ~2s for `(1024, 1024)`-shaped arrays. That's **2.7x faster than OpenBLAS with 4 threads**. This is close to 1 TFLOP/s in single precision on a machine without a mechanical fan...

I also benchmarked the experimental numpy and tensorflow wheels from Apple that also rely on the macOS libraries (Accelerate / MLCompute):

https://gist.github.com/ogrisel/87dcf2c3ab8a304ededf75934b116b61#gistcomment-3614885

- tensorflow is slightly faster than numpy but not by a large margin.
- for float64, OpenBLAS is still competitive (when using 4 threads instead of 8 by default)

Note that the numpy wheel from the Apple tensorflow fork repo has many failing tests and even segfaults when running the tests.

@isuruf's accelerate for BLAS + netlib for LAPACK combo only has a few failures (probably not related to BLAS/LAPACK):

https://gist.github.com/isuruf/aaead06e2b1d801a20910bc44a9ffcdf#gistcomment-3614830
",2021-02-01T00:03:51Z
770477373,"OK, I had another look.  The reason that I couldn't reproduce it at first is that it is related to cython's `CYTHON_COMPILING_IN_LIMITED_API` (I assume that the debug build disables it  pypy build would also), not compiling it with the that flag, will ensure that valgrind cannot consider it ""definitely lost"" because the module is never ""cleaned up"" and a pointer will be kept until the end.
On the up-side: That proofs that the leak is really just one tuple, and it doesn't really matter much.

Now I find it a bit strange that is only one tuple is lost per file, and I do not really see where there might be a `DECREF` missing (unless we actually have an import error that gets eaten later).  I am a bit wondering if Pythons tuple free-list could be involved, but I expect it should be cleared out correctly (and even if not, should probably not show up as lost memory in valgrind.)",2021-02-01T00:07:10Z
770481663,"@ogrisel to play devils advocate here, but is the result correct? Issues with float32 dot products where quite common bug reports when users picked up accelerate (intentionally or not).",2021-02-01T00:25:32Z
770515118,"I agree with the others: using weights = 1/sigma disagrees with the standard convention that the ""weights"" for gaussian uncertainties are 1/sigma2, see for example

https://en.wikipedia.org/wiki/Weighted_least_squares

The implementation in numpy is not wrong though, it corresponds to this comment in Wikipedia: ""When the errors are uncorrelated, it is convenient to simplify the calculations to factor the weight matrix as w_ii = W_ii. The normal equations can then be written in the same form as ordinary least squares.""

Still, there is at least a documentation error that must be corrected: 'w' is defined as the ""weights"" and users are told to use 1/sigma for Gaussian errors in the 'w' parameter comment, but are then told ""the weights are 1/sigma**2"" for the same situation in the 'cov' parameter comment. The user doesn't know whether to send in w=1/sigma or w=1/sigma^2! I had to read the source code in detail and still wasn't confident I had it right until I tried a test case that w = 1/sigma gives an accurate solution for Gaussian errors. This should not be necessary for users!!

In this sense, this is really just a documentation issue: 'w' should not be called the ""weights"" but the ""square root of the weights,"" keeping the example that for Gaussian errors one would supply 'w'=1/sigma. I recommend in that case modifying the 'cov' documentation to state something like ""the weights are w^2 = 1/sigma**2"" to keep clear the relationship between polyfit's 'w' and the standard usage of the term ""weights"" in discussing weighted linear least squares regression.

It's reasonable to keep 'w' as an argument if polyfit will only ever support uncorrelated uncertainties in y values since fewer computations are required if the user supplies that, so if they have w they don't have to convert it into something else just for polyfit to convert it back again.

That being said, I support instead deprecating w and replacing it with an alternative parameter with a clearer definition like ""sigma"", ""weights"", or actually I propose that ""var"" for ""variance in the y values"" would be event better, for the following reasons:
- uncertainty computations are most simply expressed and efficiently performed in terms of variances
- variance also linguistically accommodates non-Gaussian alternatives, like Poisson errors, where one would just send in 'var'=y
- variance would also allow to incorporate not just individual y variances but also their covariances: the user would send in 'var' = the covariance matrix of the y values, and the algorithm would detect this case by the dimensional difference between y and var, and compute the inverse of var and use that as W rather than just computing w = 1/var. For that, the algorithm would require a (very minor) refactoring to work with W instead of w.",2021-02-01T02:06:09Z
770517277,"Having the `.local` install wasn't a problem, the newly compiled numpy version was properly installed, but could not be found. So this seems like a path problem. I just installed the 1.20.0 release in `.local`  and everything was OK except for an extra compile step.",2021-02-01T02:14:20Z
770529528,Thanks @Carreau .,2021-02-01T02:55:21Z
770678522,"It's possible, it just needs special-casing. ",2021-02-01T08:39:42Z
770739325,Any updates here @seiko2plus @Qiyu8 ? I can update [PR 18183](https://github.com/numpy/numpy/pull/18183) once this gets merged.,2021-02-01T10:10:38Z
770875892,Thanks @cgohlke .,2021-02-01T13:58:19Z
770882325,Thanks @Mitchell-Faas .,2021-02-01T14:07:06Z
770883048,Closed by #18263.,2021-02-01T14:08:10Z
770887229,"Most of the numpy tests pass for the ""Accelerate BLAS + netlib LAPACK combo"". 

- detailed test results for numpy 1.20 with  ""Accelerate BLAS + netlib LAPACK combo"": https://gist.github.com/isuruf/aaead06e2b1d801a20910bc44a9ffcdf#gistcomment-3614821
- detailed test results for numpy 1.20 with openblas from conda-forge: https://gist.github.com/ogrisel/be1fd7f9d00f32f3c702a10a9886239d

The same tests fail on M1 for default numpy + openblas install from conda-forge.  So those failures are unlikely to stem from the use of Accelerate/vecLib.

All the scikit-learn tests pass (but arguably we do not test all our algorithms with float32). We have too failures with numpy 1.20 but they are unrelated to macos/arm64.

I am not what would be the numpy test results if we were to use LAPACK from Accelerate/vecLib (instead of the slow netlib) in addition to using it for BLAS.  I guess @isuruf will tell us soon (no pressure ;)

The experimental numpy wheel from [apple/tensorflow_macos](https://github.com/apple/tensorflow_macos) has test many failures and segfaults when running `numpy.test()`. Not sure why, the apple devs only provide the wheel, not the build instructions to reproduce the build.

More benchmarks results here: https://github.com/xianyi/OpenBLAS/issues/2814#issuecomment-752786856",2021-02-01T14:13:59Z
770895553,"I had previously tried to build a patched numpy against Accelerate for both BLAS and LAPACK using this: https://github.com/numpy/numpy/compare/master...ogrisel:accelerate-arm64 (before being able to test the Accelerate BLAS + netlib combo obtained from @isuruf [gist](https://gist.github.com/isuruf/aaead06e2b1d801a20910bc44a9ffcdf)).

With my patched numpy with Accelerate BLAS+LAPACK I had many tests failing with bad numerical values, including the polyfit check at numpy import time. But maybe something was wrong in my build config. In particular, my patched numpy build did not use [isuruf/vecLibFort](https://github.com/isuruf/vecLibFort).",2021-02-01T14:25:45Z
770902767,"Ok, I'll try to make a PR today if possible--to me it makes sense not to impose the change downstream for the special case if not broken/needed.",2021-02-01T14:36:11Z
770920234,"Can you list your dependencies? One that comes to mind is numba, which is only just merging my 5-month-old PR to fix these warnings in their code (https://github.com/numba/numba/pull/6176).",2021-02-01T15:01:08Z
770924179,"Sure: numpy, pyproj, h5py, pykdree, netCDF4, libxml2, pint (no numba)",2021-02-01T15:06:56Z
770924758,"Can you run with warnings as errors, to get the full traceback?",2021-02-01T15:07:45Z
770925453,"I can do that this evening, I'll update here with the results ",2021-02-01T15:08:48Z
770929817,"Memoryviews can be detected and converted to `ndarray`s with `asarray`, then passed through the rest of the function just fine.",2021-02-01T15:14:25Z
770974866,"I wonder if we should instead change the `MutableSequence` check to `Sequence`? That would allow that path to automatically work for `memoryview`, and would avoid spurious warnings that precede errors when readonly sequences like `tuple`.",2021-02-01T16:17:53Z
770981517,"A missing object in the stub files is arguably ground for a backport, especially since 
these are rather simple and straightforward annotations.

Are there objections against backporting these 3 lines to 1.20.1? https://github.com/numpy/numpy/blob/d075ba2ce1579138e0fd53a611f88ad18003bbdc/numpy/__init__.pyi#L315-L317
",2021-02-01T16:27:25Z
770982543,"> There's certainly some contention that me doing this downstream was sub-optimal anyway, but I don't think there's any claim that it should be disallowed/considered a bug to shuffle a memoryview.

That was specific to that particular use of memoryviews post-shuffle, not the shuffling per se. This is a good change.",2021-02-01T16:29:00Z
770985299,I don't have a problem with backporting this.,2021-02-01T16:33:04Z
770987048,@BvB93 Want to make a PR?,2021-02-01T16:35:50Z
770989538,"Sure, I'll make a PR a bit later this week.
Before that, I'm going to comb through the stub files once more though, to check if we missed any similar cases.",2021-02-01T16:39:29Z
770993304,"Both changes seem fine to me (`isinstance` check or Sequence).  Since a non-mutable sequence should error already, I somewhat doubt that modifying that as Eric suggested has any downsides. Presumably, any of the tensor frameworks will either be both or neither.  So I think Eric's suggestion is probably good.

I am not quite sure that the memoryview change can't go wrong in principle (memoryviews do support suboffsets for example), but I am willing to take the risk of such an unlikely backcompat break (it would go to an error), it probably is nicer for memoryviews. Although, I doubt there is a reason to optimize for them here.",2021-02-01T16:44:57Z
770995411,"Tested accelerate with Swift, and by far I'm noticing a drastic performance improvement. Trained Apple's test network (just 0-9 as a binary image) with 798 epochs with automatic tuning parameters - since that's a thing in their own test code which is neat imho - in less than a second. Images were 20x20.

I will also start looking into a resolve, but I'm new to numpy's repo so it will likely take me longer than the rest to identify a potential fix. Will start with testing the accelerate+arm64 branch mentioned above and work out using the error given during installation to narrow down what's going on. 


**Testing today**:
Seems `arch -x86_64 python3 -m pip install [library]` seems to avoid the installation issues again with Python 3.8 (at /usr/bin/python3). Looking into the linked BLAS/LAPACK content, the old stuff builds but external installs still fail because they keep pulling from pip/pypi builds of numpy. Perhaps this is an issue on external repositories and not on NumPy's end? Also, I could only find one reference of altivec.h as an import. Going to look more into that as well to identify what's going on there.",2021-02-01T16:47:58Z
771046076,"I have convinced myself there is nothing actionable in NumPy, so opened a cython issue: https://github.com/cython/cython/issues/3994 just in case.  My hope is that with some new Python or Cython version, this just goes away. I feel this has to do with module cleanup, and I am not even certain Python *itself* is cleaned up in that regard yet.",2021-02-01T18:05:31Z
771050712,"@seberg Thanks for investigating!

> I feel this has to do with module cleanup, and I am not even certain Python itself is cleaned up in that regard yet.

CPython definitely isn't clean, but there are efforts to clean it up in some future version. Maybe one day...",2021-02-01T18:12:18Z
771051741,"Thanks again, @seberg!",2021-02-01T18:13:43Z
771078424,Thanks Bas.,2021-02-01T18:53:55Z
771080643,Thanks Bas.,2021-02-01T18:57:41Z
771082742,There is also a `float96` that I don't see anywhere. It is extended precision (long double) on 32 bit linux platforms.,2021-02-01T19:01:24Z
771083322,"Hmmpf, `allclose` isn't really very useful for datetimes probably, but breaking this was not intentional really.

The reason is that previous versions of NumPy were ill defined:
```
In [31]: np.promote_types(""m8[ns]"", np.float64)
TypeError: invalid type promotion

In [32]: np.promote_types(np.float64, ""m8[ns]"")
Out[32]: dtype('<m8[ns]')
```
was assymetric with respect to timedelta promotion.  Since simple math seems unaffected (it never worked), I am not sure we should try to undo this.  Should we make `isclose` work specifically for `timedelta64` maybe?",2021-02-01T19:02:23Z
771094835,"I'm afraid this one is on you @seberg, it didn't exist before #18230",2021-02-01T19:19:22Z
771097391,Thanks Bas.,2021-02-01T19:23:51Z
771136243,"Took a while to get the backtrace (for info, add
```
[tool:pytest]
addopts = -W error
```
to `setup.cfg`).  Looks like it routes back to NetCDF:
```
test/test_position.py:254: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../util/sv/util/cached.py:18: in __get__
    result = self.method(inst)
sv/sd5/position.py:173: in depth
    return self.bath.dem.interpolate(
sv/sd5/bathymetry.py:103: in interpolate
    return self.bilinear(lat, lng)
sv/sd5/bathymetry.py:91: in bilinear
    (, i1, i2) = self.x.linear(lng)
sv/sd5/bathymetry.py:153: in linear
    (i, j) = self.bracket(v)
sv/sd5/bathymetry.py:135: in bracket
    i = bisect_left(self.array, v)
netCDF4/_netCDF4.pyx:4492: in netCDF4._netCDF4.Variable.__getitem__
    ???
netCDF4/_netCDF4.pyx:4559: in netCDF4._netCDF4.Variable._toma
    ???
```
If you'd like me to close this and open an issue in that package, please say.",2021-02-01T20:28:37Z
771142710,"Actually, looks like this was fixed [a couple of weeks ago](https://github.com/Unidata/netcdf4-python/commit/d50b949ea3982a6281c6bce25d335736ad067b64) in netCDF4 , just not made its way into the package yet.  So please close this.",2021-02-01T20:39:21Z
771150291,Thanks for tracking this down yourself!,2021-02-01T20:53:19Z
771176439,"I think we can close this as wontfix/invalid, there's nothing actionable here as far as I can tell.",2021-02-01T21:37:44Z
771182400,"That makes sense. If two or more people confirm the tests pass and it's as fast as or faster than the `x86_64` wheel, it should be safe to upload to PyPI.",2021-02-01T21:49:18Z
771193160,"Sorry, but this code was better unchanged - using `from None` was a deliberate choice which I asked for here: https://github.com/numpy/numpy/pull/16418#discussion_r432444813",2021-02-01T22:08:55Z
771206138,"I see. Thanks for the quick reply! @eric-wieser 

> Sorry, but this code was better unchanged - using `from None` was a deliberate choice which I asked for here: [#16418 (comment)](https://github.com/numpy/numpy/pull/16418#discussion_r432444813)
",2021-02-01T22:31:48Z
771227764,"Does anyone know what these warnings are about, that show up in the travis build?:
```
 numpy/core/src/umath/scalarmath.c.src:555:1: note: the ABI of passing aggregates with 16-byte alignment has changed in GCC 5
```
I don't really see that they can be related to these changes.",2021-02-01T23:16:26Z
771241304,Hi I have a failed commit earlier. How can I suppress the former one? Thanks!,2021-02-01T23:49:15Z
771272445,I pushed in Eric's suggestion after confirming the random suite still passes locally for this feature branch.,2021-02-02T01:07:30Z
771274178,"Is there any chance these changes are related to the error reported here on aarch64?: https://github.com/scikit-image/scikit-image/pull/5197

I tried to look into it, but couldn't make much headway. I don't see anything wrong with our call.  For all I know, it is just as likely this was always flaky.",2021-02-02T01:12:35Z
771326148,"If you use the memoryviews directly (replace `[:] = 0, 0` with `[0] = 0` and use `shm.buf` directly), you will see that Python complains about the buffer being closed.  I am not sure about shared memory, so did not quite follow it all.

Further, if you use `np.frombuffer(shm.buf)`, it seems that you will get an exception that the shared memory cannot be closed.  `np.ndarray` behaves differently with respect to the `base` object though, it additionally has this bit of code:
```
    if (PyMemoryView_Check(obj)) {
        buf->base = PyMemoryView_GET_BASE(obj);
    }
```
Which is in `PyArray_BufferConverter`, which is used solely in this one place (although it is public API). I think those lines should simply be removed, getting the base of a memoryview here apparently circumvents the safety checks that the Python devs carefully put into place.",2021-02-02T03:25:12Z
771329575,"`shm.buf` is basically a `memoryview` of an `mmap.mmap`, which gets closed as soon as shm goes out of scope. my suggestion is to not let shm go out of scope by subclassing ndarray and attaching the `shm` as an attribute. This way, the shm lives as long as the array does.

```
class SHMArray(np.ndarray): #copied from https://numpy.org/doc/stable/user/basics.subclassing.html#slightly-more-realistic-example-attribute-added-to-existing-array

    def __new__(cls, input_array, shm=None):
        obj = np.asarray(input_array).view(cls)
        obj.shm = shm
        return obj

    def __array_finalize__(self, obj):
        if obj is None: return
        self.shm = getattr(obj, 'shm', None)
```",2021-02-02T03:33:49Z
771335237,"this is a duplicate of https://github.com/numpy/numpy/issues/9537
",2021-02-02T03:49:16Z
771336803,"Yeah, although it highlights a slight variation in `np.ndarray` rather than `np.frombuffer`.  It would be nice if `shared_memory.SharedMemory` implemented the buffer protocol, then numpy could directly pick it as a base.  Right now, I guess the trick with an array subclass is best. You can still call `np.asarray()` on that to get a base-class array if you like.",2021-02-02T03:53:51Z
771418311,"Is `timedelta64` the only built-in dtype that will fail [`multiarray.result_type(y, 1.)`](https://github.com/numpy/numpy/blob/872373b030eb6c181ab7f29c0928ccd8b0f74528/numpy/core/numeric.py#L2353)? I think we will need to special-class `datetime64` and what about `S` and `U` (on HEAD).",2021-02-02T07:07:19Z
771421682,"Linking to the PR for future reference: Unidata/netcdf4-python#1066, this is marked for release with netCDF4 1.5.6",2021-02-02T07:13:56Z
771424964,Thanks @charris,2021-02-02T07:20:17Z
771450651,"You haven't shown the exact code that you used, so we can't check that you are using `np.linalg.eig` correctly.  I can't reproduce your result:
```
In [21]: a = np.array([[ 2., -1., 0., 0., 0., 0., 0.], 
    ...: [-1., 2., -1., 0., 0., 0., 0.], 
    ...: [ 0., -1., 2., -1., 0., 0., 0.], 
    ...: [ 0., 0., -1., 2., -1., 0., 0.], 
    ...: [ 0., 0., 0., -1., 2., -1., 0.], 
    ...: [ 0., 0., 0., 0., -1., 2., -1.], 
    ...: [ 0., 0., 0., 0., 0., -1., 2.]])                                                                           

In [22]: evals, evecs = np.linalg.eig(a)

In [23]: evals[4]  # eigenvalue
Out[23]: 0.15224093497742724

In [24]: evecs[:, 4]  # corresponding eigenvector
Out[24]: 
array([-0.19134172, -0.35355339, -0.46193977, -0.5       , -0.46193977,
       -0.35355339, -0.19134172])
```
We can verify that `evecs[:, 4]` is an eigenvector associated with the eigenvalue `evals[4]`:
```
In [25]: a @ evecs[:, 4]
Out[25]: 
array([-0.02913004, -0.0538253 , -0.07032614, -0.07612047, -0.07032614,
       -0.0538253 , -0.02913004])

In [26]: evals[4] * evecs[:, 4]
Out[26]: 
array([-0.02913004, -0.0538253 , -0.07032614, -0.07612047, -0.07032614,
       -0.0538253 , -0.02913004])
```",2021-02-02T08:10:06Z
771496387,"Altivec is a PPC vector instruction set (and related C/C++ API). The numpy build system assumes that if it's building for macos also known as darwin on a non-x86_64 machine, then it's a PPC machine (because of the time of early MacOSX that would run on both Intel and PPC CPUs) and therefore numpy passes the altivec flags to the compiler. This is no longer valid since nowadays non-Intel chips for macos are more likely to be arm64 chips (Apple M1 in particular) that do not support altivec but other vector instruction sets instead. My [branch above](https://github.com/numpy/numpy/compare/master...ogrisel:accelerate-arm64) fixes that (among other temporary changes meant to remove intentional code changes that prevent to numpy to link against accelerate).",2021-02-02T09:27:13Z
771596620,"You're seeing two things happening here:

* `unique` requires entries to be sortable. The array `np.array([1, ""1""], dtype=object)` is not sortable, and also fails in `unique`. I think there are other issues open about this.
* There's some weird coercion going on that makes `np.array([1, ""1""])` a string array but `np.array([1, ""1"", None])` an object array. This means that `unique` somewhat works without `None` in the array, but notice that it replaced your `1` with `""1""`. I think @seberg probably wants to remove this behavior, which would make both cases in your report fail.",2021-02-02T12:18:34Z
771612323,@seiko2plus thoughts?,2021-02-02T12:49:16Z
771777781,"I think computing p-values is  out of scope for NumPy, and this issue would be more appropriate for SciPy.  SciPy has both `scipy.stats.pearsonr` and `scipy.stats.linregress`.  Both of these currently accept just a 1-d array for `x` and `y`, but there are issues on the SciPy github repository related to extending `pearsonr` to handle higher dimensional arrays.",2021-02-02T16:46:39Z
771785487,"That code will indeed give a FutureWarning on master, the point is that we actually go to `object` as a fallback, so you will just always get an object array; Of course `""1"" > 1` fails in Python, so at the end that will still mean that both will error in the future.

One of these days, maybe we have to think about having a very lightweight hashtable or so to make this type of operations more powerful (to be clear, its on nobody's agenda, and the typical answer is ""use pandas"" or similar). Although, I am actually not sure that `unique` would even be a prime target.",2021-02-02T16:55:18Z
771801875,"`median` has quite a lot of overheads compared to `partition`(I have some work that might significantly reduce those soon  by 50% IIRC). But I think the main difference you are timing is the fact that median adds a check for `NaN` (by partitioning also the `-1` element):
```
In [6]: In [1]: for n in [10_001, 100_001]:
   ...:    ...:     print(n)
   ...:    ...:     x = np.random.rand(n)
   ...:    ...:     assert np.median(x) == np.sort(x)[n//2] == np.partition(x, n//2)[n//2]
   ...:    ...:     %timeit np.median(x)
   ...:    ...:     %timeit np.partition(x, [n//2, -1])[n//2]
   ...: 
10001
112 s  1.05 s per loop (mean  std. dev. of 7 runs, 10000 loops each)
79.8 s  372 ns per loop (mean  std. dev. of 7 runs, 10000 loops each)
100001
1.09 ms  577 ns per loop (mean  std. dev. of 7 runs, 1000 loops each)
1.06 ms  1.64 s per loop (mean  std. dev. of 7 runs, 1000 loops each)
```",2021-02-02T17:12:14Z
771806790,"@mattip `datetime64` already cannot do any promotions for example (unlike for timedeltas, it never makes sense to convert a number to a date, while it can make sense to convert it to a time difference measured in ""seconds"" or ""milliseconds"").

`S` and `U` are supposed to error in `result_type`. The consistent error behaviour seems like the only reasonable thing to me right now.  But we could probably hack that behaviour above back for a while at least.  Or we try and fix the places like `np.isclose` where it seems to affect users (which may be playing catch-up a bit, admittedly).",2021-02-02T17:16:20Z
771819035,"https://github.com/numpy/numpy/pull/18302/commits/12d99670040c265a615694fd3b8ae1492807c93f and https://github.com/numpy/numpy/pull/18302/commits/c21509a595eea2cf9d64893a58adb5abcce68bb1 would qualify for a backport in my opinion.
The remaining two commits aren't nearly as important; I'd say that can remain exclusive to >= 1.21 unless someone has a strong opinion about.",2021-02-02T17:29:51Z
771819370,"Marking as bug, because I think we should add the workaround (which will cause a Python error rather than a segfault).  That won't actually help you @ikeboy but it would at least make it more clear that this pattern is invalid (and maybe why).

We do have `np.memmap`, I wonder if we should consider including an array-from-shared-memory factory in NumPy independently. Although, there are likely good libraries that include that and more.",2021-02-02T17:30:11Z
771820431,It would be best if you made the backport PRs.,2021-02-02T17:31:13Z
771820436,I've found a handful of other missing annotations in https://github.com/numpy/numpy/pull/18302; I'll get create a PR to close this issue after it's been merged.,2021-02-02T17:31:13Z
771822437,"> It would be best if you made the backport PRs.

Will do; I'll create the backport after this PR has been merged.",2021-02-02T17:33:17Z
771830274,"Sorry, looks like #18301 replaces this PR.",2021-02-02T17:42:33Z
771841976,"Thanks, LGTM. I find it a bit surprising that a memoryview isn't considered a mutable sequence (is that supposed to be only the length?), but I guess if Python changes it, we have a test that notifies us.

I would be fine with a memoryview fast path as you had it, although I guess this might be nicer for backporting.",2021-02-02T17:56:12Z
771852768,"`MutableSequence`s have to implement `insert`, which `memoryview` would never do.",2021-02-02T18:07:58Z
771877437,"Ah right. So I think this is still good enough to warn for tensorflow as well? In which case I think we should go with it or is there some other reason to prefer MutableSequence, that I am missing?

Although, I would be happy to entertain the thought of making things more strict in 1.21.",2021-02-02T18:39:34Z
771884018,"Thanks, to be clear, are you actually running into this error?",2021-02-02T18:47:54Z
771930353,"The reason I ask is, that I do not really expect anyone hitting this. So if you do and it isn't completely obviously just an oversight I would like to know (even if you trivially work around or so).",2021-02-02T19:50:04Z
771940993,"Sorry, forgot about these. Adding `static` uniformly seems great to me (some of this might cause me a merge conflict, but that is trivial). Thanks @alexhenrie.",2021-02-02T20:04:27Z
771942654,Thank you!,2021-02-02T20:06:48Z
771945663,"I doubt that is a measurable performance impact (and compilers might just optimize it in the next version), and am pretty ambivalent about the style change.  So, I guess I will just put it in the next time I see the PR, unless anyone has an opinion.",2021-02-02T20:10:55Z
771951644,"Yes, I am, while debugging [lscsoft/lalsuite!1526](https://git.ligo.org/lscsoft/lalsuite/-/merge_requests/1526). What is the correct way in C to declare a new PyArray_Descr to wrap a specific kind of Python object?",2021-02-02T20:18:52Z
771964196,"Some people have used a trick by making structured descriptor instead.  I am not sure how you managed to wrap a specific python object. As far as I could tell, you should never be able to delete such an array correctly, because you *cannot* implement reference counting for your custom dtype's items!

I really *want* API to wrap a custom Python kind of python object as a dtype, but I am slightly shocked you managed to do it; and it wasn't super high on my agenda.  I have to dig in and see what exactly you are doing and what is working correctly and what may not be...

So there is the hack I have seen, which I hope will continue to work is something I created a test for here: https://github.com/numpy/numpy/pull/17320 ",2021-02-02T20:35:35Z
771972109,I'll do this one!,2021-02-02T20:46:55Z
771972372,"Out of curiosity, should `abs(inf + nanj)` be `inf` or `nan`?  My first thought was `nan` because there's a `nan` in there, but the tests seem to be expecting `inf`.",2021-02-02T20:47:18Z
771975100,"> should `abs(inf + nanj)` be `inf` or `nan`?

I am sure NaN is correct. If an `inf` result is tested, that could well be a bug in the complex `abs` implementation.

EDIT: I am basing this on the fact that ""all NaNs are NaNs"" for complex numbers (i.e. inf+nanj should be the same as nan+nanj). If you disregard that, I guess an `inf` result does make sense. A `NaN` result would seem safer to me though.",2021-02-02T20:51:17Z
771990288,"@lpsinger I tried to understand what the code does to see if there is some clear different solution, or this is a problem I need to dig into.  But I have to admit, the whole SWIG logic is making it hard for me to really follow things. The dtype doesn't really seem to do a whole lot?

For example, your `copyswap` function seems to simply copy memory around, so do you actually need the `HAS_REF` flag at all?",2021-02-02T21:11:42Z
771992719,"> @lpsinger I tried to understand what the code does to see if there is some clear different solution, or this is a problem I need to dig into. But I have to admit, the whole SWIG logic is making it hard for me to really follow things. The dtype doesn't really seem to do a whole lot?
> 
> For example, your `copyswap` function seems to simply copy memory around, so do you actually need the `HAS_REF` flag at all?

I don't know the answers to those questions. @kwwette would.

Meanwhile, though, I'm [trying](https://git.ligo.org/lscsoft/lalsuite/-/merge_requests/1526/diffs?commit_id=733b9886f4657de52877717fb49483a6a65d3284#ba2003ee5bf2dab757c03c9e8a349d2776ccfdd3_693_677) the workaround that you suggested:

```c
      // Numpy workaround for registering user-defined object types. See
      // https://github.com/numpy/numpy/pull/18303,
      // https://github.com/numpy/numpy/pull/17320
      (*pdescr)->names = Py_BuildValue(""(s)"", ""swigobj"");
      (*pdescr)->fields = Py_BuildValue(""{s:(s,i)}"", ""swigobj"", ""O"", 0);
```

It's working so far on my own machine; now I'm waiting to see the results of our CI pipeline.

",2021-02-02T21:15:26Z
771993284,"Just to note, my half plan is that we deprecate `""S""` mostly (which doesn't actually change much). It would still be valid input to some functions like `np.array()` that *do* honor the fact that it can represent any length; and those should be the vast majority of use cases.
For all other cases, we would then force `""S0""` to get the same result without any ambiguity (at least in the long run).",2021-02-02T21:16:18Z
771997208,"OK, to be clear: I am happy to discuss and look into this more; it just may be useful to have a brief chat to jump into whats going on.
Even if nothing much comes out, it could be an interesting case study for when the new DType API has ripened enough.

I added the error thinking that nobody can (usefully) do this. I am still not sure whether you managed or not, but I never had the intention of breaking working code. The idea was to warn new users and remove one worry of strange corner cases that might be hard or impossible to keep supporting unmodified.",2021-02-02T21:22:21Z
771997966,"> > should `abs(inf + nanj)` be `inf` or `nan`?
> 
> I am sure NaN is correct. If an `inf` result is tested, that could well be a bug in the complex `abs` implementation.

Okay, I recently wrote a C program to check `cabsl(+-inf + nanj)` and `cabsl(nan +- infj)`, since those tests have been failing for months.  All four return `nan`.  The corresponding tests fail:
```bash
FAILED numpy/core/tests/test_multiarray.py::test_npymath_complex[complex256-inf-nan-npy_cabs-absolute] - AssertionError:
FAILED numpy/core/tests/test_multiarray.py::test_npymath_complex[complex256--inf-nan-npy_cabs-absolute] - AssertionError:
FAILED numpy/core/tests/test_multiarray.py::test_npymath_complex[complex256-nan-inf-npy_cabs-absolute] - AssertionError:
FAILED numpy/core/tests/test_multiarray.py::test_npymath_complex[complex256-nan--inf-npy_cabs-absolute] - AssertionError:
```
with the error:
```python
E       AssertionError:
E       Not equal to tolerance rtol=1e-07, atol=0
E
E       x and y nan location mismatch:
E        x: array(inf)
E        y: array(nan, dtype=float128)
```
I just noticed the dtype for `x` isn't right, so I'll look into fixing that.",2021-02-02T21:23:27Z
772009234,"@kwwette, @duncanmmacleod, would you care to take over from here and follow up with @seberg?",2021-02-02T21:39:32Z
772013849,"I have also spent the day struggling with this. I tried a few things and, ultimately, ended up reverting my Lambda function to runtime of Python 3.7. Then I used of the AWS layer for SciPy (provides a working numpy) + a shared layer for pandas-gbq (provides a working pandas), found here - https://github.com/mthenw/awesome-layers  
",2021-02-02T21:44:34Z
772033348,Thanks @mckib2 .,2021-02-02T22:04:30Z
772035578,Thanks Bas.,2021-02-02T22:06:59Z
772036118,Thanks Bas.,2021-02-02T22:07:36Z
772037780,Resolved as of https://github.com/numpy/numpy/pull/18306.,2021-02-02T22:09:26Z
772038655,Thanks @Qiyu8 ,2021-02-02T22:10:28Z
772042955,"I tracked the `numpy.core._multiarray_tests.npy_cabs` call to `numpy/core/src/npymath/npy_math_complex.c.src` line 121, where it calls `npy_hypotl` and got stuck there.  I'm going to leave that be and post my other work.",2021-02-02T22:15:43Z
772081487,"Hmm, the last commit seems to have removed that issue. I have honestly no clue if it was related or not and would be happy to back it out to see if anyone is curious.",2021-02-02T23:16:54Z
772083763,@DWesl Do you want me to keep this up?,2021-02-02T23:22:16Z
772087070,"You can close it if you want.  I'm running a last round of tests before creating the pull request, to see if any of the failing tests are passing now.",2021-02-02T23:30:06Z
772089196,">  Out of curiosity, should abs(inf + nanj) be inf or nan?

Actually, it looks like the result should be `inf`, based on behavior specified in IEEE 754-2008.  I haven't read the full specification; this comment is based on a [comment](https://bugs.python.org/issue38382#msg354262) that @mdickinson made in a Python bug report.  And `inf` is the value returned by Python's `abs` and by `np.abs` in 1.19.5:

```
In [1]: import math                                                                                                 

In [2]: z = complex(math.inf, math.nan)                                                                             

In [3]: abs(z)                                                                                                      
Out[3]: inf

In [4]: import numpy as np                                                                                          

In [5]: np.__version__                                                                                              
Out[5]: '1.19.5'

In [6]: np.abs(z)                                                                                                   
Out[6]: inf
```
And there is also
```
In [11]: np.hypot(np.inf, np.nan)                                                                                   
Out[11]: inf
```",2021-02-02T23:34:41Z
772092209,"Makes sense, the usual safe bet is to say is to replace NaN with ""any possible value"".  The oddball is just that for complex two values for which `np.isnan()` is True, will have different values for these cases.  But I suppose that isn't really an issue.",2021-02-02T23:42:15Z
772105054,"@seberg Thanks for helping us with this.

LALSuite is a large scientific library written in C, for which we generate Python wrappings using SWIG plus some custom code. LALSuite contains a number of data structures of the following form:
```
typedef struct {
   double value;
   ...
} MyType;

typedef struct {
   size_t length;
   MyType **data;   // array of length 'length' containing items of type 'MyType*'
} MyTypeVector;
```
We want to be able to view the `data` member of a `MyTypeVector` instance from Python, i.e. given an instance `v` of type `MyTypeVector`, the user can write `v.data[0].value` and access the first `MyType` array element and its fields.

To do this we've been using NumPy arrays as a view of the `data` member of `MyTypeVector`. When the user types `v.data`, they get a NumPy array with an object type, where each element is viewing a `MyType*` pointer. To then make each element accessible from Python when the use types `v.data[0]`, we provide a custom getitem function which accesses the `data` vector, and returns the requested `MyType*` element wrapped in a class provided by SWIG. The SWIG wrapper class then makes the `MyType*` fields available, e.g. so that `v.data[0].value` works.

This has worked well so far. We've not noticed any problems with memory leaks with e.g. reference counting. In the statement `val = v.data[0].value`, a wrapper class to the `v.data[0]` would be created on the fly, exist temporarily while its `.value` attribute is stored in `val`, and then (I presume) would be destroyed as it's no longer referenced anywhere. The underlying memory of the array is owned by the `MyTypeVector` type, and we have a custom memory tracking system in place that e.g. prevents `v` from being destroyed as long as `v.data`, `v.data[0]`, etc. are referenced anywhere.

It seems so far that for Numpy 1.20 we just need to work out a more portable way of creating the NumPy array descriptor that we've done previously. We'd appreciate though whether you're aware of any other changes to NumPy that might mean we can no longer using the NumPy array views as described above.",2021-02-03T00:13:24Z
772113999,"@kwwette OK, I honestly expect you can probably just delete those flags.

The point here is that `data` (the `MyType **data` field representation in Python) is the only place where memory gets allocated. I assume that `MyType` is really just a ""naive"" type (i.e. it has a fixed size and contains no pointers).  So you have to create a  dtype for `MyType` (which you are free to do). Issues would only arise if `MyType` included a ""reference"":
```
typedef struct {
    double value;
    char *dynamically_allocated_storage;  /* If I copy/delete this element, what would happen? */
} MyType;
```
or alternatively:
```
typedef struct {
    double value;
    PyObject *obj;  /* object field needs reference counting */
} MyType;
```
which for example is a use case that users who want to implement strings run into.  To implement those correctly you do need `NPY_ITEM_IS_POINTER` and/or `NPY_ITEM_REFCOUNT` (but it won't really help you...).  If the `MyType` struct does not have these properties, these flags should simply be unnecessary, the only thing I can think of that they may achieve is that `data.view(...)` might be forbidden.

Just be sure, if `MyType` is a trivial struct of C types (or more specifically NumPy dtypes), it might be easier to use a structured dtype, though?",2021-02-03T00:33:59Z
772116074,"Oh, one thing. If you want `v.data[0].value` to work, you would have to use `np.record`. That is a bit weirder, but `np.dtype((np.record, np.dtype([(""value"", ""d"")])))` for example should do the trick if used as a dtype.",2021-02-03T00:38:45Z
772129836,"@seberg Thanks for the pointers. We'll try dropping the flags you mentioned and see how that goes.

I don't think we'd be able to translate the C structs into NumPy structured types, at least not easily. LALSuite is a library of thousands of functions, structs, etc. so the point of using SWIG is to automatically generate the Python interface from the library C headers, without having to manually maintain/sync separate C and Python interface descriptions. So we'd have to find some way of automating the generation of NumPy structured types. That may be possible but would require a lot more work, and might break existing code.

There are also some array element types in LALSuite (i.e. `MyType`) which are more than just ""naive"" types, i.e. they themselves point to dynamic memory. In these cases the consequence of assignment, e.g. `v.data[0] = elem` should be that the `MyType` pointer `data[0]` is assigned to `elem` (clobbering any existing value), and `elem` is marked as no longer ""owning"" the `MyType` pointer it wraps, which is now owned by `v`. That's probably not entirely Pythonic, but there's limits to what you can do wrapping a C library ... It would be useful to keep this functionality e.g. for initialising arrays, but I guess it could be dropped - and `v.data` made read-only - if it can no longer be supported.",2021-02-03T01:08:55Z
772143636,"OK, so the point of those flags (internally to NumPy) is dynamic memory (really we only use Python Objects, which are also a type of dynamic memory). But the problem is that the additional functionality to handle dynamic memory correctly is not accessible for user dtypes. The problem I see is this:
```
arr = v.data
arr_copy = arr.copy()
del v, arr  # I assume `v` may ""clean up""
arr_copy[0].value  # segfault?
# and/or:
del arr_copy  # may not clean up the elements correctly
```
At that point `arr_copy` does not know anything about `v` and cannot keep its memory alive (`arr` itself probably has `v` as its base. It does not own its memory, so these issues do not apply.).
So if you have a `MyType` with dynamically allocated data you may have a problem (with or without the NumPy change).

If this is a larger headache for you, I can also undo the change. Right now, all the flag probably achieves is disabling a few things like: `v.data.view(some_dtype_with_correct_shape)`.  But ~there~ whether clean or not, it probably will just stay identically working/broken...

I would like to get to a point where user DTypes can contain references (i.e. have their own memory management if necessary), but it is on the backburner list, simply because it is something that can be achieved later on.",2021-02-03T01:41:45Z
772150413,"@seberg I'm not sure I've ever tried to copy one of our NumPy array views, as in your `arr_copy = arr.copy()` line. I suspect you're right in that the correct memory tracking would break down here. It's not a problem we've encountered so far, however, so I guess none of our users have tried it, at least not for these struct-type arrays. (I imagine it would fine for an array of `double`, however.)

Is there a way to disable `copy()` operations for a particular NumPy type, e.g. a flag, or a ""not implemented"" error we can raise. I think I'd be okay with just disallowing this behaviour in instances where it doesn't make sense or may be unsafe.",2021-02-03T01:57:35Z
772199603,The powerpc CI failure is certainly not related,2021-02-03T03:42:59Z
772199753,thanks @sleeplessinseattledev ,2021-02-03T03:43:25Z
772240429,"Build errors appear not from this PR, see #18312 ",2021-02-03T05:27:35Z
772294768,"i think we should move the fast check build to the linux_native build so the flaky mingw does not prevent the rest of the CI from running. We could also mark that test as flaky, but then we will never pay attention to it when it fails",2021-02-03T07:21:56Z
772295697,Thanks @abhayaman669 ,2021-02-03T07:23:39Z
772296742,You're welcome @mattip. :),2021-02-03T07:25:41Z
772301902,Huh. It is still here https://numpy.org/doc/stable/genindex.html but not easily accessible with the new theme. I wonder where we should link it. The [theme docs](https://pydata-sphinx-theme.readthedocs.io/en/latest/) which use the theme themselves do not really expose it. I opened an issue with the theme authors to see if we are missing something.,2021-02-03T07:35:43Z
772302953,"Do you know how difficult it would be to set up a CI run with cygwin, at least for the 64-bit case? Is it supported by github actions/Azure?",2021-02-03T07:37:51Z
772323618,"Annex G of the C standard is also a useful (and more easily available) source of information for these corner cases. For the C11 standard, G.6p6 has:

> Each of the functions cabs and carg is specified by a formula in terms of a real function (whose special cases are covered in annex F): cabs(x + iy) = hypot(x, y) [...]

while F.10.4.3p1 has:

> hypot(, y) returns +, even if y is a NaN.",2021-02-03T08:18:43Z
772327420,"AFAIK, there is not really any magic here, and a link to the genindex is included in sphinx's default `index.rst` template. See https://github.com/sphinx-doc/sphinx/blob/3.x/sphinx/templates/quickstart/master_doc.rst_t, which has a 

```
Indices and tables
==================

* :ref:`genindex`
* :ref:`modindex`
* :ref:`search`
```

section on the index.rst homepage. Which is probably the reason that many projects have that on the homepage. 
So but if you want those links somewhere, you need to put them somewhere.

And in the pre 1.20 numpy docs, it seems you didn't have a index.rst but a html template for that page? Which included those links as well but in html: https://github.com/numpy/numpy/blob/maintenance/1.19.x/doc/source/_templates/defindex.html

Now, I don't actually directly see an easy link to the genindex on https://numpy.org/doc/1.19/? Where is the link to be found?
",2021-02-03T08:25:49Z
772328518,"> Now, I don't actually directly see an easy link to the genindex on https://numpy.org/doc/1.19/? Where is the link to be found?

Upper right corner, there is a blue box ""index""",2021-02-03T08:27:48Z
772330309,"Ah, I see there was this ""index"" link in the top right corner, which is what was probably meant?

![image](https://user-images.githubusercontent.com/1020496/106718878-ecdf7500-6601-11eb-87bd-957f70d175b5.png)

That indeed seems to be a feature of the old theme, that's not present in the new theme.

Do you have an idea where you would like to see it in the new layout?",2021-02-03T08:31:05Z
772338497,Can you show what the error traceback used to look like vs what it now looks like?,2021-02-03T08:44:41Z
772342087,"This has changed with numpy 1.20 & it now raises a `ValueError`.  Will that behavior stay?

```python
import numpy as np

x = np.arange(10)
x = np.pad(x, 1, ""constant"", constant_values=np.nan)
```

```python-traceback
ValueError                                Traceback (most recent call last)
<ipython-input-5-c7a26ff07a7f> in <module>
      6 
      7 x = np.arange(10)
----> 8 x = np.pad(x, 1, ""constant"", constant_values=np.nan)
      9 print(x)

<__array_function__ internals> in pad(*args, **kwargs)

~/conda/envs/xarray_dev/lib/python3.8/site-packages/numpy/lib/arraypad.py in pad(array, pad_width, mode, **kwargs)
    801         for axis, width_pair, value_pair in zip(axes, pad_width, values):
    802             roi = _view_roi(padded, original_area_slice, axis)
--> 803             _set_pad_area(roi, axis, width_pair, value_pair)
    804 
    805     elif mode == ""empty"":

~/conda/envs/xarray_dev/lib/python3.8/site-packages/numpy/lib/arraypad.py in _set_pad_area(padded, axis, width_pair, value_pair)
    145     """"""
    146     left_slice = _slice_at_axis(slice(None, width_pair[0]), axis)
--> 147     padded[left_slice] = value_pair[0]
    148 
    149     right_slice = _slice_at_axis(

ValueError: cannot convert float NaN to integer
```



",2021-02-03T08:51:01Z
772379433,"@eric-wieser can you please guide me on how can I check this.
I tried to run the test using `python -c 'import numpy; numpy.test()'` but this is not working for me.
Thank you.",2021-02-03T09:51:29Z
772513944,"I've heard of someone who's done it.  I'll see if I can find their work.

The Azure WindowsFast failure seems to be a problem finding `nm.exe`, which I think is unrelated.",2021-02-03T13:41:06Z
772531437,[egor-tensin/setup-cygwin](https://github.com/egor-tensin/setup-cygwin) looks promising.  Do you want me to try adding that here? In another PR?,2021-02-03T14:00:24Z
772535308,"> i think we should move the fast check build to the linux_native build

Good idea.",2021-02-03T14:05:47Z
772552294,You are correct.,2021-02-03T14:31:05Z
772570340,"PR welcome. As this is a bugfix, I think we should fix both [mtrand.pyx](https://github.com/numpy/numpy/blob/master/numpy/random/mtrand.pyx#L3514) and [_generator.pyx](https://github.com/numpy/numpy/blob/master/numpy/random/_generator.pyx#L3015)",2021-02-03T14:56:35Z
772594589,"I expect it will stay, unless this is an issue for you? But if you put in `np.array(np.nan)`, chances are you get the old behaviour, and that difference makes things a bit tricky.",2021-02-03T15:28:56Z
772597976,"Thanks, seems good. This makes the initial check a ""full"" test run (instead of fast), but maybe it doesn't matter.",2021-02-03T15:33:06Z
772604641,"There are a few things already going on in this PR. 
- test parameterization
- refactor a helper-function implementation for cygwin
- blocklist some system library functions
- check for `sys.platform == 'cygwin' or sys.platform == 'win32'` rather than just the latter

I think we should try to set up CI for cygwin in another PR, then circle back to this once it is working. That github action looks plausible, you would need the python package and then could follow the [way pypy overrides the PATH](https://github.com/numpy/numpy/blob/master/.github/workflows/build_test.yml#L198) to make sure the cygwin python and gcc get used.",2021-02-03T15:42:22Z
772605169,@seberg @charris does that make sense as a plan?,2021-02-03T15:43:08Z
772608457,"It is still the [fastest Azure job](https://dev.azure.com/numpy/numpy/_build/results?buildId=15243&view=logs&jobId=40b6d036-694f-5d1f-586c-a0e1fd689e6f&j=40b6d036-694f-5d1f-586c-a0e1fd689e6f). Maybe we should be looking at caching the 32-bit build tools, which take ~5 minutes to install per job.",2021-02-03T15:47:40Z
772610876,"Oh wow, lets just put it in. Thanks Matti!",2021-02-03T15:51:02Z
772611896,"I haven't seen [this compiler failure](https://github.com/mattip/numpy/runs/1823488257?check_suite_focus=true#step:4:3108) before:
```
during IPA pass: profile
numpy/core/src/multiarray/einsum_sumprod.c.src: In function longdouble_sum_of_products_contig_three:
numpy/core/src/multiarray/einsum_sumprod.c.src:1264:1: internal compiler error: in coverage_begin_function, at coverage.c:656
 1264 | }
      | ^
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-9/README.Bugs> for instructions.
```

But I don't see it here, it all passed. Does github actions rerun automatically?",2021-02-03T15:52:21Z
772619204,"```python
import numpy as np
x = np.arange(10)
x = np.pad(x, 1, ""constant"", constant_values=np.array(np.nan))
```
Also errors for me. I actually prefer an error over a weird number. I assume this comes back to https://numpy.org/doc/stable/release/1.20.0-notes.html#numpy-scalars-are-cast-when-assigned-to-arrays?

(Honestly, that paragraph is super confusing to me & I cannot reproduce the mentioned changes between 1.19.5 and 1.20.0. Also the paragraph suggest that numpy will error in fewer cases which contrasts to what I we see here for pad. And why would fewer errors that be a good change here? But I guess I am missing something...)

",2021-02-03T16:01:59Z
772619483,"Uhoh, I don't remember ever seeing it either. Thats a strange error, just pointing at the end of the file. Why didn't this cause CI to fail?!",2021-02-03T16:02:21Z
772631150,"Ah OK, I was refering to that, but the changes had to go back and forth a bit, so now I am also a bit confused :). An example of actual changed behaviour is:
```
np.array([np.array(np.nan)], dtype=int)
```
But it is in the opposite direction.  The error is nicer though, and I expect we can keep it.  What we really would like is to also give at least a warning for large array casts (if performance permits it), but I am unsure if at that point this would be nicer as a warning as well, since the value (at that point) is not a Python float, but rather a `np.float64`.

I think whats going on here, is that trying to retain the error in certain calls, I may have inadvertently ensured it is also being given here.

In the end, assuming we prefer the current behaviour, we can probably just retain it until we feel like changing it.  There are some slightly strange things around scalars vs. arrays that I don't like but can live with.  The reasons for these changes was that there were just too many paths doing slightly different things :(.

I may have to review the release notes...",2021-02-03T16:18:14Z
772648781,"I am not picky, myself. It would be excellent if it was in the sidebar or in the top banner. Even if it was present only in the main panel of the home page, that would be better than the current situation.",2021-02-03T16:40:41Z
772659782,"@eric-wieser closing this PR as it's mentioned in the file that this is an autogenerated file and this does not need to be edited.
Thank you.",2021-02-03T16:55:33Z
772674379,Thanks Tyler.,2021-02-03T17:15:11Z
772683390,"I opened gh-18316 for the moment. But we may have to make a call what to do about it.  Right now, my first thought is to just roll with it, but it depends a bit on how much this affects you/others.

For the future? I don't quite know... Since I expect everyone prefers the error behaviour, I would expect us to stick with it, at least unless we want `np.array([np.nan], dtype=int)` to be a warning (forever), in which case aligning it in the long run may make sense.",2021-02-03T17:27:36Z
772748829,"In the meantime I've created a PR with some refactoring in order to reduce the `git` calls: https://github.com/mgeier/sphinx-last-updated-by-git/pull/25

I've re-opened this PR as WIP and added my refactored branch: ab712cc54c1491baafd6c2d2257be8e5d62d4705

Sadly, this still times out the build process: https://app.circleci.com/pipelines/github/numpy/numpy/6016/workflows/4d82d6f9-2bd8-4268-9486-46cb1eb7ba52/jobs/18107/steps

Any ideas how I could improve this?

I guess I'll have to display some timing information to see where most of the time is spent ...",2021-02-03T19:09:49Z
772755745,I'm pretty sure this file is _not_ autogenerated - this is the file that generates the generated files!,2021-02-03T19:20:22Z
772768007,"This in itself is normal behaviour, of how the operating system and python reclaim memory. Try allocating `x` again repeatedly and you will see that the memory ""settles"" around those 800 MB.",2021-02-03T19:35:41Z
772778826,"Right. I once noticed this behaviour in one of my C extensions where I needed to repeatedly allocate and deallocate memory. It also happens with other memory allocators specially when each deallocation is significantly small compared to the total allocated memory. Memory allocators usually use a threshold on the amount of deallocation for deciding whether to give back that memory to prevent memory fragmentation, hence prevent performance degradation.",2021-02-03T19:52:45Z
772780966,The fix is now available in [KB4598291](https://support.microsoft.com/en-us/topic/february-2-2021-kb4598291-os-builds-19041-789-and-19042-789-preview-6a766199-a4f1-616e-1f5c-58bdc3ca5e3b?ui=en-us&rs=en-us&ad=us).,2021-02-03T19:56:25Z
772782560,"All the pages in `generated` are built from the numpy docstrings, so you could skip those.",2021-02-03T19:59:07Z
772829139,"Thanks @bashtage for the first concise reproducer for this, that made it relatively simple to pinpoint the problem. Closing.",2021-02-03T21:16:14Z
772831123,Windows update is out. @yaav can you update to get the fix and try to reproduce?,2021-02-03T21:19:20Z
772838251,"Answers / responses to the below statement are welcome, but certainly not expected. I'd just like to document my experience for anyone else unfortunate enough to stumble here, and maybe get some insight into what the hell is going on haha.
____

Long story very very short, a lot of my company's signal processing seems to end up in these ""BLAS"" functions called by Numpy or Scipy when trying to solve matrices. We use `scipy.signals.firwin` and `scipy.signals.filtfilt` a lot.

Anyway, I was able to see a **24x speedup simply by setting `OMP_NUM_THREADS=1`**. I suspect that OpenBLAS is just trying too hard to parallelize on the relatively small inputs we're giving it? I don't have the energy to dig into this deeply, but this seems like a pretty ridiculous experience to have had.

Evidently, this issue has lurked since my company upgraded to Numpy / Scipy versions which bundled OpenBLAS. Fortunately, our overall system's performance was not dominated by how fast we can solve matrices, but clearly one particular module was heavily. We didn't have this issue when when compiling Numpy / Scipy against ATLAS or `libblas` and `liblapack`.

This leaves me with a question: why did the maintainers elect to bundle OpenBLAS with Numpy and Scipy? Is it really efficient for large inputs? Or other circumstances? I suppose I'm just blown away how much worse it is in our use case. Am I just dumb, and missed a section of the doc on performance tuning?",2021-02-03T21:31:42Z
772863548,"```
E           AssertionError: assert False
E            +  where False = <built-in method startswith of str object at 0xee344710>('| Iterator SizeOf: 488')
E            +    where <built-in method startswith of str object at 0xee344710> = '| Iterator SizeOf: 264'.startswith

```",2021-02-03T22:15:49Z
772872687,"Went into a rabbit hole trying to add the other test, but actually found a way I think (although not the one I expected to use. The one I expected to use found an unrelated bug of course...).

Thanks, the size of `intp` and pointers isn't fixed, so that makes sense. Just deleting should solve that issue (assuming we are OK with this stdout capture; I am struggling on capsys for the other test as well, but I think I will just give up there).",2021-02-03T22:33:32Z
772881258,The test failures look legitimate.,2021-02-03T22:51:49Z
772895950,If the test becomes to convoluted we might want to omit it.,2021-02-03T23:23:04Z
772896076,"OK, hopefully CI will be happy now. I realized that I have to use `capfd` instead of `capsys` and then I can avoid those ugly subprocess calls...",2021-02-03T23:23:20Z
772983258,"I have a look at NumPy repo, and find that not every Python file has a corresponding stub file, and not every function is type-annotated.

It seems like, there is a certain decision or choice to select/decide which Python files or functions should be added type annotations in priority.

I am curious that what is the selection decision? 
Besides the top-level functions listed here, do Numpy consider any other plan?

Thanks. :-)",2021-02-04T02:41:42Z
772989725,"> #17104 has been merged now; all (public) objects in `np.random` are currently annotated as `Any`.
> 
> While still very broad, this will silence all ""Module has no attribute ..."" errors.

I am not sure whether adding these  `Any` types for silencing `Mypy` errors is a good practice?
My meaning is that type annotation aims to help understand code and static check type errors, but `any` types take no such role. 

I just feel contradictory about this.
But I don't have any other ideas to quickly remove ""Module has no attribute ..."" errors.",2021-02-04T02:58:56Z
773006266,"> The test failures look legitimate.

Yes, was looking on mobile and didn't read correctly.  I'll start looking at this right now.  If it does indeed seem like a headache, will omit the tests.  Fingers crossed for an easy fix.",2021-02-04T03:47:02Z
773008239,"yes, you are right. I did it wrong. So is the code correct? should I reopen this PR?",2021-02-04T03:53:14Z
773020608,"So I've reproduced the issue in a Docker container and there seems to be a spurious extra `-c` compile option given to the `gfortran` command (extra line breaks added for clarity):
```bash
------------------------------------------------------------ Captured stderr call ------------------------------------------------------------
error: Command ""gfortran-5 -Wall -g -ffixed-form -fno-second-underscore -fPIC -O3 -funroll-loops 
-I/home/numpy/venv/lib/python3.8/site-packages/numpy/distutils/tests/../../../numpy/core/include
-Ibuild/src.linux-i686-3.8/numpy/distutils/include -c -c ./_dummy1.f   # <--- spurious ""-c"" argument here
-o build/temp.linux-i686-3.8/_dummy1.o"" failed with exit status 127
```

Has anyone encountered this before?  This almost looks like another bug to me

_EDIT: this does not appear to be what's causing the error and might not be interesting at all_

_EDIT EDIT:_

I've identified the issue: there is no Fortran 77 compiler available.  This [f2py test check](https://github.com/numpy/numpy/blob/9f12028b9453c17b72b26355fd503e512af96a5d/numpy/f2py/tests/util.py#L345) prevents most of the tests from running in these Docker containers.  The test in this PR needs a similar check.",2021-02-04T04:31:57Z
773055849,Hey @seberg sorry for the long pause. Basically when I added `np.float128` test i noticed a crash. I'm working on fixing that adding some more cases.,2021-02-04T06:06:16Z
773241013,"Hi @jinwuxia,

Due to the sheer size and complexity of the numpy library, a large number of functions are currently defined (as `Any`-based placeholders) in the main namespace. The general idea is to move them to their own stub file once the annotations are properly implemented.",2021-02-04T11:33:54Z
773243904,"> I am not sure whether adding these `Any` types for silencing `Mypy` errors is a good practice?

Considering the typing of numpy is very much a work in progress, I don't see any problem with this:
The `Any`s will naturally disappear as more annotations will be added over the course of time.

Improvements are of course possible and, in fact, would be very much welcome additions.",2021-02-04T11:39:28Z
773245625,"Removed 7 aliases/classes annotated in https://github.com/numpy/numpy/pull/18284:
* `bool8`
* `bytes0`
* `complex256`
* `float128`
* `object0`
* `string_`
* `void0`",2021-02-04T11:42:26Z
773262055,I agree that this is the pragmatic approach that allows for improvement when people who rely on these modules have time to invest.,2021-02-04T12:13:06Z
773380406,"This style was introduced deliberately in #17382 by @bjnath.

https://github.com/numpy/numpy/blob/9f12028b9453c17b72b26355fd503e512af96a5d/doc/source/_static/numpy.css#L20-L29

I also think it looks a bit odd, and personally would have diverged from the pydata theme only when it came to numpy colors and/or fonts - but I don't feel strongly enough to back those lines out.",2021-02-04T15:10:34Z
773386122,"The code for the Freedman Diaconis binwidth estimator is here: https://github.com/numpy/numpy/blob/f36e940a4726abb38c4929259e8eaf00d68c3d18/numpy/lib/histograms.py#L199

Aside from cruft, the function is two lines long:

```
iqr = np.subtract(*np.percentile(x, [75, 25]))
return 2.0 * iqr * x.size ** (-1.0 / 3.0)
```

This is pretty much identical to your method, and yields the same result. However, some additional magic happens under the hood in [`_get_bin_edges`](https://github.com/numpy/numpy/blob/f36e940a4726abb38c4929259e8eaf00d68c3d18/numpy/lib/histograms.py#L360)  to turn this width into bin edges:

1. First, we round the width up to the nearest bin count that fits the range exactly:

    https://github.com/numpy/numpy/blob/f36e940a4726abb38c4929259e8eaf00d68c3d18/numpy/lib/histograms.py#L411

    This is equivalent to `int(np.ceil(np.subtract(9, 1) / 3.8459988541530894))` in your case, so the result is `3` bins. Given that the original bin width would require `2.080083823051904` bins to fit across the range, this seems reasonable.

2. Then we generate the edges to fill the range exactly:

    https://github.com/numpy/numpy/blob/f36e940a4726abb38c4929259e8eaf00d68c3d18/numpy/lib/histograms.py#L446

As you can see, it is the rounding step that is responsible for the difference in results. So your observation of the difference is correct, but the code is using the same result as yours along with some additional transformations. We could argue ad nauseum about rounding vs rounding up or rounding down, but the result would never match the optimal bin width in any case. One other alternative I can think of is keeping the optimal width, and setting the start/end points to fully contain the range. This would result in biases of the edge bins, however, which is generally undesirable.",2021-02-04T15:18:15Z
773398354,"Fair point, but I'd guess partitioning two elements should not take more than twice the time it takes to partition just one element(?), and the overhead is much more than twofold for small arrays.",2021-02-04T15:35:25Z
773399593,"Long pauses are typical, just make sure to bug me/us when you feel it would help. It is pretty heroic to dabble in these parts of NumPy!",2021-02-04T15:37:08Z
773405465,Wouldn't it make more sense to put the links in the table above these paragraphs?,2021-02-04T15:45:28Z
773412027,"Yah, sorry, I guess your focus was really the overheads.  There are many reasons for the fairly large overheads, the main one: `np.median` is written in python, which causes other overheads to pile up over the fairly long and complex function.

One thing, I have been looking at (by now, I am mostly interested in it because it caused me to cleanup ufunc code in the process to be honest), is to generally reduce overheads of some functions (`array` and friends, ufunc calls, and methods calls with keyword arguments).  That had a pretty big effect on median IIRC (although maybe much more so on even sized).

Another big thing might be to speed up `__array_function__`, which requires two design changes in it though, to go the route that seems most promising (neither of which is likely a problem, but I am not sure how big of a project it would be), which would reduce *a lot* of overheads considerably.

More limited to `median` itself, but maybe far more promising:
* Maybe the code can be restructured to be shorter/more elegant (and in this case, thus also have less overheads).
* `median` could be made a gufunc (potentially with a Python core). Even a Python core may help, since it optimizes simplifies some of the setup code at least (if the 1-D median has much less overhead compared to the N-D version).
* Parts of median could be moved to C
* Or it is soon time to look into other acceleration approaches, cython/pythran for certain functions...

Median is a bit of a complex beast, so not sure how easy these are.",2021-02-04T15:54:59Z
773432969,"Anyone knows what's going with the PyPy tests?
The failure seems to be unrelated, but it's failing nevertheless.",2021-02-04T16:23:48Z
773442249,"Looks odd to me also.

EDIT: Might be an error?",2021-02-04T16:37:21Z
773447679,"I brought this up here:

https://github.com/numpy/numpy/pull/17382#discussion_r498226765",2021-02-04T16:44:56Z
773452691,"I don't mind the right alignment. But if it trips anyone, lets just undo it? Maybe one thing is that it works best only at the very top of a page and not in the middle of a long text. (Think like a chapter start in a book.)",2021-02-04T16:50:52Z
773463036,"Thanks a lot @seberg :). The issue is a bit strange actually.

Say I run a this piece of snippet:
```py
f16 = np.float128(); b = bool(); b_ = np.bool_()
f16 == b
f16 == b_
```
in this [line](https://github.com/numpy/numpy/pull/17970/files#diff-7355537178d1b6f85a40acf970ac3317b184721c75828f33ed136ea0f6c92ae9R647) in case of `bool` this happens:
```
Breakpoint 1, LONGDOUBLE_getitem (ip=0xf73f71a0, ap=0x7ffff71ad3c3 <is_anyscalar_exact+32>) at numpy/core/src/multiarray/arraytypes.c.src:430
430     {
(gdb) f 1
#1  0x00007ffff730c511 in do_richcompare_on_scalars (self=0x7ffff5ed1e70, other=0x7ffff73f71a0 <_PyArrayScalar_BoolValues>, cmp_op=2) at numpy/core/src/umath/scalarmath.c.src:647
647                 item_self = self_descr->f->getitem(data_self, NULL);
```

Although we call getitem with NULL, ap has some value. Like how is that even possible :) 
but for `np.bool_`:
```
Program received signal SIGSEGV, Segmentation fault.
0x00007ffff704fce8 in PyArray_DESCR (arr=0x0) at numpy/core/include/numpy/ndarraytypes.h:1536
1536        return ((PyArrayObject_fields *)arr)->descr;
(gdb) f 1
#1  0x00007ffff705377b in LONGDOUBLE_getitem (ip=0x7ffff5ed1e80, ap=0x0) at numpy/core/src/multiarray/arraytypes.c.src:431
431         return PyArray_Scalar(ip, PyArray_DESCR((PyArrayObject *)ap), NULL);
(gdb) f 2
#2  0x00007ffff730c511 in do_richcompare_on_scalars (self=0x7ffff5ed1e70, other=0x7ffff73f71a0 <_PyArrayScalar_BoolValues>, cmp_op=2) at numpy/core/src/umath/scalarmath.c.src:647
647                 item_self = self_descr->f->getitem(data_self, NULL);
```

Naturally we hit a null pointer dereference and crash. What is super weird is that off all the combinations of all dataypes in numpy and python(I tried [this](https://gist.github.com/ganesh-k13/6201227c5d3d65902c6eaf71357e72b1#file-comparision-py)), only a `float128` with `np.bool_` gives this error and that line has nothing to do with the other operand.

Ref: Only this line of all lines crashed to be precise: https://gist.github.com/ganesh-k13/6201227c5d3d65902c6eaf71357e72b1#file-comparision-py-L127

Any pointers on this would be very helpful.
",2021-02-04T17:06:00Z
773471872,"Umm, one thing is that most of *our* `getitem` functions do have this line `if ((ap == NULL) || PyArray_ISBEHAVED_RO(ap)) {` and I think there is some internal code that assumes passing `NULL` will work for basic/naive dtypes.

Honestly, while calling with `NULL` should not be done (especially outside of NumPy), I think this is ""by design"" that we do call it with `NULL` for basic dtypes in a few places. This happens to be one of them, and I guess before that path was never taken for `longdouble`.",2021-02-04T17:18:46Z
773475430,"I honestly do not think there is currently a way to do that. It might be possible to add one. The only way I would have thought of is  keeping `copyswap` and/or `copyswapn` to be NULL. But since you implemented `copyswap`, I assume you need it.  If copying already doesn't work (for these datatypes containing flexible data), you would be lucky, and just dropping the flags seems perfect.
One thing I noticed glancing at the code is that it looks like the function probably registers a new dtype every time (NumPy appears to have a guard against this, but only if the passed pointer remains identical, the reason is that NumPy expects you to pass in a `static` struct as `descr`. If you call it more than once, it might make sense to make the descriptor `static ... = NULL;` and just return it after the first call?)

Let me know if I can help you out now or in the future. DTypes will be revamped hopefully by the 1.21 release, which should give you cleaner solutions in the future (if you are still using this approach), but I guess that would be a bit of a long haul.

For now: We are planning on a 1.20.1 release probably this weekend already to fix some simple, but annoying issues. If it will help you, I don't need much prodding to be convinced to just remove this error message again! :)",2021-02-04T17:24:17Z
773489547,"Anecdotally I've found it ~6x faster to manually loop rather than use add.at (not a completely elementwise loop)

",2021-02-04T17:46:22Z
773498730,"> Anyone knows what's going with the PyPy tests?

It uses a nightly build since there is not an official release for win64 yet. Let's see if this repeats.",2021-02-04T18:01:21Z
773499677,"Hey, thanks for getting back to me! Your explanation makes a lot of sense. 

I see what the code is doing now, and it makes sense. I guess I just find it surprising that if you specify the bin width method, the produced bin width is not what the formula in the documentation. However, if the code is behaving as expected I guess it's best to close the issue.",2021-02-04T18:02:47Z
773509133,@eric-wieser Excellent idea. Looks like it worked: https://18129-908607-gh.circle-artifacts.com/0/doc/build/html/reference/routines.polynomials.package.html#module-numpy.polynomial,2021-02-04T18:18:21Z
773520766,"@mattip, just checked the `numpy==1.20.0` with both Python 3.8 and 3.9 after today's Windows patches - the issue has gone.",2021-02-04T18:38:00Z
773523234,"Nice and subtle: We support `datetime64 + integer`, but not `datetime64 + float` (at this time). But because `assert_allclose` defaults to `atol=0` (as an integer), things do work out in that case (and in that special case are somewhat reasonable, although the integer nature of datetime64 makes it slightly weird, its not all that weird).

The other functions who use this trick (linspace, cov, corrcoef, average, histogram) that I have found, seem to not support timedelta to begin with. So I am going for the minimal fix of just adding a special case to let timedelta pass. EDIT: Also `np.testing.assert_array_almost_equal` is unaffected.",2021-02-04T18:42:10Z
773525744,@yaav Thanks for the update. I'll close this now. Feel free to reopen if the problem returns.,2021-02-04T18:46:17Z
773527948,#17373 actually fixed this as well.,2021-02-04T18:49:47Z
773538141,"@BvB93 do we have a nice place in `np.typing` to put things like `np.typing.Float64` there? Such a rename would be the simplest solution...

DTypes probably should/could be proper heap-types as well, which would probably allow to fix this more easily, but it a swath to wade through (In some future, we probably want everything to be heap types anyway, because it should make certain things cleaner, such as HPy support).

EDIT: I guess a top-level name wouldn't solve the issue for `rational` and other user DTypes, although they are probably of niche significance here.",2021-02-04T19:07:20Z
773542026,Thanks @mckib2 ,2021-02-04T19:14:05Z
773546539,"It may be worth documenting this somewhere. A sentence like ""The actual number of bins is always chosen to divide the range into an integer number of bins that is at least as large as the estimate."", or something to that effect in `histogram`. Would you like to open a PR to include that, or would you like me to do it?",2021-02-04T19:21:29Z
773548226,"@seberg Even then, centered would make sense, left-aligned would look good, but right-aligned is weird. I think it's legible either way, but I'll leave the issue open until the maintainers come to a consensus. ",2021-02-04T19:24:29Z
773557222,I can give that a go - thanks!,2021-02-04T19:40:45Z
773569698,"I honestly think chapter beginning/top of the page is simply artistic freedom. Of the few books I looked into (only 4-5 or so were a bit more fancy styled): one used right alignment (incidentally, one of the most styled ones), one used ""weird"" alignment (left, but multi-line was heavily indented), one did have at least an exception to left alignment in the index (right aligned).
Sure most were left alignment, but most were not really styled anyway. Center seemed only/mainly a thing if the chapters were just numbers; which feels right to me). EDIT: OK, maybe a weird sample, some other humanities books use a lot of centered :).

Admittedly, the fancier ones usually offset the title by more than just bold text, e.g. a different background, which is a bit more like a website header.

I don't want to step on Ben's feet, so happy to be corrected, but I doubt you will get much more opinion/consensus. Most of us probably don't care much: so a few slight nudges toward changing it to left alignment is enough for me.

I just mention the ""top vs. inline"", because for me the fact that this is not limited to the top is enough to overcome my ""when in doubt, stick with the status-quo"" preference; simply because my working hypothesis is that Ben probably mainly looked at that when styling :).",2021-02-04T20:02:20Z
773574080,"OK, I did a bit of debugging and it turns out it was simply a bug (or rather two bugs) that caused my code to be stuck, therefore causing a build timeout.

The `git log` output for merge commits was different than I expected (it didn't contain the added/changed file names), so parsing of the log went wrong.

Now I've added the `--first-parent` option (https://github.com/mgeier/sphinx-last-updated-by-git/pull/25/commits/ca08b58a28d5f4664469d03ae3c06eb4aecfec9b) which does include the file names as expected.

This means that the ""last changed"" date will actually be the date of the merge commit and not the individual commit within the merged branch. In other words, it will be the ""change accepted date"" rather than the ""change authored date"".

I hope this is not a dealbreaker!

If you think using the ""change authored date"" would be much better, we would have to find appropriate `git log` options that allow reliably parsing this information.

The second bug was much harder to find, because it didn't happen locally on my computer.
It turns out that CircleCI runs git 2.20.1, which seems to have different default behavior to my local git 2.30.0!
I found out (more or less by accident) that adding `-m` to the options makes the old version do the same as the new version (and I think there the flag has no effect).
See also https://github.com/mgeier/sphinx-last-updated-by-git/pull/25/commits/87685c65bc1561d56baaf31988ea811526d48b18.

In case future git versions change again (or someone uses an even older git version), I've added some `assert` statements that now hopefully raise an error instead of blocking further execution.

And now, there is finally a rendered result (with different ""last updated"" info on each page!): https://18137-908607-gh.circle-artifacts.com/0/doc/build/html/index.html

> All the pages in `generated` are built from the numpy docstrings, so you could skip those.

That would be an option, but now it turns out that the `git` calls only take a few seconds all together, so this will not be necessary.

BTW, these auto-generated files get no ""last updated"" time anyway (as long as they don't have any dependencies known by Sphinx, which are in turn under version control).

In the case of auto-generated files, `git ls-files` is called once per directory to check whether files are controlled by Git or not. This takes about 15 to 20 milliseconds per directory.",2021-02-04T20:10:11Z
773596329,"The attempt at a 64-bit Cygwin CI run using GitHub actions is up at #18330.  Setting up the Cygwin environment seems to be working fine, it looks like just the running the tests bit that's having trouble.  I'm hoping someone here has experience with setting up that side of CI and can help get that working.",2021-02-04T20:51:45Z
773603521,"I suppose I should mention: `python runtests.py` gets through the build process fine, and I'm assuming it starts the testing process because it breaks when trying to import `numpy.linalg` because it can't find `lapack_lite` or `_umath_linalg`.  I'm assuming the other CI has a way of setting things up so those modules are found, but I'm not sure what that is.

I've been using `python runtests.py` from the NumPy root directory for testing on Cygwin for a while now without issues at this point.  I'm not sure what's going on here, and have not been able to reproduce this locally.",2021-02-04T21:06:14Z
773607870,"Oops, I just realized that there is actually a simple work around that seems to be sufficient and only rely on the existence of `dtype.type`, which is not a very strong requirement.",2021-02-04T21:14:32Z
773615324,"I just realized that there is after all a simple solution to this. Sorry about this, we will have a fix in 1.20.1 I think, and that is currently planned to be released by next week.",2021-02-04T21:28:54Z
773635254,"Is this the same text used to skip CI on travis and/or Azure and/or shippable? It would be nice to have a github-specific skip phrase as well, so we could select which CI service to skip. We should also document the skip phrases somewhere. Where would be a better place than googling it each time?",2021-02-04T22:08:17Z
773635451,"@jakirkham would you be able to check if gh-18332 works for you? It will allow pickling the class, but I am starting to expect the class is just part of a more elaborate scheme, and I am not sure that pans out with the fix?",2021-02-04T22:08:43Z
773658668,"@seberg just to double check: https://github.com/numpy/numpy/pull/18332 will already resolve the issue, right?",2021-02-04T22:59:03Z
773660150,"Ofc, thanks for working on this Sebastian 

Should I rebuild from source to try or do you think I can just apply that change to an existing build of NumPy 1.20?",2021-02-04T23:02:36Z
773664499,"@BvB93, yeah, thanks for looking.  I had not thought of this approach/realized it works for static ""builtin"" types.

I am not certain it will solve all issues around this, since joblib (and similar) might do more than just pickle the class (for one try to actually instantiate it with some arguments ;)... outrageous, I know!).",2021-02-04T23:12:44Z
773664667,"No need to rebuild, its just a pure python change.  Thanks!",2021-02-04T23:13:09Z
773766979,"Ohh I see, yeah in other places, we are not calling getitem in a way we are calling here. I am experimenting with a few NumPy c-api for getting the scalar values instead of just calling getitem. I think one of them will work eventually, I'll keep you updated. Thanks.",2021-02-05T03:44:51Z
773789721,"Just a FYI in joblib, we decided to change slightly the way our hashing work to avoid the problem: https://github.com/joblib/joblib/pull/1136.

Just trying to be more explicit: if the only complaint about this problem was from joblib then one option could be to not fix this issue. Reading the comments above, it seems that Dask may be affected by this from https://github.com/dask/dask/issues/7170 (but not 100% clear yet)",2021-02-05T05:02:17Z
773790267,"Looks good! Thanks again for the quick fix Sebastian  

```python
In [1]: import pickle
   ...: import numpy as np

In [2]: t = type(np.arange(10, dtype=""int64"").dtype)

In [3]: pickle.loads(pickle.dumps(t))
Out[3]: numpy.dtype[int64]
```",2021-02-05T05:03:52Z
773802674,"@seberg We now have a fix for our issue; see the diff [here](https://git.ligo.org/lscsoft/lalsuite/-/merge_requests/1526/diffs). Basically we didn't need the `NPY_ITEM_IS_POINTER` flag, as you suggested - I must have put that in originally out of paranoia - so we just needed to adapt to using `PyArray_DescrNewFromType()` instead of a static struct.

I don't remember where `copyswap` is used, it may be this only works for basic types e.g. views of `double` or structs without pointers.

The dtypes are static in that they're stored in a lookup table against a static list of type information generated by SWIG. So when a user wants to view an array of elements `MyType*`, the SWIG wrapper code provides type information for `MyType*`, which is then used as a key to find a matching NumPy dtype - or create one if none exists yet.

Thanks again for your help!",2021-02-05T05:38:18Z
773813752,Thanks @isuruf ,2021-02-05T06:05:51Z
773906129,"> Is this the same text used to skip CI on travis and/or Azure and/or shippable? It would be nice to have a github-specific skip phrase as well, so we could select which CI service to skip.

Yes, this PR contains a check for `[skip github]` which is specific.",2021-02-05T09:20:42Z
773906652,"> We should also document the skip phrases somewhere. Where would be a better place than googling it each time?

Good idea. How about https://numpy.org/devdocs/dev/development_workflow.html?",2021-02-05T09:21:41Z
773955629,First-parent sounds like the right choice to me.,2021-02-05T10:48:12Z
773962009,"@seiko2plus , @Qiyu8 I have pushed updates.",2021-02-05T11:00:35Z
773985168,"OK I understand that you won't fix it, but *at least please don't claim in the [documentation](https://numpy.org/doc/stable/reference/generated/numpy.testing.assert_allclose.html#numpy.testing.assert_allclose) that they're equivalent!* I know the documentation has a disclaimer that says ""note that allclose has different default values"", but the sentence ""The test is equivalent to allclose(actual, desired, rtol, atol)"" is very confusing.",2021-02-05T11:48:10Z
773994213,"The pypy37 build via setup-python succeeded, once again seeing the compiler error on the ""full"" CI run:
```
numpy/core/src/multiarray/einsum_sumprod.c.src: In function longdouble_sum_of_products_contig_three:
numpy/core/src/multiarray/einsum_sumprod.c.src:1264:1: internal compiler error: in coverage_begin_function, at coverage.c:656
 1264 | }
      | ^
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-9/README.Bugs> for instructions.
```

@seiko2plus is there some bad interaction between SIMD and coverage?",2021-02-05T12:07:54Z
773998268,"Documenting it either in [Asking for your changes to be merged](https://numpy.org/devdocs/dev/development_workflow.html#asking-for-your-changes-to-be-merged-with-the-main-repo) or [Additional things ...](https://numpy.org/devdocs/dev/development_workflow.html#additional-things-you-might-want-to-do) sounds good, with a small preference for the former. Would you like to add that here or in a separate PR?",2021-02-05T12:16:41Z
774034586,"> First-parent sounds like the right choice to me.

Probably, I'm not sure.

I think arguments can be made for and against it.

Initially I liked the idea of using the ""PR accepted date"", but I see two limitations with that:

* if a project uses a `git rebase` workflow, the merge date will not be visible (but we could change from ""author date"" to ""commiter date"", then it would be fine). The date of fast-forward merges would obviously not be visible, because they don't change anything on the commits. In other words, the date is inconsistent between different merging strategies (which may be used within one project).
* When looking at a source file on Github, it often shows a different date under ""latest commit"", e.g.
  * Oct 27, 2020: https://18137-908607-gh.circle-artifacts.com/0/doc/build/html/user/basics.rec.html
  * Oct 26, 2020: https://github.com/numpy/numpy/blob/master/doc/source/user/basics.rec.rst

For these reasons I think it would be better to *not* use `--first-parent`.

I initially used `--first-parent` because it was the only setting that created properly parse-able output, but in the meantime I found out that the `-m` flag (which I needed for old `git` versions anyway) actually leads to the output I was initially expecting.

@eric-wieser What do you think?

I now get the log output I want, and the ""build devdocs w/ref warnings"" finishes successfully (now even with status output from my extension!), but the ""build devdocs"" step times out.

I'm not sure whether that's caused by my extension, because the generated HTML artifact files (even though the CSS seems to be missing) *do* contain different dates on each one.",2021-02-05T13:31:35Z
774034929,"> Documenting it either in Asking for your changes to be merged or Additional things ... sounds good, with a small preference for the former. Would you like to add that here or in a separate PR?

Agree, I can put it there. Prefer a follow-up PR. This is actively annoying me with failed builds whenever I push any commit to my own fork, so I'd like to get it in as is.",2021-02-05T13:32:15Z
774044208,Can you suggest a less confusing version?,2021-02-05T13:50:07Z
774045989,"The same error also occurs in  `numpy==1.19.5`.
Maybe I am just missing something and there is no bug?",2021-02-05T13:53:34Z
774048941,"Suggestion, maybe something like the following? 

""Due to different default parameter values, its behaviour is different to `allclose`, but they are functionally equivalent.""",2021-02-05T13:59:02Z
774075085,@pearu Thoughts?,2021-02-05T14:44:36Z
774076674,Thanks @rgommers ,2021-02-05T14:47:13Z
774082863,"> it may be necessary to copy this to each job

Appveyor is the only platform that may need copies and a yml alias would be easier to maintain than multiple long strings.",2021-02-05T14:56:21Z
774082897,"Verified it works as expected on my fork:

<img width=""678"" alt=""image"" src=""https://user-images.githubusercontent.com/98330/107049681-ae48e680-67ca-11eb-9d27-d6849da8b2c4.png"">


<img width=""378"" alt=""image"" src=""https://user-images.githubusercontent.com/98330/107049587-94a79f00-67ca-11eb-8cc4-027f86445618.png"">
",2021-02-05T14:56:26Z
774084689,"@charris what's a yml alias?

What I meant is, you can only put this under a job, so may need to repeat it multiple times in a file (example: https://github.com/scipy/scipy/blob/master/.github/workflows/linux.yml#L16) and per yml file.",2021-02-05T14:59:22Z
774089345," > what's a yml alias?

- https://support.atlassian.com/bitbucket-cloud/docs/yaml-anchors/ 
- https://github.com/cyklo/Bukkit-OtherBlocks/wiki/Aliases-(advanced-YAML-usage)

",2021-02-05T15:07:13Z
774091334,"This problem also causes the @ operator to fail, since it relies on \_\_matmul__:

```py
import numpy as np
a = np.ones((3,4))
b = np.ones((4,3))
ma = np.ma.masked_array(a, mask=np.ones(a.size))
mb = np.ma.masked_array(b, mask=np.ones(b.size))
ma @ mb
```

Result:

    Traceback (most recent call last):
      File ""<stdin>"", line 1, in <module>
      File ""/opt/anaconda3/envs/xelab/lib/python3.7/site-packages/numpy/ma/core.py"", line 3049, in __array_wrap__
        m = reduce(mask_or, [getmaskarray(arg) for arg in input_args])
      File ""/opt/anaconda3/envs/xelab/lib/python3.7/site-packages/numpy/ma/core.py"", line 1764, in mask_or
        return make_mask(umath.logical_or(m1, m2), copy=copy, shrink=shrink)
    ValueError: operands could not be broadcast together with shapes (3,4) (4,3) 
",2021-02-05T15:10:38Z
774091740,"Thanks, learned something. Likely won't work though, from the atlassian link:
```
YAML anchors and aliases cannot contain the ' [ ', ' ] ', ' { ', ' } ', and ' , ' characters.
```
So looks like you cannot encode `[ci skip]` et al.",2021-02-05T15:11:18Z
774096266,"Nit:
```
!f2py    integer intent(in): code
```
should read
```
!f2py    integer intent(in):: code
```

I cannot reproduce the failure: I pasted the Fortran code to `issue18335.f90` and run
```
$ f2py -c -m foo ~/test/f2py/issue18335.f90
```
with success using
```
$ f2py | grep Version
Version:     1.21.0.dev0+655.g2f466b318
```",2021-02-05T15:18:42Z
774106476,"Oh, thanks for checking. I am/was a bit worried that the original issue was more general and possibly did something like:
```
arg = ""f8""
dt = np.dtype(arg)
dtype_type = type(dt)

dtype_type(arg)
```
I.e. after unpickling, you would instantiate the dtype again with arguments originally meant for `np.dtype`. But that would break currently (and I am not sure I want it to work generally, since `np.dtype` is a mini-language basically).",2021-02-05T15:34:28Z
774111228,"@rgommers I think that refers to the variable names, not the values. Strings are strings.",2021-02-05T15:42:32Z
774112770,"OK, glad it seems to work for you. I do think that `NPY_ITEM_IS_POINTER` might have bought you some small things for the structs with dynamic data (and its techncially more correct there).  Basically, it probably would disallow a few things that cannot work in any case. By no means would it protect you from all the ways it could go wrong, though.
Unfortunately that may just be impossible right now, but maybe we can give you new API in the future!",2021-02-05T15:44:58Z
774119895,"```
aliases:
    - &test ""[hi]""

hi: &test
```
Passed a yaml validator.    ",2021-02-05T15:56:29Z
774139589,"No worries. At least in our use case, we created an array first

I know I have written and seen code like `np.dtype(arg).type`. Though that is a bit different than what we have here. Also I don't think I've seen the result be pickled in that case. That said, this case does appear to pickle correctly

```python
In [1]: import pickle
   ...: import numpy as np

In [2]: pickle.loads(pickle.dumps(np.dtype(""int64"").type))
Out[2]: numpy.int64
```",2021-02-05T16:29:49Z
774142808,@seberg are you happy with this now?,2021-02-05T16:35:14Z
774143459,"Yeah, pickling the `dtype.type` is totally fine (in fact, it is the solution here at the moment).  `dtype.type` is the scalar type though which is different from `type(dtype)` its a bit confusing :).",2021-02-05T16:36:18Z
774144409,Does this need a backport? ,2021-02-05T16:38:02Z
774145034,"@pearu Thanks for looking at this.

Yes, I realised the missing `::`.  My apologies.
But it does not change the outcome for me.  Still getting same error message on compile.

I was trying to grep the f2py code, but could not spot an obvious place where there might be a confusion between `signed_char` and `signed char`.  Even if the statement in `!f2py` had been wrong, it still should not produce C code with wrong data types, I would say.  So something deeper must be going wrong.

Yes, suspected this will not be easy to reproduce, due to the odd interaction - the compile error is in a different subroutine than the one that was changed.  Which seems odd enough to me.

```python
subroutine pyexit(code)

  implicit none
  integer(kind=4), intent(IN) :: code

  !f2py    integer intent(in) :: code
  !f2py    intent(callback, hide) :: endkepler()
  external endkepler

  call endkepler()

end subroutine pyexit


!=======================================================================

subroutine pygets(ttymsg)

  implicit none
  character*(*), intent(inout) :: ttymsg

  integer, parameter :: n = 132
  integer :: i
  integer(kind=1), dimension(n) :: data

  !f2py    intent(callback, hide) :: ttykepler(data)
  external ttykepler

  call ttykepler(data)

  do i = 1, min(n, len(ttymsg))
     ttymsg(i:i) = char(data(i))
  end do
end subroutine pygets
```
still getting 

```
....
/tmp/tmprjo5dtlj/src.linux-x86_64-3.9/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHzmodule.c: At top level:
/tmp/tmprjo5dtlj/src.linux-x86_64-3.9/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHzmodule.c:29:63: error: unknown type name signed_char; did you mean signed char?
   29 | typedef void(*cb_ttykepler_in_pygets__user__routines_typedef)(signed_char *);
      |                                                               ^~~~~~~~~~~
      |                                                               signed char

...

/tmp/tmprjo5dtlj/src.linux-x86_64-3.9/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHzmodule.c: At top level:
/tmp/tmprjo5dtlj/src.linux-x86_64-3.9/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHzmodule.c:831:42: error: unknown type name signed_char; did you mean signed char?
      |                                          ^~~~~~~~~~~
      |                                          signed char
...
```",2021-02-05T16:39:08Z
774147317,"Yes, I think it should be good; if someone else with a pickle mind has a quick look, that would be great, though :).",2021-02-05T16:43:07Z
774147660,"> the ""build devdocs"" step times out.

I have re-based this PR and now CircleCI succeeds, hooray!

Rendered: https://18162-908607-gh.circle-artifacts.com/0/doc/build/html/index.html

I guess the remaining CI failures are unrelated, right?",2021-02-05T16:43:43Z
774151168,"I tried devel version
```
1.21.0.dev0+655.g2f466b318 3.9.1 (default, Dec 16 2020, 01:02:01) 
[GCC 10.2.1 20201125 (Red Hat 10.2.1-9)]
```
but same result.",2021-02-05T16:48:59Z
774151245,"@charris No, no back port needed. The win64 PyPy run was added after the 1.20 branch, and the Linux one works the same, just cleaner.",2021-02-05T16:49:06Z
774152223,"BTW, the reason why I tried this change kin the first place is because with the argument in the callback I get a segfault with numpy 1.20 that does not occur with 1.19.5.  I will file separate report.",2021-02-05T16:50:57Z
774152274,In it goes. Thanks Matti.,2021-02-05T16:51:03Z
774157871,"@2sn can you send the full output of the `f2py` process? It is strange that the `code` argument is typed as `signed_char`, could you try a rename `code` -> `foo`?",2021-02-05T17:00:28Z
774159874,Thanks Sebastian.,2021-02-05T17:03:34Z
774172524,"Yes, I had tried renaming, no change.
What is strange is that when I pass it on to the `endkepler` call it complies (it still causes segfault later in numpy 1.20+)
The output is rather long, as there is a lot more than what I sent.

<details> <summary> Output </summary>

```
In [3]: k = K('xxx', 'z')
 [DEBUG] /home/alex/python/source/kepler/code/kepler.f90 newer than /home/alex/python/source/kepler/code/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz.cpython-39-x86_64-linux-gnu.so.
        Reading file 'kepler.f90' (format:free)
analyzeline: Creating module block '_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz'
analyzeline: Creating additional interface block (groupcounter=2).
Line #15 in kepler.f90:""  nargs = 1 ""
        crackline:3: No pattern for line
Line #16 in kepler.f90:""  inputarg(0) = 'python' ""
        crackline:3: No pattern for line
Line #18 in kepler.f90:""  inputarg(1) = '-v' ""
        crackline:3: No pattern for line
Line #43 in kepler.f90:""  DO i=0,20 ""
        crackline:3: No pattern for line
Line #44 in kepler.f90:""     DO j=1,80 ""
        crackline:3: No pattern for line
Line #45 in kepler.f90:""        inputarg(i)(j:j) = input(j+i*80) ""
        crackline:3: No pattern for line
Line #47 in kepler.f90:""     inputarg(i) = trim(inputarg(i)) ""
        crackline:3: No pattern for line
Line #63 in kepler.f90:""  write(6, ""(A)"") ' [PYTHON]  setting up kepler ...' ""
        crackline:3: No pattern for line
        Reading file '/home/alex/kepler/source/typecom' (format:fix)
        Reading file '/home/alex/python/source/kepler/code/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz/gridcom' (format:fix)
Line #131 in kepler.f90:""  name = trim(namedat) ""
        crackline:3: No pattern for line
Line #144 in kepler.f90:""           intent(callback, hide) :: endkepler()""
        analyzeline: missing __user__ module (could be nothing)
Line #144 in kepler.f90:""           intent(callback, hide) :: endkepler()""
        analyzeline: appending intent(callback) endkepler to pyexit arguments
analyzeline: Creating additional interface block (groupcounter=4).
Line #158 in kepler.f90:""           intent(callback, hide) :: plotkepler()""
        analyzeline: missing __user__ module (could be nothing)
Line #158 in kepler.f90:""           intent(callback, hide) :: plotkepler()""
        analyzeline: appending intent(callback) plotkepler to pyplot arguments
analyzeline: Creating additional interface block (groupcounter=4).
Line #176 in kepler.f90:""           intent(callback, hide) :: ttykepler(data)""
        analyzeline: missing __user__ module (could be nothing)
Line #176 in kepler.f90:""           intent(callback, hide) :: ttykepler(data)""
        analyzeline: appending intent(callback) ttykepler to pygets arguments
analyzeline: Creating additional interface block (groupcounter=4).
Line #181 in kepler.f90:""  do i = 1, min(n, len(ttymsg)) ""
        crackline:3: No pattern for line
Line #182 in kepler.f90:""     ttymsg(i:i) = char(data(i)) ""
        crackline:3: No pattern for line
        Reading file '/home/alex/kepler/source/typecom' (format:fix)
        Reading file '/home/alex/python/source/kepler/code/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz/gridcom' (format:fix)
{'before': '', 'this': 'use', 'after': ':: ieeevals, only : ieee_nans '}
Line #215 in kepler.f90:""  use :: ieeevals, only : ieee_nans ""
        analyzeline: Could not crack the use statement.
        Reading file '/home/alex/kepler/source/typecom' (format:fix)
        Reading file '/home/alex/python/source/kepler/code/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz/gridcom' (format:fix)
Line #254 in kepler.f90:""  do i=1,m ""
        crackline:3: No pattern for line
Line #255 in kepler.f90:""     do j=1,n ""
        crackline:3: No pattern for line
Line #256 in kepler.f90:""        tn = tem(i) ""
        crackline:3: No pattern for line
Line #257 in kepler.f90:""        dn = den(j) ""
        crackline:3: No pattern for line
Line #266 in kepler.f90:""        do k=1,l ""
        crackline:3: No pattern for line
Line #267 in kepler.f90:""           select case (idata(k)) ""
        crackline:3: No pattern for line
Line #268 in kepler.f90:""           case (0) ""
        crackline:3: No pattern for line
Line #270 in kepler.f90:""           case (1) ""
        crackline:3: No pattern for line
Line #272 in kepler.f90:""           case (2) ""
        crackline:3: No pattern for line
Line #274 in kepler.f90:""           case (3) ""
        crackline:3: No pattern for line
Line #276 in kepler.f90:""           case (4) ""
        crackline:3: No pattern for line
Line #278 in kepler.f90:""           case (5) ""
        crackline:3: No pattern for line
Line #280 in kepler.f90:""           case (6) ""
        crackline:3: No pattern for line
Line #282 in kepler.f90:""           case (7) ""
        crackline:3: No pattern for line
Line #284 in kepler.f90:""           case (8) ""
        crackline:3: No pattern for line
Line #286 in kepler.f90:""           case (9) ""
        crackline:3: No pattern for line
Line #288 in kepler.f90:""           case (10) ""
        crackline:3: No pattern for line
Line #290 in kepler.f90:""           case (11) ""
        crackline:3: No pattern for line
Line #292 in kepler.f90:""           case (12) ""
        crackline:3: No pattern for line
Line #294 in kepler.f90:""           case (13) ""
        crackline:3: No pattern for line
Line #295 in kepler.f90:""              ga=dn/ptot*(pt/et*(ptot/dn**2-ed)+pd)  ""
        crackline:3: No pattern for line
Line #296 in kepler.f90:""              fa=(ptot-ed*dn**2)/(et*tn*ga*dn)  ""
        crackline:3: No pattern for line
Line #298 in kepler.f90:""           case (14) ""
        crackline:3: No pattern for line
Line #300 in kepler.f90:""           case default ""
        crackline:3: No pattern for line
        Reading file '/home/alex/kepler/source/typecom' (format:fix)
        Reading file '/home/alex/python/source/kepler/code/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz/gridcom' (format:fix)
        Reading file '/home/alex/kepler/source/typecom' (format:fix)
        Reading file '/home/alex/python/source/kepler/code/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz/gridcom' (format:fix)
        Reading file '/home/alex/kepler/source/typecom' (format:fix)
        Reading file '/home/alex/python/source/kepler/code/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz/gridcom' (format:fix)
        Reading file '/home/alex/kepler/source/typecom' (format:fix)
        Reading file '/home/alex/python/source/kepler/code/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz/gridcom' (format:fix)
        Reading file '/home/alex/kepler/source/typecom' (format:fix)
        Reading file '/home/alex/python/source/kepler/code/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz/gridcom' (format:fix)
        Reading file 'data.f' (format:fix,strict)
Line #4 in data.f:""      module data""
        crackline:2: No pattern for line
        Reading file '/home/alex/kepler/source/kepcom' (format:fix)
        Reading file '/home/alex/kepler/source/typecom' (format:fix)
        Reading file '/home/alex/python/source/kepler/code/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz/gridcom' (format:fix)
        Reading file '/home/alex/kepler/source/burncom' (format:fix)
        Reading file '/home/alex/python/source/kepler/code/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz/nburncom' (format:fix)
        Reading file '/home/alex/kepler/source/logcom' (format:fix)
        Reading file '/home/alex/kepler/source/sysdef1com' (format:fix)
        Reading file '/home/alex/kepler/source/zburncom' (format:fix)
        Reading file '/home/alex/kepler/source/uuidcom' (format:fix)
Line #2071 in /home/alex/kepler/source/kepcom:""      equivalence      (pdumx(0),ipdum(0)) ""
        crackline:2: No pattern for line
Line #2078 in /home/alex/kepler/source/kepcom:""      equivalence      (pdumx(0),pdum) ""
        crackline:2: No pattern for line
Line #2101 in /home/alex/kepler/source/kepcom:""      equivalence  (maxit,zmaxit),       (nstop,znstop),       (nedit,znedit),  (ndump,zndump),       (ipup,zipup),         (iwinsize,ziwinsiz),  (lenqmax,zlenqmax),   (iwinloc,ziwinloc),   (maxbak,zmaxbak),  (npflag,znpflag),     (irnet,zirnet),       (npixedit,znpixedi),  (iytsflag,ziytsfla),  (jmmin,zjmmin),       (izonef,zizonef),  (idzonef,zidzonef),   (jshell0,zjshell0),   (jshell1,zjshell1),  (jqse,zjqse),         (jartv1,zjartv1),     (jartv2,zjartv2),  (jartv3,zjartv3),     (ipixtype,zipixtyp),  (nplotsym,znplotsy),  (igridt,zigridt),     (igridm,zigridm),     (igridv,zigridv),  (jp0,zjp0),           (jp1,zjp1) ""
        crackline:2: No pattern for line
Line #2124 in /home/alex/kepler/source/kepcom:""      equivalence  (itvstart,zitvstar),  (ivplot,zivplot),     (irtype,zirtype),  (iudflag,ziudflag),   (nsdump,znsdump),     (niterbar,zniterba),  (iterbarm,ziterbam),  (iautoout,ziautoou),  (iflgabar,ziflgaba),  (jmcalc,zjmcalc),     (iupdflag,ziupdfla),  (niondump,zniondum),  (nisodump,znisodum),  (nzondump,znzondum),  (iflagye,ziflagye),  (njeditq,znjeditq),   (iexciteh,ziexcite),  (iterqmh,ziterqmh),  (neditq,zneditq),     (iter1qe,ziter1qe),   (jnse,zjnse),  (jlcalc,zjlcalc),     (jpause,zjpause),     (nnewoutf,znnewout),  (neditq1,zneditq1),   (isi30brn,zisi30br),  (iqerrflg,ziqerrfl),  (noqsecon,znoqseco),  (irzopt,zirzopt),     (nsurfz,znsurfz) ""
        crackline:2: No pattern for line
Line #2147 in /home/alex/kepler/source/kepcom:""      equivalence  (mlcompf,zmlcompf),   (nupdate,znupdate),   (netmax,znetmax),  (neditb,zneditb),     (nedita,znedita),     (jmeditb,zjmeditb),  (neditall,znedital),  (ncomp,zncomp),       (ncent,zncent),  (nedt,znedt),         (nzro,znzro),         (ninv,zninv),  (nbkup,znbkup),       (ntty,zntty),         (ileqs,zileqs),  (negbkup,znegbkup),   (nbkupmax,znbkupma),  (nsubcycm,znsubcyc),  (mazful,zmazful),     (inburn,zinburn),     (npage,znpage),  (nsavez,znsavez),     (nsaveb,znsaveb),     (neditz1,zneditz1),  (neditz2,zneditz2),   (medit,zmedit),       (meditfin,zmeditfi),  (neditp,zneditp),     (newflam,znewflam),   (noiland,znoiland) ""
        crackline:2: No pattern for line
Line #2174 in /home/alex/kepler/source/kepcom:""      equivalence  (ibackgnd,zibackgn),  (ngridmin,zngridmi),  (ncycqq,zncycqq),  (lentrack,zlentrac),  (idtmaxl,zidtmaxl),   (idtlook,zidtlook),  (izonezms,zizonezm),  (numiso,znumiso),     (maptime,zmaptime),  (icalcne,zicalcne),   (ionflag,zionflag),   (nfirstq,znfirstq),  (iburnye,ziburnye),   (nangmix,znangmix),   (ncnvout,zncnvout),  (kaptab,zkaptab),     (nangsmg,znangsmg),   (ipapsize,zipapsiz),  (iold,ziold),         (nwndout,znwndout),   (kapverb,zkapverb),  (llimout,zllimout),   (nenout,znenout),     (iproyld,ziproyld),  (ipromin,zipromin),   (ipromax,zipromax),   (iprownd,ziprownd),  (minapro,zminapro),   (maxapro,zmaxapro),   (nenuout,znenuout),  (irecb,zirecb),       (nadapb,znadapb),     (ivspecl,zivspecl),  (ivrate,zivrate),     (magnet,zmagnet),     (nosht,znosht) ""
        crackline:2: No pattern for line
Line #2199 in /home/alex/kepler/source/kepcom:""      equivalence  (netmin,znetmin),     (nstrout,znstrout),   (mixcycl,zmixcycl),  (lburn,zlburn),       (lbbkup,zlbbkup),     (lcout,zlcout),  (iplotb,ziplotb),     (minzone,zminzone),   (levcnv,zlevcnv),  (mingain,zmingain),   (minloss,zminloss),   (minnucl,zminnucl),  (iazonef,ziazonef),   (kapburn,zkapburn),   (maxzone,zmaxzone),  (minnucg,zminnucg),   (mingaind,zmingaind), (minlossd,zminlossd),  (minnucgd,zminnucgd), (minnucld,zminnucld), (mixout,zmixout),  (irprox,zirprox),     (n14pg,zn14pg),       (ibwarn,zibwarn),  (ifallbk,zifallbk),   (nzsave,znzsave),     (icutbin,zicutbin),  (ncycr,zncycr),       (nconvers,znconvers), (nwndvers,znwndvers),  (ipdtmin,zipdtmin) ""
        crackline:2: No pattern for line
Line #2210 in /home/alex/kepler/source/kepcom:""      equivalence  (minneul,zminneul), (minneug,zminneug),  (minneuld,zminneuld), (minneugd,zminneugd),  (iwimpb,ziwimpb), (lumdata,zlumdata) ""
        crackline:2: No pattern for line
Line #2215 in /home/alex/kepler/source/kepcom:""      equivalence      (qdumx(0),iqdum(0)) ""
        crackline:2: No pattern for line
Line #2223 in /home/alex/kepler/source/kepcom:""      equivalence      (qdumx(0),qdum) ""
        crackline:2: No pattern for line
Line #2248 in /home/alex/kepler/source/kepcom:""      equivalence      (jm,zjm), (ncyc,zncyc), (iter,ziter), (idtcon,zidtcon),      (imax,zimax), (nnet,znnet), (numit,znumit), (nreac,znreac),      (itert,zitert), (jfcr,zjfcr), (jfct,zjfct), (jfcl,zjfcl),      (ncycb,zncycb), (ncycbt,zncycbt), (nburnz,znburnz),      (nburnzt,znburnzt), (imaxb,zimaxb), (nnetb,znnetb),      (numitb,znumitb), (jlm,zjlm), (jbmax,zjbmax), (jpist,zjpist),      (jpistm,zjpistm), (nye,znye), (nyem,znyem),      (idmptime,zidmptim),      (nylib,znylib), (nylib0,znylib0), (ndatq,zndatq),      (locqnext,zlocqnex), (ndatl,zndatl), (ninvl,zninvl),      (ngbkup,zngbkup), (mncbkup,zmncbkup), (nacbkup,znacbkup),      (isebkup,zisebkup), (ilastpl,zilastpl), (itimeg,zitimeg),      (itimed,zitimed), (iterbar, ziterbar) ""
        crackline:2: No pattern for line
Line #2253 in /home/alex/kepler/source/kepcom:""      equivalence      (numi,znumi) ""
        crackline:2: No pattern for line
Line #2258 in /home/alex/kepler/source/kepcom:""      equivalence      (ionn,zionn) ""
        crackline:2: No pattern for line
Line #2263 in /home/alex/kepler/source/kepcom:""      equivalence      (numib,znumib) ""
        crackline:2: No pattern for line
Line #2268 in /home/alex/kepler/source/kepcom:""      equivalence      (ionnb,zionnb) ""
        crackline:2: No pattern for line
Line #2273 in /home/alex/kepler/source/kepcom:""      equivalence      (jrate,zjrate) ""
        crackline:2: No pattern for line
Line #2278 in /home/alex/kepler/source/kepcom:""      equivalence      (jdtc,zjdtc) ""
        crackline:2: No pattern for line
Line #2283 in /home/alex/kepler/source/kepcom:""      equivalence      (locqz,zlocqz), (locqz0,zlocqz0) ""
        crackline:2: No pattern for line
Line #2288 in /home/alex/kepler/source/kepcom:""      equivalence      (ndatzed,zndatzed), (ncyczed,zncyczed) ""
        crackline:2: No pattern for line
Line #2293 in /home/alex/kepler/source/kepcom:""      equivalence      (netnum,znetnum) ""
        crackline:2: No pattern for line
Line #2299 in /home/alex/kepler/source/kepcom:""      equivalence      (netnumb,znetnumb), (limab,zlimab),      (limzb,zlimzb), (limcb,zlimcb) ""
        crackline:2: No pattern for line
Line #2322 in /home/alex/kepler/source/kepcom:""      equivalence      (lowamul,zlowamul), (ihwamul,zihwamul), (lossrot,zlossrot),      (nzpuls,znzpuls), (losseadv,zlosseadv), (ziacceadv,iacceadv),      (ziaccadv,iaccadv), (zjloss,jloss), (zjacc,jacc),      (zjaccemx,jaccemx), (zisurf,isurf), (znlogout,nlogout),      (zipnuc,ipnuc), (zipnu,ipnu), (zidecmode,idecmode),      (zjmdec,jmdec), (znlog,nlog), (znoparm,noparm),      (znsekout,nsekout), (ziadapv,iadapv), (zittyv,ittyv),      (zinuenc,inuenc), (zinuebnc,inuebnc), (zihe4cc,ihe4cc),      (zibdatov,ibdatov), (znentout,nentout), (znentlev,nentlev),      (nsetparm,setparm), (znangdis,nangdis), (ziccrate,iccrate),      (znidecay,nidecay), (znnuout,nnuout), (zjslosse,jslosse),      (zloczone,loczone), (ziaccunit,iaccunit), (zir1212,ir1212),      (znang3d,nang3d), (znrotout,nrotout) ""
        crackline:2: No pattern for line
Line #2330 in /home/alex/kepler/source/kepcom:""      equivalence      (odum,iodum) ""
        crackline:2: No pattern for line
Line #2338 in /home/alex/kepler/source/kepcom:""      equivalence      (igridbuf,gridbuf) ""
        crackline:2: No pattern for line
Line #2346 in /home/alex/kepler/source/kepcom:""      equivalence      (yp00,snold), (ypbtp,snbt), (ypbdp,snbd),      (yn00,abarold), (ynbtp,abarnbt), (ynbdp,abarnbd) ""
        crackline:2: No pattern for line
Line #2351 in /home/alex/kepler/source/kepcom:""      equivalence      (hdumx,ihdum) ""
        crackline:2: No pattern for line
Line #2358 in /home/alex/kepler/source/kepcom:""      equivalence      (hdumx, hdum) ""
        crackline:2: No pattern for line
Line #2363 in /home/alex/kepler/source/kepcom:""      equivalence      (cdumx,cdum1) ""
        crackline:2: No pattern for line
Line #2371 in /home/alex/kepler/source/kepcom:""      equivalence      (cdumx,cdum) ""
        crackline:2: No pattern for line
Line #2376 in /home/alex/kepler/source/kepcom:""      equivalence      (sdumx,isdum) ""
        crackline:2: No pattern for line
Line #2383 in /home/alex/kepler/source/kepcom:""      equivalence      (sdumx,sdum) ""
        crackline:2: No pattern for line
Line #2388 in /home/alex/kepler/source/kepcom:""      equivalence      (zdumx(1),izdum(1)) ""
        crackline:2: No pattern for line
Line #2393 in /home/alex/kepler/source/kepcom:""      equivalence      (zdumx(1), zdum(1)) ""
        crackline:2: No pattern for line
Line #2398 in /home/alex/kepler/source/kepcom:""      equivalence      (bdumx(1), bdum(1)) ""
        crackline:2: No pattern for line
Line #2403 in /home/alex/kepler/source/kepcom:""      equivalence      (bdumx(1),ibdum(1)) ""
        crackline:2: No pattern for line
Line #2414 in /home/alex/kepler/source/kepcom:""      equivalence      (angjx(0),angj(0,1)),      (angjy(0),angj(0,2)),      (angjz(0),angj(0,3)),      (angwx(0),angw(0,1)),      (angwy(0),angw(0,2)),      (angwz(0),angw(0,3)) ""
        crackline:2: No pattern for line
Line #2423 in /home/alex/kepler/source/kepcom:""      equivalence      (angltv(1),angltx),      (anglossv(1),anglossx),      (anglwndv(1),anglwndx) ""
        crackline:2: No pattern for line
Line #2428 in /home/alex/kepler/source/kepcom:""      equivalence      (edumx, edum) ""
        crackline:2: No pattern for line
Line #15 in data.f:""      save""
        crackline:2: No pattern for line
crackline: groupcounter=1 groupname={0: '', 1: 'module', 2: 'interface', 3: 'subroutine', 4: 'interface', 5: 'subroutine'}
crackline: Mismatch of blocks encountered. Trying to fix it by assuming ""end"" statement.
Post-processing...
        Block: _kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:unknown_interface
buildimplicitrules: no implicit rules for routine 'unknown_interface'.
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:unknown_interface
buildimplicitrules: no implicit rules for routine 'unknown_interface'.
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:unknown_interface
get_parameters: got ""name 'selectedchar' is not defined"" on 'selectedchar'
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:unknown_interface
get_parameters: got ""EOL while scanning string literal (<string>, line 1)"" on 'selectedcharkind(""iso'
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:unknown_interface
get_parameters: got ""invalid syntax (<string>, line 1)"" on '6.02254d+23'
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:unknown_interface
get_parameters: got ""invalid syntax (<string>, line 1)"" on '1.38054d-16'
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:unknown_interface
get_parameters: got ""invalid syntax (<string>, line 1)"" on '9.10908d-28'
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:unknown_interface
get_parameters: got ""invalid syntax (<string>, line 1)"" on '6.65205d-25'
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:unknown_interface
get_parameters: got ""invalid syntax (<string>, line 1)"" on '7.5648d-15'
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:unknown_interface
get_parameters: got ""invalid syntax (<string>, line 1)"" on '6.62559d-27'
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:unknown_interface
get_parameters: got ""invalid syntax (<string>, line 1)"" on '6.670d-08'
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:unknown_interface
get_parameters: got ""invalid syntax (<string>, line 1)"" on '2.997925d+10'
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:unknown_interface
get_parameters: got ""invalid syntax (<string>, line 1)"" on '3.1415926536d+0'
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:unknown_interface
get_parameters: got ""invalid syntax (<string>, line 1)"" on '1.9892d+33'
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:unknown_interface
get_parameters: got ""invalid syntax (<string>, line 1)"" on '6.9599d+10'
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:unknown_interface
get_parameters: got ""unexpected EOF while parsing (<string>, line 1)"" on '0.782333098d0'
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:unknown_interface
get_parameters: got ""invalid syntax (<string>, line 1)"" on '3.1558d+07'
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:unknown_interface
get_parameters: got ""unexpected EOF while parsing (<string>, line 1)"" on '83143573.716d0'
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:unknown_interface
get_parameters: got ""invalid syntax (<string>, line 1)"" on '1.d0/solmass'
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:unknown_interface
get_parameters: got ""invalid syntax (<string>, line 1)"" on '1.d0/solrad'
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:unknown_interface
get_parameters: got ""invalid syntax (<string>, line 1)"" on 'pie*4.d0/3.d0'
analyzevars: character array ""character*8 namezone(55)"" is considered as ""character namezone(55,8)""; ""intent(c)"" is forced.
analyzevars: character array ""character*8 names(30)"" is considered as ""character names(30,8)""; ""intent(c)"" is forced.
analyzevars: character array ""character*10 unitzone(55)"" is considered as ""character unitzone(55,10)""; ""intent(c)"" is forced.
analyzevars: character array ""character*2 izsym(121)"" is considered as ""character izsym(121,2)""; ""intent(c)"" is forced.
analyzevars: character array ""character*8 netsym(5)"" is considered as ""character netsym(5,8)""; ""intent(c)"" is forced.
analyzevars: character array ""character*1 charset(40)"" is considered as ""character charset(40,1)""; ""intent(c)"" is forced.
analyzevars: character array ""character*1 capchar(26)"" is considered as ""character capchar(26,1)""; ""intent(c)"" is forced.
analyzevars: character array ""character*8 nameparm(583)"" is considered as ""character nameparm(583,8)""; ""intent(c)"" is forced.
analyzevars: character array ""character*8 nameqprm(160)"" is considered as ""character nameqprm(160,8)""; ""intent(c)"" is forced.
analyzevars: character array ""character*8 nameedit(91)"" is considered as ""character nameedit(91,8)""; ""intent(c)"" is forced.
analyzevars: character array ""character*10 unitedit(91)"" is considered as ""character unitedit(91,10)""; ""intent(c)"" is forced.
analyzevars: character array ""character*8 namelvar(40)"" is considered as ""character namelvar(40,8)""; ""intent(c)"" is forced.
analyzevars: character array ""character*16 nameqql(250)"" is considered as ""character nameqql(250,16)""; ""intent(c)"" is forced.
analyzevars: character array ""character*8 namedatm(8)"" is considered as ""character namedatm(8,8)""; ""intent(c)"" is forced.
analyzevars: character array ""character*48 labldatm(8)"" is considered as ""character labldatm(8,48)""; ""intent(c)"" is forced.
analyzevars: character array ""character*8 nametvar(52)"" is considered as ""character nametvar(52,8)""; ""intent(c)"" is forced.
analyzevars: character array ""character*10 unittime(52)"" is considered as ""character unittime(52,10)""; ""intent(c)"" is forced.
analyzevars: character array ""character*33 lbuf(4)"" is considered as ""character lbuf(4,33)""; ""intent(c)"" is forced.
analyzevars: character array ""character*8 ldecay(8192)"" is considered as ""character ldecay(8192,8)""; ""intent(c)"" is forced.
analyzevars: character array ""character*8 ionbmax(8192)"" is considered as ""character ionbmax(8192,8)""; ""intent(c)"" is forced.
analyzevars: character array ""character*1 lempty(8)"" is considered as ""character lempty(8,1)""; ""intent(c)"" is forced.
analyzevars: character array ""character*8 ions(100)"" is considered as ""character ions(100,8)""; ""intent(c)"" is forced.
analyzevars: character array ""character*8 ionsb(8192)"" is considered as ""character ionsb(8192,8)""; ""intent(c)"" is forced.
analyzevars: character array ""character*8 idtcsym(30)"" is considered as ""character idtcsym(30,8)""; ""intent(c)"" is forced.
analyzevars: character array ""character*8 isymr(50)"" is considered as ""character isymr(50,8)""; ""intent(c)"" is forced.
analyzevars: character array ""character*8 namedatq(250)"" is considered as ""character namedatq(250,8)""; ""intent(c)"" is forced.
analyzevars: character array ""character*48 labldatq(250)"" is considered as ""character labldatq(250,48)""; ""intent(c)"" is forced.
analyzevars: character array ""character*8 namedzed(30,10)"" is considered as ""character namedzed(30,10,8)""; ""intent(c)"" is forced.
analyzevars: character array ""character*80 savdcmd0(30)"" is considered as ""character savdcmd0(30,80)""; ""intent(c)"" is forced.
analyzevars: character array ""character*8 namedatl(250)"" is considered as ""character namedatl(250,8)""; ""intent(c)"" is forced.
analyzevars: character array ""character*48 labldatl(250)"" is considered as ""character labldatl(250,48)""; ""intent(c)"" is forced.
analyzevars: character array ""character*8 isosym(50)"" is considered as ""character isosym(50,8)""; ""intent(c)"" is forced.
analyzevars: character array ""character*16 isoicon(50)"" is considered as ""character isoicon(50,16)""; ""intent(c)"" is forced.
analyzevars: character array ""character*80 savedcmd(200)"" is considered as ""character savedcmd(200,80)""; ""intent(c)"" is forced.
analyzevars: character array ""character*8 icon(1984)"" is considered as ""character icon(1984,8)""; ""intent(c)"" is forced.
analyzevars: charselector={'len': '16'} unhandled.analyzevars: character array ""character*256 clog(16768)"" is considered as ""character clog(16768,256)""; ""intent(c)"" is forced.
analyzevars: character array ""character*8 nameoprm(128)"" is considered as ""character nameoprm(128,8)""; ""intent(c)"" is forced.
analyzevars: character array ""character*1 cdum1(117328)"" is considered as ""character cdum1(117328,1)""; ""intent(c)"" is forced.
analyzevars: character array ""character*8 cdum(14666)"" is considered as ""character cdum(14666,8)""; ""intent(c)"" is forced.
                        Block: test
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:test
buildimplicitrules: no implicit rules for routine 'test'.
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:test
buildimplicitrules: no implicit rules for routine 'test'.
                        Block: start_
analyzevars: charselector={'len': '80'} unhandled.analyzevars: character array ""character*1 input(1680)"" is considered as ""character input(1680,1)""; ""intent(c)"" is forced.
analyzevars: character array ""character*80 inputarg(21)"" is considered as ""character inputarg(21,80)""; ""intent(c)"" is forced.
                        Block: execute_
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:execute_
buildimplicitrules: no implicit rules for routine 'execute_'.
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:execute_
buildimplicitrules: no implicit rules for routine 'execute_'.
                        Block: cycle_
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:cycle_
buildimplicitrules: no implicit rules for routine 'cycle_'.
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:cycle_
buildimplicitrules: no implicit rules for routine 'cycle_'.
                        Block: terminate_
                        Block: loadbuf_
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:loadbuf_
buildimplicitrules: no implicit rules for routine 'loadbuf_'.
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:loadbuf_
buildimplicitrules: no implicit rules for routine 'loadbuf_'.
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:loadbuf_
get_parameters: got ""name 'selectedchar' is not defined"" on 'selectedchar'
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:loadbuf_
get_parameters: got ""EOL while scanning string literal (<string>, line 1)"" on 'selectedcharkind(""iso'
                        Block: pyexit
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:pyexit
buildimplicitrules: no implicit rules for routine 'pyexit'.
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:pyexit
buildimplicitrules: no implicit rules for routine 'pyexit'.
                                        Block: endkepler
                        Block: pyplot
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:pyplot
buildimplicitrules: no implicit rules for routine 'pyplot'.
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:pyplot
buildimplicitrules: no implicit rules for routine 'pyplot'.
                                        Block: plotkepler
                        Block: pygets
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:pygets
buildimplicitrules: no implicit rules for routine 'pygets'.
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:pygets
buildimplicitrules: no implicit rules for routine 'pygets'.
                                        Block: ttykepler
                        Block: getentropies_
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:getentropies_
buildimplicitrules: no implicit rules for routine 'getentropies_'.
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:getentropies_
buildimplicitrules: no implicit rules for routine 'getentropies_'.
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:getentropies_
get_parameters: got ""name 'selectedchar' is not defined"" on 'selectedchar'
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:getentropies_
get_parameters: got ""EOL while scanning string literal (<string>, line 1)"" on 'selectedcharkind(""iso'
                        Block: eosedit_
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:eosedit_
buildimplicitrules: no implicit rules for routine 'eosedit_'.
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:eosedit_
buildimplicitrules: no implicit rules for routine 'eosedit_'.
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:eosedit_
get_parameters: got ""name 'selectedchar' is not defined"" on 'selectedchar'
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:eosedit_
get_parameters: got ""EOL while scanning string literal (<string>, line 1)"" on 'selectedcharkind(""iso'
                        Block: eos_
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:eos_
buildimplicitrules: no implicit rules for routine 'eos_'.
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:eos_
buildimplicitrules: no implicit rules for routine 'eos_'.
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:eos_
get_parameters: got ""name 'selectedchar' is not defined"" on 'selectedchar'
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:eos_
get_parameters: got ""EOL while scanning string literal (<string>, line 1)"" on 'selectedcharkind(""iso'
                        Block: burnzone_
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:burnzone_
buildimplicitrules: no implicit rules for routine 'burnzone_'.
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:burnzone_
buildimplicitrules: no implicit rules for routine 'burnzone_'.
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:burnzone_
get_parameters: got ""name 'selectedchar' is not defined"" on 'selectedchar'
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:burnzone_
get_parameters: got ""EOL while scanning string literal (<string>, line 1)"" on 'selectedcharkind(""iso'
                        Block: eosburn_
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:eosburn_
buildimplicitrules: no implicit rules for routine 'eosburn_'.
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:eosburn_
buildimplicitrules: no implicit rules for routine 'eosburn_'.
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:eosburn_
get_parameters: got ""name 'selectedchar' is not defined"" on 'selectedchar'
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:eosburn_
get_parameters: got ""EOL while scanning string literal (<string>, line 1)"" on 'selectedcharkind(""iso'
                        Block: getesk_
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:getesk_
buildimplicitrules: no implicit rules for routine 'getesk_'.
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:getesk_
buildimplicitrules: no implicit rules for routine 'getesk_'.
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:getesk_
get_parameters: got ""name 'selectedchar' is not defined"" on 'selectedchar'
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:getesk_
get_parameters: got ""EOL while scanning string literal (<string>, line 1)"" on 'selectedcharkind(""iso'
                        Block: getes_
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:getes_
buildimplicitrules: no implicit rules for routine 'getes_'.
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:getes_
buildimplicitrules: no implicit rules for routine 'getes_'.
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:getes_
get_parameters: got ""name 'selectedchar' is not defined"" on 'selectedchar'
In: :_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:kepler.f90:getes_
get_parameters: got ""EOL while scanning string literal (<string>, line 1)"" on 'selectedcharkind(""iso'
Post-processing (stage 2)...
Saving signatures to file ""./_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz.pyf""
Stopping. Edit the signature file and then run f2py on the signature file: f2py3.9 ./_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz.pyf
CompletedProcess(args=['f2py3.9', '--verbose', '-m', '_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz', '-h', '_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz.pyf', '--include-paths', '/home/alex/kepler/source:/home/alex/python/source/kepler/code/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz', 'kepler.f90', 'data.f', '--overwrite-signature'], returncode=0)
running build
running config_cc
unifing config_cc, config, build_clib, build_ext, build commands --compiler options
running config_fc
unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options
running build_src
build_src
building extension ""_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz"" sources
creating /tmp/tmppq6l92do/src.linux-x86_64-3.9
f2py options: ['--include-paths', '/home/alex/kepler/source:/home/alex/python/source/kepler/code/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz']
f2py: _kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz.pyf
Reading fortran codes...
        Reading file '_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz.pyf' (format:free)
Line #1376 in _kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz.pyf:""            intent(callback) endkepler ""
        analyzeline: missing __user__ module (could be nothing)
Line #1376 in _kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz.pyf:""            intent(callback) endkepler ""
        analyzeline: appending intent(callback) endkepler to pyexit arguments
Line #1381 in _kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz.pyf:""            intent(callback) plotkepler ""
        analyzeline: missing __user__ module (could be nothing)
Line #1381 in _kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz.pyf:""            intent(callback) plotkepler ""
        analyzeline: appending intent(callback) plotkepler to pyplot arguments
Line #1387 in _kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz.pyf:""            intent(callback) ttykepler ""
        analyzeline: missing __user__ module (could be nothing)
Line #1387 in _kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz.pyf:""            intent(callback) ttykepler ""
        analyzeline: appending intent(callback) ttykepler to pygets arguments
Post-processing...
        Block: pyexit__user__routines
                Block: pyexit_user_interface
                        Block: endkepler
        Block: pyplot__user__routines
                Block: pyplot_user_interface
                        Block: plotkepler
        Block: pygets__user__routines
                Block: pygets_user_interface
                        Block: ttykepler
        Block: _kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz
                        Block: start_
                        Block: execute_
                        Block: cycle_
                        Block: terminate_
                        Block: loadbuf_
                        Block: pyexit
In: _kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz.pyf:_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:unknown_interface:pyexit
get_useparameters: no module pyexit__user__routines info used by pyexit
                        Block: pyplot
In: _kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz.pyf:_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:unknown_interface:pyplot
get_useparameters: no module pyplot__user__routines info used by pyplot
                        Block: pygets
In: _kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz.pyf:_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz:unknown_interface:pygets
get_useparameters: no module pygets__user__routines info used by pygets
                        Block: getentropies_
                        Block: eosedit_
                        Block: eos_
                        Block: burnzone_
                        Block: eosburn_
                        Block: getesk_
                        Block: getes_
Post-processing (stage 2)...
Building modules...
        Constructing call-back function ""cb_endkepler_in_pyexit__user__routines""
          def endkepler(): return 
        Constructing call-back function ""cb_plotkepler_in_pyplot__user__routines""
          def plotkepler(): return 
        Constructing call-back function ""cb_ttykepler_in_pygets__user__routines""
          def ttykepler(data): return 
        Building module ""_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz""...
                Constructing wrapper function ""start_""...
                  start_(nargs,input)
                Constructing wrapper function ""execute_""...
                  execute_(xcmdline,logitx)
                Constructing wrapper function ""cycle_""...
                  cycle_(interactive)
                Constructing wrapper function ""terminate_""...
                  terminate_(s)
                Constructing wrapper function ""loadbuf_""...
                  datbuf,datlabel,ierr = loadbuf_(namedat,jmin,jmax)
                Constructing wrapper function ""pyexit""...
                  pyexit(foo,endkepler,[endkepler_extra_args])
                Constructing wrapper function ""pyplot""...
                  pyplot(plotkepler,[plotkepler_extra_args])
                Constructing wrapper function ""pygets""...
                  pygets(ttymsg,ttykepler,[ttykepler_extra_args])
                Constructing wrapper function ""getentropies_""...
                  datbuf = getentropies_(jmin,jmax)
                Constructing wrapper function ""eosedit_""...
                  data = eosedit_(jzone,tem,den,dt,idata,flags)
                Constructing wrapper function ""eos_""...
                  ptot,etot,sig,pt,pd,et,ed,xk,xkt,xkd,xs,xst,xsd,dxmax = eos_(jzone,tem,den,dt,flags)
                Constructing wrapper function ""burnzone_""...
                  xs,xnu,dxmax = burnzone_(jzone,tem,den,dtup,[flags])
                Constructing wrapper function ""eosburn_""...
                  ptot,etot,sig,pt,pd,et,ed,xk,xkt,xkd,xs,xnu,dxmax = eosburn_(jzone,tem,den,dtup,flags)
                Constructing wrapper function ""getesk_""...
                  p,e,pt,pd,et,ed,xk,xkt,xkd = getesk_(jzone,tem,den)
                Constructing wrapper function ""getes_""...
                  p,e,pt,pd,et,ed = getes_(jzone,tem,den)
                Constructing COMMON block support for ""namecom""...
                  namezone,names,unitzone,izsym,netsym,charset,capchar,nameparm,nameqprm,nameedit,unitedit,namelvar,nameqql,namedatm,labldatm,nametvar,unittime,namecode,nameprob,namein,nameout,namez,name80k,nameplot,namep5,namebg,namezold,idcycle,idstring,idstrng0,cflag,lbuf,cbuf,cbuf1,cbuf2,cbuf16,cbuf32,cbuf48,mess,lineout,axisy2,axisy3,cbuf64,nametlib,printflg,exitmsg,idzone
                Constructing COMMON block support for ""gencom""...
                  timemin0,timemax0,ypltmin0,ypltmax0,y2lowmon,y2himon,y3lowmon,y3himon,xlowmon,xhimon,mode,idestroy,ihsp,istore,nioerr,lenhead,lencom,lencomc,iflagzms,idebug,iplot,iztype,iptype,iqtype,numdatm,ncyct0,ncyct1,ncyctdel,ncycqqt,lentdmpt,niymaxt,istype,ibtype
                Constructing COMMON block support for ""editcom""...
                  edumx,edgamma,edgammae,edgammaf,edeit,edeet,edept,edert,edeid,edeed,edepd,edpit,edpet,edppt,edprt,edpid,edped,edppd,edprd,edxnebt,edxnebd,edetabt,edetabd,edent,edend,edpnt,edpnd,edxknt,edxknd,edsnt,edsnd,edderiv1,edderiv2,edderiv3,edderiv4,edderiv5,edsig,edsigi,edsige,edsigp,edsigr,edsigion,edei,edee,edep,eder,edpi,edpe,edpp,edpr,edxni,edxne,edxnemc,edxnp,edeta,edetap,edxkibn1,edxkibn2,edxkchrs,edxkr,edxkcond,edxkcomp,edeos1,edeos2,eddye,eddyq,eddyf,editerq,edqxc12,edqxo16,edqxne,edbetaq,edwrate,edrectot,edrpdtot,edredtot,edsnuw,edsnubps,edeectot,edepdtot,edeedtot,edebind0,edebind,edeexcit,edsnucq,edsnuc1q,edsdotq,edtauqse,edyfqse,edyffac,edqj,eddyo16q
                Constructing COMMON block support for ""lookcom""...
                  timelook,ltime,ltime0,locgb,locgb0,dscaleml,totm0lk,ncycl00,jalign1,ngridl,locqql,ibqql,gridbuf,locgbufl,jalign2,ltimelk0,timelk0,toffsetl,ncyclook,jdtclook,iterlook,jmlook,jmcalclk,nyliblk,ndatqlk,ialign,nameprbl,idtcsyml,dtlook,radiuslk,tefflook,xlumlook,dn1look,tn1look,xlumnlk,ensclook,enilook,enklook,enplook,entlook,eprolook,eneslook,enclook,enscdlk,enidlook,enkdlook,enpdlook,entdlook,eprodlk,enesdlk,encdlook,snuclook,sn1look,snn1look,pn1look,eta1look,ye1look,sig1look,cmptimel,totmlook,nzblook,idlook2,yeburn1,dlook4,dlook5,dlook6,dlook7,dlook8,dlook9
                Constructing COMMON block support for ""logcom""...
                  cray,backup,look
                Constructing COMMON block support for ""decay""...
                  solabu,ildecay,decay,solabuel,idecay,ndecay,ldecay,solabuib,nldecay
                Constructing COMMON block support for ""vars""...
                  begins,hdumx,lenhead0,jmz0,jmzb0,nburn0,iratioz0,nvar0,nheadz0,nhead0,nparmz0,nparm0,nqparmz0,nqparm0,nniz0,nhiz0,nitz0,nnizb0,nhizb0,nitzb0,nreacz0,ndtz0,ndt0,npistz0,nyez0,nsubz0,nsub0,nzonei0,nzonec0,nzoneb0,nsmall0,nsmallc0,imaxa0,imaxb0,nreac0,jmsave,lencom0,lencomc0,nedtcom0,ndatqz0,ngridz0,nylibz0,nyoffst0,lenshed0,lenqhed0,nzedz0,ncsavdz0,idumhead,pdumx,dtnew,timesec,extrap,q2fac,zmaxit,dtcr,dtct,dtcd,dtcq,dtcdt,fcrmax,fctmax,q1fac,znstop,tstop,znedit,dtedit,zndump,xmlen,fudgc,difim,fcrbu,fctbu,drmult,dtmax,fcrext,fctext,zipup,xk1mt,xk2mt,xk3mt,xk4mt,rxkcmt,pimult,prmult,pemult,transm,timex0,dyemult,dyqmult,yemin,ziwinsiz,hstatm,zlenqmax,ziwinloc,dtcp,yfloorx,cenu,t7peek,xkmin,thickfac,zmaxbak,dtcut,tfcrbu,tfctbu,xipot,dzero,znpflag,fracneut,radius0,summ0,xlum0,zirnet,znpixedi,tnucmin,setparm,ziytsfla,tbound,pbound,fclmax,fclext,dtcl,fclbu,tfclbu,dtsmult,rnratmax,rnratmin,tnratmax,tnratmin,dnratmax,dnratmin,rnmin,tnmin,dnmin,zjmmin,zizonef,zidzonef,rnmax,dypmin,dymgmin,zbound,etaconv,zjshell0,zjshell1,eexplode,texplode,tauexp,p0core,d0core,d0power,sneutmt,snucmt,dsnum,tcorefac,tqsemin,zjqse,artv1,artv2,artv3,zjartv1,zjartv2,zjartv3,zipixtyp,znplotsy,tscalem,zigridt,zigridm,zigridv,zjp0,zjp1,rpmin0,rpmax0,ymintd,ymaxtd,sscalem,pscalem,zitvstar,abunlim,zivplot,vscalem,rscalem,zirtype,convord,yplotmin,yplotmax,etacut,znadapb,fracrz1,fracrz2,rzmult0,rzmult1,rzmult2,abarrat0,abarratm,fracmlim,frcsound,convlim,woversht,xmimult,fmax0,fmax1,fmax2,ddsfrac,dtsfrac,ziudflag,znsdump,zniterba,ziterbam,ziautoou,ziflgaba,fraccore,zjmcalc,ziupdfla,zniondum,znisodum,znzondum,ziflagye,znjeditq,dtqnum,ddqnum,ziexcite,ziterqmh,ypconvh,ynconvh,ysiconvh,cnseh,fyph,fynh,fysih,zneditq,xiter1qe,ziter1qe,xthres,tqselim,o16lim,qn56lim,snuwmult,zjnse,siqselim,zjlcalc,vlimset,zjpause,fracrz0,rzmultm,fmaxm,eionmult,znnewout,zneditq1,wilsonmt,t11cut,y56gessm,zisi30br,dqselim,abunminx,tfcybu,yfloorbx,ziqerrfl,c12agmlt,xltaucon,znoqseco,accrate,xmacrete,denconv,flamerad,coulmult,zirzopt,znsurfz,fracsz0,fracsz1,xmlossm,xmloss0,totm0,fracdez,xmratbak,xfracml,zmlcompf,e1mltc12,e2mltc12,znupdate,dtfrac,bmassmin,bmassmax,btempmin,btempmax,snucmin,bdenmin,bdenmax,tchange,dchange,znetmax,zneditb,edmassl,znedita,zjmeditb,znedital,chimin,delchi,fdtn,dtbkup,zncomp,zncent,znedt,znzro,amaglim,zninv,znbkup,zntty,zileqs,znegbkup,bkupdiv,bkupmass,bkupmp,znbkupma,znsubcyc,zmazful,al26mult,zinburn,znpage,znsavez,znsaveb,vloss,abunlimb,scalem,zneditz1,zneditz2,zmedit,zmeditfi,zneditp,c12flame,o16flame,timenew,sharp1,sharpr,sharp2,znewflam,taunu,enu53,tmunu,tenu,znoiland,charsizg,charsizc,charsizz,charsizh,widthtd,zibackgn,dscalem,zngridmi,zncycqq,zlentrac,zidtmaxl,zidtlook,backfacq,tempstop,denstop,vinstop,o16stop,timezms,zizonezm,q1faczms,tempcig,yflrxcig,fmaxmcig,fmax0cig,toffset,abunminb,abunmaxb,znumiso,timeref,tosetref,timecmin,timecmax,yemax,abarsemi,drmultlo,woverslo,zmaptime,vminmap,vmaxmap,vratmap,tempcdep,o16odep,tempchar,denchar,abarchar,zonemmin,zicalcne,xneconv,zionflag,xnemin,xkapgam,egamp,tshock,tnucleo,tenvel,znfirstq,xlmxmult,binm10,binm20,binalp,binbet,bina0,binmdt,rocher,xlanger1,xlanger2,ziburnye,relmult,geemult,grbparm,swmult,tsharp,xmlossn,znangmix,angfmu,angfc,angfjc,angrcrit,angric,angfjdsi,angfjshi,angfjssi,angfjez,angfjgsf,fmin,zncnvout,zkaptab,fkapz,zfakexp,angsmt,znangsmg,angsml,angsmm,zipapsiz,hstatxm,hstatym,xmlossw,ziold,rhotrans,znwndout,zkapverb,xl0limf,xl0limk,zllimout,znenout,zipromin,zipromax,ziprownd,ziproyld,zminapro,zmaxapro,proymin,proymax,xkdmin,h1hdep,he4hedep,znenuout,optconv,rloss,tloss,tapprox,semilan,profmin,profmax,proamin,proamax,yelimb,zirecb,bmasslow,si28dep,zivspecl,zivrate,zmagnet,znosht,alpth,abarstep,zbarstep,xmustep,znetmin,awcorotz,ymcorot,znstrout,zmixcycl,zlburn,zlbbkup,rlossmin,zlcout,xmagfmu,xmagft,xmagfnu,xmagfdif,dxncbkup,ziplotb,zminzone,zonemmax,tenubar,zlevcnv,zmingain,zminloss,zminnucl,ddmin,ziazonef,dynfac,h1hburn,c12heign,he4hebrn,zonermin,zonermax,xmixnova,accmass,accmassf,vinstopm,zlowamul,pdmult,edmult,zihwamul,zkapburn,fackap,awwkloss,zlossrot,ymjkep,zmaxzone,cfakexp,zminnucg,zmingaind,zminlossd,zminnucgd,zminnucld,tweakmin,centmult,zmixout,zirprox,zn14pg,r3amult,zibwarn,zifallbk,xnumu12,znzsave,axion,zmhiconv,rnhiconv,zicutbin,znconvers,znwndvers,h1hign,wimp,zipdtmin,zminneul,zminneug,zminneuld,zminneugd,wimpsip,wimpsin,wimpsdp,wimpsdn,wimprho0,wimpv0,wimpvelo,ziwimpb,angw0z,angw0m,angjaccz,zoneymax,accdepth,pulse051,pulsb15,znzpuls,fracadz,zlosseadv,ziacceadv,ziaccadv,tnumin,zisurf,znlogout,zipnuc,zipnu,amasslow,umasslow,zidecmode,decrate,fracdec,zjmdec,zlumdata,acctimef,xl0timef,znsekout,ziadapv,zittyv,zihe4cc,zinuenc,zinuebnc,zibdatov,h1hm2,h1hm5,h1hm10,znentout,znentlev,entdm,xl0ratef,accratef,znangdis,xmagfbr,xmagfbt,accdnfac,taucorot,ziccrate,znidecay,znnuout,pulsb150,pulstau,xheatl,xheatym,xheatdm,rmaxeadv,xmlosse,zjslosse,zloczone,ziaccunit,convmass,taumin,taurat,taulim,zir1212,frsurf,psurfmlt,znang3d,angjaccx,angjaccy,angw0x,angw0y,awcorotx,awcoroty,znrotout,qdumx,dt,zjm,dtold,zncyc,fcr,fct,ziter,told,tfcr,tfct,fcl,tfcl,zidtcon,zimax,znnet,znumit,totm,znreac,zitert,zjfcr,zjfct,zjfcl,q23,s0exp,xlumn,enini,enink,eninp,enint,ensc,eni,enk,enp,ent,epro,enes,enc,enscd,enid,enkd,enpd,entd,eprod,enesd,encd,xlum,radius,teff,zncycb,zncycbt,znburnz,znburnzt,zimaxb,znnetb,znumitb,zjlm,zjbmax,dtsub,zjpist,zjpistm,velocty0,tbounce,fluxnu0,fluxnua,chi0,fluxnub,chi1,fluxnuc,chi2,znye,znyem,zidmptim,znylib,znylib0,zndatq,zlocqnex,zndatl,eninr,enr,anglint,anglt,xmlossr,zfak,enn,enpist,enpistd,capeff,xmlost,radconv,zninvl,zngbkup,zmncbkup,znacbkup,zisebkup,enhd,pboundac,velnegm,fbrate,fallback,zncycr,h1init,enrd,zilastpl,zitimeg,zitimed,entloss,eniloss,enkloss,enploss,enrloss,angloss,anglwnd,angit,wimpcrsi,wimpcrsd,wimparad,wimpateq,wimparat,wimpalum,eprodw,eprow,eprodwx,eprowx,xmacc,dmacc,zjloss,zjacc,delmass,xmdec,decmass,dmdec,xmacrate,he4init,zinit,xladv,dvloss,dvacc,dvdec,env,envd,envc,envcd,enpuls,enpulsd,ennidec,ennidecd,enxheatd,enxheat,zjaccemx,pboundat,angltx,anglty,angltz,anglossx,anglossy,anglossz,anglwndx,anglwndy,anglwndz,ziterbar,sdumx,tpist,rpist,yemass,yeq0,aion,zion,znumi,zionn,aionb,zionb,znumib,zionnb,dtc,zjdtc,timeused,totalr,rater,qval,zjrate,rrx,compsurf,zlocqz,zlocqz0,ratzdump,ratiodez,ratioadz,zndatzed,zncyczed,zedmass1,zedmass2,wind,windb,zdumx,ym,rn,rd,un,xln,qln,qld,difi,znetnum,xm,dn,tn,td,en,pn,zn,etan,sn,snn,abar,zbar,xkn,xnei,stot,angj,angdg,angd,dsold,tsold,snold,snbd,snbt,abarold,abarnbd,abarnbt,ypbtime,ynbtime,ppn,bdumx,znetnumb,zlimab,zlimzb,zlimcb,timen,dtimen,dnold,tnold,ymb,sburn,etab,pbuf,ppnb,burnamax,burnmmax,ibcmax,nabmax,nzbmax,ionbmax,nibmax,nbmax,ndumalign1,angi,angri,angw,stotd,epsgrav,enbtn,enbdn,pnbtn,pnbdn,gamma1,gamma2,bfvisc,bfdiff,angdgeff,difieff,bfviscef,bfdiffef,compsurfb,bfbr,bfbt,sv,ziobuf,scmt,dbuf,ylib,ppnold,ppnbold,angjold,windold,windbold,rnold,snw,snwcrsi,snwcrsd,sadv,zm,enved,killburn
                Constructing COMMON block support for ""sysdef""...
                  modebin,pathsep,backslash,calign1
                Constructing COMMON block support for ""sysdef1""...
                  nempty,lempty
                Constructing COMMON block support for ""charsave""...
                  cdumx,namep0,namec0,iflag80,iqbrnflg,craybox,idword,nxdirect,lastrun,lastmod0,ions,ionsb,idtcsym,isymr,nameqq,nameqlib,nameolds,namenews,namedatq,labldatq,namedzed,savdcmd0,namedatl,labldatl,nameqql0,nameqql1,nameqlbl,nsdirect,isosym,isoicon,savedcmd,datapath,icon
                Constructing COMMON block support for ""zbrn""...
                  yz,jzone,timez,denz,tempz,rnz,sburnz,dtimez,etaz,limabz,limzbz,limcbz,sneutz,dyz,chiminz,delchiz,fdtnz,dtbkupz,amaglimz,bkupdivz,bkupmasz,c12mt1z,c12mt2z,al26mtz,bkupmpz,btime,tweakminz,taunuz,enu53z,tmunuz,tenuz,tenubarz,c12agmtz,r3amultz,pdmultz,edmultz,ioldz,jcount,ncompz,ncentz,nedtz,nzroz,ninvz,nbkupz,nttyz,ileqstpz,negbkupz,nbkupmxz,mazfulz,imaxbz,ncycle,ixforcez,nzbakup,lzbkquit,nzxbkup,lowamulz,ihwamulz,n14pgz,ibwarnz,inuencz,inuebncz,ihe4ccz,ibdatovz,iccratez,ir1212z
                Constructing COMMON block support for ""flameit""...
                  xmburn,fc12mult,rflame
                Constructing COMMON block support for ""uuidcom""...
                  uuidrun,uuidcycle,uuiddump,uuidprev,uuidprog,uuidcurprog,uuidexec,nuuidhist,gitsha,hostname,username,gitbranch,uuidhist
                Constructing COMMON block support for ""logdat""...
                  clog,ilog,znlog
                Constructing COMMON block support for ""oparmdat""...
                  nameoprm,iotype,odum,znoparm
                Constructing COMMON block support for ""python""...
                  py_done
        Wrote C/API module ""_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz"" to file ""/tmp/tmppq6l92do/src.linux-x86_64-3.9/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHzmodule.c""
        Fortran 77 wrappers are saved to ""/tmp/tmppq6l92do/src.linux-x86_64-3.9/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz-f2pywrappers.f""
  adding '/tmp/tmppq6l92do/src.linux-x86_64-3.9/fortranobject.c' to sources.
  adding '/tmp/tmppq6l92do/src.linux-x86_64-3.9' to include_dirs.
copying /home/alex/Python/lib/python3.9/site-packages/numpy/f2py/src/fortranobject.c -> /tmp/tmppq6l92do/src.linux-x86_64-3.9
copying /home/alex/Python/lib/python3.9/site-packages/numpy/f2py/src/fortranobject.h -> /tmp/tmppq6l92do/src.linux-x86_64-3.9
  adding '/tmp/tmppq6l92do/src.linux-x86_64-3.9/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz-f2pywrappers.f' to sources.
build_src: building npy-pkg config files
running build_ext
new_compiler returns <class 'distutils.unixccompiler.UnixCCompiler'>
customize UnixCCompiler
customize UnixCCompiler using build_ext
********************************************************************************
<class 'distutils.unixccompiler.UnixCCompiler'>
preprocessor  = ['gcc', '-pthread', '-E']
compiler      = ['gcc', '-pthread', '-Wno-unused-result', '-Wsign-compare', '-DNDEBUG', '-g', '-fwrapv', '-O3', '-Wall']
compiler_so   = ['gcc', '-pthread', '-Wno-unused-result', '-Wsign-compare', '-DNDEBUG', '-g', '-fwrapv', '-O3', '-Wall', '-fPIC']
compiler_cxx  = ['g++', '-pthread']
linker_so     = ['gcc', '-pthread', '-shared']
linker_exe    = ['gcc', '-pthread']
archiver      = ['ar', 'rcs']
ranlib        = None
libraries     = []
library_dirs  = ['/home/alex/Python/lib']
include_dirs  = ['/home/alex/Python/include/python3.9']
********************************************************************************
get_default_fcompiler: matching types: '['gnu95', 'intel', 'lahey', 'pg', 'nv', 'absoft', 'nag', 'vast', 'compaq', 'intele', 'intelem', 'gnu', 'g95', 'pathf95', 'nagfor', 'fujitsu']'
customize Gnu95FCompiler
find_executable('gfortran')
Found executable /usr/bin/gfortran
customize Gnu95FCompiler
customize Gnu95FCompiler using build_ext
********************************************************************************
<class 'numpy.distutils.fcompiler.gnu.Gnu95FCompiler'>
version_cmd     = ['/usr/bin/gfortran', '-dumpversion']
compiler_f77    = ['/usr/bin/gfortran', '-fPIC', '-O3', '-funroll-loops', '-fno-second-underscore', '-fconvert=big-endian', '-I/home/alex/kepler/source', '-I/home/alex/python/source/kepler/code/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz', '-fPIC', '-O3', '-funroll-loops']
compiler_f90    = ['/usr/bin/gfortran', '-fPIC', '-O3', '-funroll-loops', '-fno-second-underscore', '-fconvert=big-endian', '-I/home/alex/kepler/source', '-I/home/alex/python/source/kepler/code/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz', '-fPIC', '-O3', '-funroll-loops']
compiler_fix    = ['/usr/bin/gfortran', '-Wall', '-g', '-ffixed-form', '-fno-second-underscore', '-fPIC', '-O3', '-funroll-loops', '-fno-second-underscore', '-fconvert=big-endian', '-I/home/alex/kepler/source', '-I/home/alex/python/source/kepler/code/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz', '-fPIC', '-O3', '-funroll-loops']
linker_so       = ['/usr/bin/gfortran', '-Wall', '-g', '-Wall', '-g', '-shared']
archiver        = ['/usr/bin/gfortran', '-cr']
ranlib          = ['/usr/bin/gfortran']
linker_exe      = ['/usr/bin/gfortran', '-Wall', '-Wall']
version         = LooseVersion ('10')
libraries       = ['gfortran']
library_dirs    = ['/usr/lib/gcc/x86_64-redhat-linux/10', '/usr/lib/gcc/x86_64-redhat-linux/10', '/home/alex/Python/lib']
object_switch   = '-o '
compile_switch  = '-c'
include_dirs    = ['/home/alex/Python/include/python3.9']
********************************************************************************
building '_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz' extension
compiling C sources
C compiler: gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC

creating /tmp/tmppq6l92do/tmp
creating /tmp/tmppq6l92do/tmp/tmppq6l92do
creating /tmp/tmppq6l92do/tmp/tmppq6l92do/src.linux-x86_64-3.9
compile options: '-DNPY_DISABLE_OPTIMIZATION=1 -I/tmp/tmppq6l92do/src.linux-x86_64-3.9 -I/home/alex/Python/lib/python3.9/site-packages/numpy/core/include -I/home/alex/Python/include/python3.9 -c'
gcc: /tmp/tmppq6l92do/src.linux-x86_64-3.9/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHzmodule.c
gcc: /tmp/tmppq6l92do/src.linux-x86_64-3.9/fortranobject.c
In file included from /home/alex/Python/lib/python3.9/site-packages/numpy/core/include/numpy/ndarraytypes.h:1944,
                 from /home/alex/Python/lib/python3.9/site-packages/numpy/core/include/numpy/ndarrayobject.h:12,
                 from /home/alex/Python/lib/python3.9/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from /tmp/tmppq6l92do/src.linux-x86_64-3.9/fortranobject.h:13,
                 from /tmp/tmppq6l92do/src.linux-x86_64-3.9/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHzmodule.c:16:
/home/alex/Python/lib/python3.9/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning ""Using deprecated NumPy API, disable it with "" ""#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION"" [-Wcpp]
   17 | #warning ""Using deprecated NumPy API, disable it with "" \
      |  ^~~~~~~
In file included from /home/alex/Python/lib/python3.9/site-packages/numpy/core/include/numpy/ndarraytypes.h:1944,
                 from /home/alex/Python/lib/python3.9/site-packages/numpy/core/include/numpy/ndarrayobject.h:12,
                 from /home/alex/Python/lib/python3.9/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from /tmp/tmppq6l92do/src.linux-x86_64-3.9/fortranobject.h:13,
                 from /tmp/tmppq6l92do/src.linux-x86_64-3.9/fortranobject.c:2:
/home/alex/Python/lib/python3.9/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning ""Using deprecated NumPy API, disable it with "" ""#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION"" [-Wcpp]
   17 | #warning ""Using deprecated NumPy API, disable it with "" \
      |  ^~~~~~~
/tmp/tmppq6l92do/src.linux-x86_64-3.9/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHzmodule.c:29:63: error: unknown type name signed_char; did you mean signed char?
   29 | typedef void(*cb_ttykepler_in_pygets__user__routines_typedef)(signed_char *);
      |                                                               ^~~~~~~~~~~
      |                                                               signed char
/tmp/tmppq6l92do/src.linux-x86_64-3.9/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHzmodule.c: In function endkepler_:
/tmp/tmppq6l92do/src.linux-x86_64-3.9/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHzmodule.c:544:16: warning: variable capi_i set but not used [-Wunused-but-set-variable]
  544 |     int capi_j,capi_i = 0;
      |                ^~~~~~
/tmp/tmppq6l92do/src.linux-x86_64-3.9/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHzmodule.c:544:9: warning: variable capi_j set but not used [-Wunused-but-set-variable]
  544 |     int capi_j,capi_i = 0;
      |         ^~~~~~
/tmp/tmppq6l92do/src.linux-x86_64-3.9/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHzmodule.c: In function plotkepler_:
/tmp/tmppq6l92do/src.linux-x86_64-3.9/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHzmodule.c:693:16: warning: variable capi_i set but not used [-Wunused-but-set-variable]
  693 |     int capi_j,capi_i = 0;
      |                ^~~~~~
/tmp/tmppq6l92do/src.linux-x86_64-3.9/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHzmodule.c:693:9: warning: variable capi_j set but not used [-Wunused-but-set-variable]
  693 |     int capi_j,capi_i = 0;
      |         ^~~~~~
/tmp/tmppq6l92do/src.linux-x86_64-3.9/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHzmodule.c: At top level:
/tmp/tmppq6l92do/src.linux-x86_64-3.9/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHzmodule.c:836:42: error: unknown type name signed_char; did you mean signed char?
  836 | extern void F_FUNC(ttykepler,TTYKEPLER) (signed_char *data) {
      |                                          ^~~~~~~~~~~
      |                                          signed char
/tmp/tmppq6l92do/src.linux-x86_64-3.9/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHzmodule.c:817:50: warning: get_active_cb_ttykepler_in_pygets__user__routines defined but not used [-Wunused-function]
  817 | static cb_ttykepler_in_pygets__user__routines_t *get_active_cb_ttykepler_in_pygets__user__routines(void) {
      |                                                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/tmp/tmppq6l92do/src.linux-x86_64-3.9/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHzmodule.c:168:12: warning: f2py_size defined but not used [-Wunused-function]
  168 | static int f2py_size(PyArrayObject* var, ...)
      |            ^~~~~~~~~
analyzevars: charselector={'len': '16'} unhandled.error: Command ""gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -DNPY_DISABLE_OPTIMIZATION=1 -I/tmp/tmppq6l92do/src.linux-x86_64-3.9 -I/home/alex/Python/lib/python3.9/site-packages/numpy/core/include -I/home/alex/Python/include/python3.9 -c /tmp/tmppq6l92do/src.linux-x86_64-3.9/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHzmodule.c -o /tmp/tmppq6l92do/tmp/tmppq6l92do/src.linux-x86_64-3.9/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHzmodule.o -MMD -MF /tmp/tmppq6l92do/tmp/tmppq6l92do/src.linux-x86_64-3.9/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHzmodule.o.d"" failed with exit status 1
---------------------------------------------------------------------------
CalledProcessError                        Traceback (most recent call last)
~/python/source/kepler/build.py in build_module(self)
    170         try:
--> 171             self.f2py([
    172                 '--f90flags={}'.format(

~/python/source/kepler/build.py in f2py(self, args)
     68              + list(args))
---> 69         result = subprocess.run(
     70             args,

~/Python/lib/python3.9/subprocess.py in run(input, capture_output, timeout, check, *popenargs, **kwargs)
    523         if check and retcode:
--> 524             raise CalledProcessError(retcode, process.args,
    525                                      output=stdout, stderr=stderr)

CalledProcessError: Command '['f2py3.9', '--verbose', '--f90flags=-fPIC -O3 -funroll-loops -fno-second-underscore -fconvert=big-endian -I/home/alex/kepler/source -I/home/alex/python/source/kepler/code/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz', '--f77flags=-fPIC -O3 -funroll-loops -fno-second-underscore -fconvert=big-endian -I/home/alex/kepler/source -I/home/alex/python/source/kepler/code/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz', '--include-paths', '/home/alex/kepler/source:/home/alex/python/source/kepler/code/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz', '-luuid', '-c', '-m', '_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz', '/home/alex/python/source/kepler/code/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz/kepler.a', 'kepler.f90', 'data.f', '_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHz.pyf']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Exception                                 Traceback (most recent call last)
<ipython-input-3-8daf234d987e> in <module>
----> 1 k = K('xxx', 'z')

~/python/source/kepler/code/main.py in __call__(*args, **kwargs)
    240             cache = dict()
    241             cls._cache = cache
--> 242         obj = type.__call__(*args, **kwargs)
    243         key = cls._key_func(obj)
    244         return cache.setdefault(key, obj)

~/python/source/kepler/code/main.py in __init__(self, *args, **kwargs)
    398     def __init__(self, *args, **kwargs):
    399         kwl = {k:kwargs.pop(k) for k in self._loadargs if k in kwargs}
--> 400         self._kepler, loaded = kepbin.load(return_loaded = True, **kwl)
    401         if self.__class__._check_cache(self):
    402             print(f' [KEPLER] Returning original object {self} instead of a new one.')

~/python/source/kepler/code/kepbin.py in load(profile, return_loaded, **kwargs)
     50     modules = set(sys.modules)
     51 
---> 52     kwargs, _builder = build(
     53         profile = profile,
     54         return_params = True,

~/python/source/kepler/code/kepbin.py in build(profile, return_params, **kwargs)
     36 
     37     _builder = _BuildKepler(**kwargs)
---> 38     _builder.run()
     39 
     40     print(f' [{__name__}] Using JMZ = {kwargs[""JMZ""]}, NBURN = {kwargs[""NBURN""]}.')

~/python/source/kepler/build.py in run(self)
     94         """"""
     95         if self.build_library_check() or self.build_check():
---> 96             self.build_module()
     97 
     98     def test_executable(self):

~/python/source/kepler/build.py in build_module(self)
    195                 ])
    196         except subprocess.CalledProcessError:
--> 197             raise Exception(""creating module failed"")
    198         os.chdir(cwd)
    199 

Exception: creating module failed
```

</details>

code
```f90
!=======================================================================

subroutine pyexit(foo)

  implicit none
  integer(kind=4), intent(IN):: foo

  !f2py    integer intent(in) :: foo
  !f2py    intent(callback, hide) :: endkepler()
  external endkepler

  call endkepler()

end subroutine pyexit
```",2021-02-05T17:25:29Z
774216693,Thanks Bas.,2021-02-05T18:42:03Z
774219997,"I may have some clue what seems to be dropping out:
I compiled both, and saved the c files, here the diff (the first-compiled version is that with the extra argument to `endkepler`, which compiles:
```diff
<  * Generation date: Fri Feb  5 18:31:50 2021
---
>  * Generation date: Fri Feb  5 18:15:33 2021
25a26
> typedef signed char signed_char;
31c32
< typedef void(*cb_endkepler_in_pyexit__user__routines_typedef)(void);
---
> typedef void(*cb_endkepler_in_pyexit__user__routines_typedef)(int *);
54a56
> #define pyobj_from_int1(v) (PyLong_FromLong(v))
537,538c539,540
< /*typedef void(*cb_endkepler_in_pyexit__user__routines_typedef)(void);*/
< extern void F_FUNC(endkepler,ENDKEPLER) (void) {
---
> /*typedef void(*cb_endkepler_in_pyexit__user__routines_typedef)(int *);*/
> extern void F_FUNC(endkepler,ENDKEPLER) (int *foo_cb_capi) {
546a549
>     int foo=(*foo_cb_capi);
552c555
<     CFUNCSMESS(""cb:Call-back function cb_endkepler_in_pyexit__user__routines (maxnofargs=0(-0))\n"");
---
>     CFUNCSMESS(""cb:Call-back function cb_endkepler_in_pyexit__user__routines (maxnofargs=1(-0))\n"");
565c568
<     (*cb_endkepler_in_pyexit__user__routines_cptr)();
---
>     (*cb_endkepler_in_pyexit__user__routines_cptr)(foo_cb_capi);
594a598,600
>     if (cb->nofargs>capi_i)
>         if (CAPI_ARGLIST_SETITEM(capi_i++,pyobj_from_int1(foo)))
>             goto capi_fail;
1405,1406c1411,1413
< ""  def endkepler(): return \n\
< "";
---
> ""  def endkepler(foo): return \n\
>   Required arguments:\n""
> ""    foo : input int"";
1462c1469
<     if (create_cb_arglist(endkepler_cb.capi,endkepler_xa_capi,0,0,&endkepler_cb.nofargs,&endkepler_cb.args_capi,""failed in processing argument list for call-back endkepler."")) {
---
>     if (create_cb_arglist(endkepler_cb.capi,endkepler_xa_capi,1,0,&endkepler_cb.nofargs,&endkepler_cb.args_capi,""failed in processing argument list for call-back endkepler."")) {
```
so in the second diff, the first version does a 
```c
typedef signed char signed_char;
```
that is missing in the second version.  Although it should be there.  Then gcc does not know that `signed_char` actually stands for `signed char`.
I attach the full c files (as txt, since github does not allow `.c` files)

[2.txt](https://github.com/numpy/numpy/files/5934618/2.txt)
[1.txt](https://github.com/numpy/numpy/files/5934619/1.txt)

So it seems that for whatever reason, under some circumstances that typedef gets dropped.  I would speculate that in most circumstances it might not hurt to throw this in, unless there is a name collision.",2021-02-05T18:48:49Z
774225127,"Using `var` instead of `sigma` may be acceptable. However, using the latter would have the advantage that we would be consistent with the notation used by [scipy.optimize.curve_fit](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html). Scipy explicitly distinguishes the cases where a user passes a vector of `sigma` or a full 2D covariance matrix.",2021-02-05T18:58:40Z
774233153,"It looks like a f2py bug related to `needs` processing. I wish I could reproduce it..

Can you reproduce it with the Fortran code given in this issue only or does the reproducer require the full set of Fortran source you have?",2021-02-05T19:14:23Z
774246780,"I rather like the right-alignment.

I personally see right aligned headings on websites and brands as a way to purposely and artistically stand out (visually represent going 'against the grain', haha).

For documentation, I would expect it to look professional and not flashy, but the more I compare it to left-aligned, the more I like the little bit of flair, the character it adds.

Here's a preview of how it would look left-aligned, with the top two images from [Ben's PR](https://github.com/numpy/numpy/pull/17382).
![NumpyHeadingsIssue](https://user-images.githubusercontent.com/46167686/107080690-a2a3f280-67bf-11eb-8685-9ea4b9eece94.png)

I also don't feel strongly either way, unless it affects someone's ability to read or access the documentation, or if another person brought it up as an issue?",2021-02-05T19:40:24Z
774248803,"There are lots of uses for weights besides normalizing the variance, for instance, masking or robust least squares (IRLS).",2021-02-05T19:44:42Z
774257965,"It is part of a larger code project, so not so easy to tear out.
Linking would not work, but since the issue is at compile time, I may attempt to just selectively uncomment routines.  

Here a set files that may suffice for compilation
[xxx.zip](https://github.com/numpy/numpy/files/5934942/xxx.zip)

the attached version is the one that compiles; for the error you'd need to replace by 
```fortran
subroutine pyexit(code)

  implicit none
  integer(kind=4), intent(IN):: code

  !f2py    integer intent(in) :: code
  !f2py    intent(callback, hide) :: endkepler()
  external endkepler

  call endkepler()

end subroutine pyexit

```
",2021-02-05T20:02:32Z
774261538,"actually, no, for me if I just have
```fortran
subroutine pyexit(code)

  implicit none
  integer(kind=4), intent(IN):: code

  !f2py    integer intent(in) :: code
  !f2py    intent(callback, hide) :: endkepler()
  external endkepler

  call endkepler()

end subroutine pyexit
!=======================================================================

subroutine pygets(ttymsg)

  implicit none
  character*(*), intent(out) :: ttymsg

  integer, parameter :: n = 132
  integer :: i
  integer(kind=1), dimension(n) :: data

  !f2py    character*(*), intent(out) :: ttymsg
  !f2py    intent(callback, hide) :: ttykepler(data)
  external ttykepler

  call ttykepler(data)

  do i = 1, min(n, len(ttymsg))
     ttymsg(i:i) = char(data(i))
  end do

end subroutine pygets
```
and all else commented out, I already get the error message
```shell
/home/alex/python/source/kepler/code/_kepler_build/src.linux-x86_64-3.9/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHzmodule.c: At top level:
/home/alex/python/source/kepler/code/_kepler_build/src.linux-x86_64-3.9/_kepler_NBURN_8192_JMZ_1983_FULDAT_fuldat1_f_CPU_Intel_R_Xeon_R_W_2145_CPU_3_70GHzmodule.c:622:42: error: unknown type name signed_char; did you mean signed char?
  622 | extern void F_FUNC(ttykepler,TTYKEPLER) (signed_char *data) {
      |                                          ^~~~~~~~~~~
      |                                          signed char
```
Is it conceivable that commented-out parts cause issues?",2021-02-05T20:10:03Z
774262436,"No, even removing all the comments still break it.

SInce it works for you, am I use wrong f2py flags?",2021-02-05T20:11:52Z
774278689,"Marked for 1.20.1 for now, but I doubt that we should delay the quick release of that, if this isn't a simple and quick thing.",2021-02-05T20:44:31Z
774282310,No worries.  I very much appreciate the NumPy community looking at my issues.,2021-02-05T20:52:21Z
774286602,"The more I look at it, the more awkward the right alignment looks. When I read the docs, I do so to get the most information in the shortest amount of time. I read English from left to right. It takes an extra moment to adjust to the weird alignment. My personal feeling is that these docs should be all about utility: form follows function and all that.",2021-02-05T21:01:45Z
761737340,"Thanks, @seiko2plus , I'll rebase once that's merged and add the dispatches  ",2021-01-17T05:31:04Z
761746590,"I think this should have an attribution, something like
```
Based on the VCL library, which is (c) Copyright 2012-2020 Agner Fog and licensed under
the Apache License version 2.0.
```

@rgommers is that correct?",2021-01-17T07:17:32Z
761772578,"> I think this should have an attribution, something like
> 
> ```
> Based on the VCL library, which is (c) Copyright 2012-2020 Agner Fog and licensed under
> the Apache License version 2.0.
> ```
> 
> @rgommers is that correct?

No that sounds wrong. Reminder, we don't do Apache2, see for example: https://github.com/numpy/numpy/issues/13447#issuecomment-488547793.

We could change that at some point, but it's a significant change and needs a strong motivation and decision on the mailing list.

VCL is a little hard to find, so here is a link: https://github.com/vectorclass/version2. If this PR took code from there, that seems like a blocker for accepting it.",2021-01-17T11:07:07Z
761772753,"Now read the PR description, the whole PR is based on that, and it seems valuable. It may be worth considering, especially if there are no good alternatives. We'd be deciding to give up on GPLv2 compatibility.",2021-01-17T11:08:51Z
761779667,"@mattip Hmmm... Its a little vague, not sure what im supposed to do",2021-01-17T11:24:50Z
761802147,"According to the above mention, the NumPy 1.19.5 release uses a workaround for the Windows 2004 bug, instead of waiting for Microsoft to fix it.

> NumPy 1.19.5 is a short bugfix release. Apart from fixing several bugs, the main improvement is the update to OpenBLAS 0.3.13 that works around the windows 2004 bug while not breaking execution on other platforms. This release supports Python 3.6-3.9 and is planned to be the last release in the 1.19.x cycle.",2021-01-17T12:12:13Z
761812595,"@rgommers, we can ask for re-licensing, since we're not taking the same exact code plus the original code itself is based on **T. Granlund** and **P. L. Montgomery** work.",2021-01-17T13:26:33Z
761814845,"@charris, Why do we still support Darwin/PowerPC? this flag `faltivec` should be removed.",2021-01-17T13:44:07Z
761819448,"@mhvk,
> Much of the work also ends up being done in python anyway

That's the whole idea behind this solution. 

> That said, @eric-wieser is also right to warn about problems for newcomers

I see it as more friendly for newcomers if you compare it with web template engines, It still python after all.
",2021-01-17T14:15:56Z
761833505,"> I see it as more friendly for newcomers if you compare it with web template engines, It still python after all.

If one were to chose a single tool, indeed, but there are already a couple... Though perhaps this can already replace something? In particular, `tempita` seems to be used for exactly two files: `numpy/random/_bounded_integers.p{yx,xd}.in`, so perhaps one can just rewrite those two and remove `tempita`? (though looking at #8096, it seems to be there since cython uses it?! And from the discussion at http://numpy-discussion.10968.n7.nabble.com/Vendorize-tempita-td43505.html it may be that scipy uses `npy_tempita`...) 

Also, the main argument for an existing template engine like `jinja2` is that at least some people will have seen it before (and one can easily find documentation/examples/etc), so less to learn.",2021-01-17T15:51:09Z
