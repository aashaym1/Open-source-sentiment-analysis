,id,body,created_at
0,505691219, how is it possible that numpy does not have a matrix_transpose function I hid an undocumented one at np.linalg.linalg.transpose that uses the same broadcasting rules as the other linalg functions. You're welcome ;),2019-06-26T02:24:17Z
1,511477435,"Another hint: If you use numpy.add.at, a much faster alternative is numpy.bincount with its optional weight argument: python import perfplot import numpy numpy.random.seed(0) def numpy_add_at(data): a, i = data out0 = numpy.zeros(1000) numpy.add.at(out0, i, a) return out0 def numpy_bincount(data): a, i = data return numpy.bincount(i, weights=a, minlength=1000) perfplot.show( setup=lambda n: (numpy.random.rand(n), numpy.random.randint(0, 1000, n)), kernels=[numpy_add_at, numpy_bincount], n_range=[2 k for k in range(24)], ) ![out](",2019-07-15T16:40:19Z
2,671426864,"Here: Python 3.10.0a0 (heads/master:39042e0, Aug 10 2020, 0902) [GCC 8.3.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. import numpy import sys numpy.__version__ '1.19.1' sys.version '3.10.0a0 (heads/master:39042e0, Aug 10 2020, 0902) \n[GCC 8.3.0]' ",2020-08-10T15:32:49Z
3,685809035,"Sorry, I retract my retraction. That failure comes from 1.17.0. In master, it _is_ allowed: python a = np.array([np.zeros((3, 2)), np.zeros((3, 3))], dtype=object) a array(list([array([[0., 0.], [0., 0.], [0., 0.]]), array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]])]), dtype=object) a.shape () huh? np.__version__ '1.20.0.dev0+5dbc66e' but frankly the output is nonsense.",2020-09-02T15:22:57Z
4,685815029,"Hmmm, yeah, although I think that one is likely just an independent bug, the discovered shape is incorrect to begin with.",2020-09-02T15:31:41Z
5,685817615,"Hmm, ok, what we could do (or argue as being correct), is that the shape here should always be (2,) when dtype=object. But that is also a bit strange, it would resolve all of these issues in a probably even decent way, but is a bit complicated by itself. Mind creating a new issue for it? I am looking if I can find some obvious thing right now... if not, yes. I think the shape should be discovered as (2, 3) here probably, and then we just get the old result.",2020-09-02T15:35:35Z
6,685824689," Mind creating a new issue for it? I am looking at it right now, my shape updating code was pretty old, and required some more love. I guess the code coverage looked like the case was covered but it wasn't. (there was a simple sign error) But I am also looking whether I can make the warning distinguish the case where dtype=object will definitely not help in the future (i.e. if an array would have to be split into sequences).",2020-09-02T15:46:47Z
7,691932282,Just this comment left to address: I cleaned up the whitespace myself,2020-09-14T09:19:37Z
8,736272909,"Just ran into an issue with the inconsistency of the documentation for PyArray_AsCArray. I'm on 1.19. The [documentation]( says the signature is int PyArray_AsCArray(PyObject op, void ptr, npy_intp dims, int nd, int typenum, int itemsize), but I'm getting a compile error, and looking at [the code on GitHub]( that doesn't seem to be the case.",2020-12-01T07:14:25Z
9,736518952,"Note that this latter discussion is offtopic. The bug report is about making bincount work for uint64, an unsigned type. If you want it to work with negative numbers, you should perhaps open a new issue.",2020-12-01T12:23:02Z
10,754299396,"Can you try passing dtype=object, and see if the result fits in an int64?",2021-01-05T00:03:49Z
11,754306454,Will do what you're after (will be part of 1.20.0),2021-01-05T00:23:35Z
12,754338224," After changing to A = np.random.rand(200, 100) + 0j, the perf result remains the same, which means complex matmul is twice slower than noncomplex.",2021-01-05T02:07:01Z
13,754351973,"I would guess that the relevant code change is probably array size. NumPy doesn't have blocked iteration, so if you have outofcache sized arrays that makes a massive difference?",2021-01-05T02:47:07Z
14,754377755,"LGTM, Thanks !",2021-01-05T04:02:25Z
15,754380859,whohoo. thanks. ,2021-01-05T04:12:19Z
16,754396818,"Sorry, I confused myself. This needs to be backported unfortunately. We may have more tests for the casts, but not for promotions (but then the main promotion tests are one huge test in test_numeric it is possible float16 ones were just never added.",2021-01-05T04:59:10Z
17,754441388,"Hello, As for SLEEF, there are a couple of things to pay attention. 1. Beware of this bug : It seems that there are other bugs on clang, but I cannot reliably reproduce the problems. GCC seems okay, but gcc 10.2 cannot compile the quad library with SVE support due to ICE. 2. Arithmetic functions (add, sub, mul, and div) do not give perfectly correctlyrounded results. They have very small amount of error. Please tell me if this is problematic. ",2021-01-05T06:55:03Z
18,754465288," I didn't see your scalar version quadruple precision implementation, the vectorized version has an error bound like 0.5000000001 ULP, which is unbearable in numpy's precision standard, maybe scalar version can overcame this flaw.",2021-01-05T07:44:38Z
19,754491313,"It is not so hard to just implement a correctlyrounded version of arithmetic functions. The problem is that there is a tradeoff between speed and accuracy. If 0.5000000001 ULP is not bearable, I will add correctlyrounded but slower functions.",2021-01-05T08:37:00Z
20,754493377,"Note this will try to allocate an array of 232 elements of type object, which may eventually succeed in systems with 32GB of memory. Nevertheless, we should ideally respond to ctrl+C here.",2021-01-05T08:41:29Z
21,754501074, Is that actually incorrect with the transpose? I think so. The original reproducer seems correct to me. In the extended candidate reproducer I vary n and k and the results seem correct (I fail to reproduce the bug I observe when calling openblas DGEMM via numpy). I will probably need to use a debugger to double check the exact cblas_dgemm arguments when calling from numpy.,2021-01-05T08:57:04Z
22,754536109,"Perhaps arithmetic functions used for the trigonometric functions do not necessarily have to be correctlyrounded, only the arithmetic functions exposed to the user for IEEE 754 conformity. In this context, the question arises as to whether GNU libquadmath arithmetic functions are correctlyrounded or not. ",2021-01-05T10:02:30Z
23,754543515,"Arithmetic functions are implemented within glibc, and they are correctly rounded.",2021-01-05T10:15:32Z
24,754571149,Thanks ,2021-01-05T11:11:21Z
25,754575920,[Paul Zimmermann (INRIA)]( recently compared the accuracy of several math libraries: [accuracy.pdf]( This includes a comparison of quadrupole precision between glibc and icc. See also his talk [How Slow is Quadruple Precision]( on [Variable Precision in Mathematical and Scientific Computing](,2021-01-05T11:21:20Z
26,754583270,"I found a temporary fix for this: It is only necessary to set the AR shell variable using export AR=/usr/bin/ar However, this is not really ""the"" solution, and I am confused as to whose fault this is. Someone mentioned that this might be a problem with the python distribution I am using (which is condaforge).",2021-01-05T11:37:22Z
27,754583426,": Have you installed Cython? Cythonizing sources Processing numpy/random/_bounded_integers.pxd.in Processing numpy/random/_philox.pyx Traceback (most recent call last): File ""/private/var/folders/gy/jzs3xnwd1z3203d75y_31nxc0000gn/T/pipreqbuild6ap9n5ru/tools/cythonize.py"", line 59, in process_pyx from Cython.Compiler.Version import version as cython_version ModuleNotFoundError: No module named 'Cython' ",2021-01-05T11:37:45Z
28,754600626,"Quadruple precision arithmetic functions are slow because packing to/unpacking from IEEE 754 format are pretty complicated. If computation speed is important, doubledouble is much faster. SLEEF quad library is designed in a way that the frontend and backend are separated. The frontend does packing and unpacking. Computation by the backend is all carried out with tripledouble operations. Thus, it is easy to add another frontend for a different format.",2021-01-05T12:16:51Z
29,754751458,"This is outside of numpy's control: python x = np.array([ float('nan'), 56.3, 35.2, float('nan'), 5.7, 22., 18.9, 8., 7.7, 47.3, 51.6, 54.1, 47.3, 51.6]) y = np.roll(x, 1) numpy is not in the picture any more x = x.tolist() y = y.tolist() but your tests still fails print(max(x)) print(max(y)) ",2021-01-05T16:39:20Z
30,754757624," Could the same magic be used to make datetime64 and timedelta64 available? Yes, but let's leave that to a follow up Trying to follow up on this, added ctypedef class numpy.datetime64 [object PyObject]: pass ctypedef class numpy.timedelta64 [object PyObject]: pass along with the declarations in this PR, but at import time I'm getting RuntimeWarning: numpy.datetime64 size changed, may indicate binary incompatibility. Expected 16 from C header, got 32 from PyObject which is enough to stop the tests from running. Did you have something else in mind for how the followup would work?",2021-01-05T16:50:51Z
31,754777032,"Is numpy.datetime64 currently exposed as the full struct? You can add an ignore flag like for ndarray I guess, although I am surprised it is necessary, and I am curious whether this is just a matter of recompiling? (assuming this is within NumPy itself, if it is outside you should not have to recompile of course.)",2021-01-05T17:21:39Z
32,754781261,We could add a PyInterrupt_Occurred() (if that was the command) check every time we entering another level of sequence unpacking. That should be easy to do for someone familiar with the Python CAPI and not scared of digging into the code.,2021-01-05T17:29:14Z
33,754782336,"There are uses for quad precision apart from it starting to show up with hardware support. I looked into it because I'd like to use it for a new datetime type. I think astropy uses double double for that, they need the extra precision. As a historical note, VAX had a quad precision type back in the 1980's and I found it useful on occasion. ",2021-01-05T17:31:14Z
34,754786736,", you need to use object PyDatetimeScalarObject as I do in the top PR comment. I suspect you'll need to remove the current PyDatetimeScalarObject cython struct two, to prevent them overlapping.",2021-01-05T17:39:05Z
35,754796988,"Even with a uint8 dtype, that array seems to have more than 4GiB of size. I expect you are storing it compressed, and reading it into NumPy will decompress it. Probably just np.asarray(mask_raster) is enough to give the error.",2021-01-05T17:55:58Z
36,754820429,"For your problem, I assume that find_objects and then using just boolean logic w = mask_raster == object_id and np.count_nonzero(w) and object_values = original_data[w] is better though. unique needs to sort the array, that is a huge waste probably, when all your blobs are likely well defined and in one area.",2021-01-05T18:35:13Z
37,754843509," I think this could be made better looking using stacked parametrizations, the product/nested iteration, for instance, can be done python .mark.parametrize(""one"", [...]) .mark.parametrize(""two"", [...]) def test_me(one, two): pass Done",2021-01-05T19:18:17Z
38,754844141,"When I disable ctypedef struct PyDatetimeScalarObject: PyObject_HEAD npy_datetime obval PyArray_DatetimeMetaData obmeta ctypedef struct PyTimedeltaScalarObject: PyObject_HEAD npy_timedelta obval PyArray_DatetimeMetaData obmeta and in its place put ctypedef class numpy.datetime64 [object PyDatetimeScalarObject]: cdef npy_datetime obval cdef PyArray_DatetimeMetaData obmeta ctypedef class numpy.timedelta64 [object PyTimedeltaScalarObject]: cdef npy_timedelta obval cdef PyArray_DatetimeMetaData obmeta I get compiletime errors below where PyDatetimeScalarObject and PyTimedeltaScalarObject are referenced. Disabling those functions for now numpy compiles, but when running pytest numpy/core/tests/test_cython.py i get compiletime errors for checks.pyx complaining about just about everything in the cnp namespace. just passing instead of declaring the attributes on datetime64/timedelta64 gives the same errors.",2021-01-05T19:19:25Z
39,754844612,Thanks ,2021-01-05T19:20:14Z
40,754854756,So the potential issue with the compiler assuming alignment only applies with the loop unrolling macro? EDIT: Or is it that it is a performance issue with auto vectorization? Which probably does not matter elsewhere.,2021-01-05T19:39:33Z
41,754955283, unique val of ar1: [0. 1.] and ar2: [0. 1.] That shows that ar1 and ar2 are arrays of floating point values. The bitwise operators do not accept floating point inputs. Convert them to integer arrays (e.g. ar1.astype(int)) before passing them to bitwise_or.,2021-01-05T23:05:52Z
42,754961893,Thank you!!,2021-01-05T23:24:30Z
43,755087498,"I think that's how it has always behaved. np.any() is essentially np.logical_or.reduce(), and np.logical_or() on object arrays follows the behavior of the or operator, returning the first of its operands that is truthy, not a bool. np.any() predates the builtin any(), so the difference is unfortunate, but not a bug.",2021-01-06T05:26:34Z
44,755096139," thanks. The behavior surfaced using pandas.DataFrame.any() and expectations were based on their docs. You mentioned a difference between np.any() and builtin any() is this the difference between np.any(arr) and arr.any()? Both seem to give the same result. python arr = np.array(['s', 1], dtype=object) np.any(arr) also returns s. ",2021-01-06T05:49:47Z
45,755101299,"In addition to this, would it be a good idea to have a function that does what we expect numpy.fromfunction to do? This would call the provided function ""no of elements in shape"" (shape[0]shape[1]...) times giving it tuples like so (0,0) (0, 1)... It would also always return an np.array of shape shape where the value at (i, j)th index is function(i, j)",2021-01-06T06:03:19Z
46,755112270,"ok, then I will use my workaround and close the issue. sorry for bothering and thank you for your time",2021-01-06T06:32:16Z
47,755117409,Could you move the deprecation test to the proper file? Otherwise this looks good to me.,2021-01-06T06:45:07Z
48,755171534, I see this didn't make it into the 1.19.5 version ( shouldn't this simple patch be backported?,2021-01-06T08:54:42Z
49,755212503,"I would also find NumPy popcount/countbits useful. In case it's helpful for anyone: as well as Numba it's also available in JAX, as [jax.lax.population_count]( and here's an implementation in terms of other NumPy functions: ",2021-01-06T10:16:02Z
50,755216577,"Argh yes, that is an oversight. I should have put on a label, then would have seen it when deciding what to backport for 1.19.5. Done now; luckily 1.20.0 is also imminent, so the damage is very limited. I checked the fix is in the 1.20.x branch.",2021-01-06T10:24:40Z
51,755293840, I should have put on a label The milestone needed to be 1.19.5. NumPy 1.20.0 is waiting for pyarrow to release. If they don't get a move on I'll release anyway.,2021-01-06T13:17:49Z
52,755295179,I'm not planning on another 1.19 release.,2021-01-06T13:20:40Z
53,755300196,", wieser, doliskani, thank you for the insightful discussion! Given the ambiguity of the problem, let's close this one asis.",2021-01-06T13:31:37Z
54,755324181," As a comparison of popularity, I did some Google search to see if there are people interested in the functions of these __builtin routines. It seems that judging by the relevance of top results and the total number of search results: popcount (""python count one bits""): 4070000 results ... parity (""python integer parity""): 1040000 results Note parity is trivial to implement if you have an efficient popcount. (parity=popcount&1)",2021-01-06T14:17:50Z
55,755385874,"No, I meant the Python builtin any(). It came into the language after Numeric (numpy's predecessor) had the function Numeric.any() (now numpy.any()). If the builtin any() had existed earlier (with its boolreturning behavior), we probably would have made Numeric.any() match.",2021-01-06T15:59:06Z
56,755387641,"Actually, that timing may not be true. Faulty memory.",2021-01-06T16:01:41Z
57,755392477,"I think they came in with numpy, which was released a little before Python 2.5, the first release with the any() and all() builtins. So a closerrun thing than I remember. We might also have had any() and all() in the Numericbased scipy earlier than that, though it's more effort than I think it's worth to do the code archaeology to figure out.",2021-01-06T16:09:14Z
58,755397144,Sounds like an OpenBLAS cpu detection problem. frbg Thoughts?,2021-01-06T16:16:09Z
59,755400474,"Done, we could consider deprecating even more, but I guess this might be a good (and easy) first step. Yeah, I think I first wrote that at a time when I was considering to stop relying on that file so much (The original reason, and most of the magic in that file, was to work around python issues with warnings, which were eventually fixed).",2021-01-06T16:20:04Z
60,755401771,"Note that we do test on aarch64 during the wheel build, so docker may also be at fault. Where did you get it?",2021-01-06T16:20:50Z
61,755432611,"Great, has a link to which includes an [example]( for extending with quad types.",2021-01-06T17:08:19Z
62,755438271,"Not sure what the illegal instruction could be the getauxval(AT_HWCAP) in dynamic_arm64.c may be a priviledged call in some circumstances but it has been in the code for two years, the only recent change was my addition of an attempt to read cpuid information from /sys/devices in the event that getauxval did not succeed. (OpenBLAS PRs 2952 and 3004) export OPENBLAS_CORETYPE=ARMV8 (or whatever the actual hardware is) before launching python should hopefully get around this.",2021-01-06T17:18:29Z
63,755444034," Hey, I was facing the same problem and due to the issue that you mentioned. Thanks :) ",2021-01-06T17:28:35Z
64,755446663,Thanks ,2021-01-06T17:33:16Z
65,755704225," here's an implementation in terms of other NumPy functions: This is an implementation of divideandconquer popcount, which is going to be substantially slower than something that uses native popcount instructions. shows 4× slowdown on native code, but the Python implementation will be way slower than that I suspect.",2021-01-06T21:11:51Z
66,755743899,Haha yes I wasn’t claiming it was efficient . Perhaps I should have added an explicit warning.,2021-01-06T22:09:42Z
67,755762142,"Here's a numpyonly approach I coded up this morning (to fill the need that lead me here) I have not profiled it, but it made my simulation acceptably fast. python create a lookup table for set bits per byte lookuptable = np.array([bin(i).count('1') for i in range(256)]).astype(np.int32) def count_set_bits(v): 'returns the count of set bits in each element of v' assert v.dtype in (np.uint8, np.uint16, np.uint32, np.uint64), 'must be an unsigned int dtype' return lookuptable[np.reshape(v.view(np.uint8), v.shape + (1, ))].sum(axis=1) ",2021-01-06T22:43:41Z
68,755822314,"Tabledriven is probably the best choice for this situation, and this is a nice implementation. Thanks! That said, still want a popcount primitive. :)",2021-01-07T01:20:36Z
69,755916689,"For you reference (Macbook M1), OPENBLAS=""$(brew prefix openblas)"" MACOSX_DEPLOYMENT_TARGET=11.1 pip3 install numpy pandas nousepep517 It will install pandas1.2.0 and numpy1.19.5 successfully.",2021-01-07T06:32:01Z
70,755993207,"LGTM. This is a reshuffle to enable changes in the future, with no real code changes.",2021-01-07T09:22:28Z
71,755994852,"I think we can close this issue and declare the bounty complete. The original bounty says: NumPy contains SIMD vectorized code for x86 SSE and AVX. This issue is a feature request to implement native, equivalent enablement for Power VSX, achieving equivalent speedup appropriate for the SIMD vector width of VSX (128 bits). We have implemented the infrastructure needed to enable the porting of loops via Universal SIMD, and even ported some of the loops, so the enablement is complete.",2021-01-07T09:25:22Z
72,756008671,There is a warning in doc building: /home/circleci/repo/doc/source/release/1.21.0notes.rst WARNING: Unexpected indentation. /home/circleci/repo/doc/source/release/1.21.0notes.rst WARNING: Block quote ends without a blank line; unexpected unindent. ,2021-01-07T09:49:28Z
73,756048017,"Thanks . If we need to change anything, I think copying the content from [the installation docs]( would be better. But I am not sure we should add anything. This is the README for the github repo, so it is expected people who see this will know what to do. Did you come across a reference to this page as a resource for how to get started with NumPy?",2021-01-07T11:04:28Z
74,756098326,"When I read _Hierarchy and abstract classes_ from [NEP 42,]( I can read out the following. A quadrupoleprecision type should be implementing something like this in future: text Abstract Type: DType Numeric Floating Float128 Concrete Type: DType Float128 with future numpy1.21 or higher is what I assume is meant? ",2021-01-07T12:52:54Z
75,756140369,"I can confirm that OPENBLAS_CORETYPE=ARMV8 python3 c ""import numpy"" doesn't segfault, while omitting the OPENBLAS_CORETYPE env var does.",2021-01-07T14:12:36Z
76,756142087,It may be relevant that /sys/devices/system/cpu/cpu0/regs does not exist on this system.,2021-01-07T14:15:19Z
77,756157883,Can you elaborate on the change you're suggesting? Are you talking about the parameters section? The examples section?,2021-01-07T14:42:14Z
78,756163207," I hope 1.21 is what it means, I would have hoped to be closer to having it done, but thats life. There doesn't need to be an abstract type, Float128 can be a subclass of an abstract Floating, but it doesn't necessarily matter too much to begin with, and those abstract dtypes don't even exist yet.",2021-01-07T14:51:25Z
79,756193514,The code itself looks fine. It seems like we are adding alot of boilerplate but I don't see how to avoid it. There is a conflict and coverage is still complaining that some of the test conditions are never hit. Could you write some clevel tests that abuse the API just to see that the errors are correctly raised with no problems?,2021-01-07T15:40:02Z
80,756218686,I restarted Azure since it hit gh18119 on the 64bit run (failed to find nm.exe),2021-01-07T16:19:38Z
81,756220811,"Since it is internal we can put off adding tests for now, but should somehow mark it with a comment TODO: test this or so.",2021-01-07T16:22:57Z
82,756262654,"Thank you. I had actually looked at this (in the context of the PR that changed the keyword to __asm__) and wondered why this particular instance had not been labeled as volatile, but fooled myself with the argument that it ""appeared"" to have worked just fine for so long. (Could be the version included in 1.19.5 was built with a newer gcc or just subtly different options than before) OTOH, now that I have stumbled upon this thread in the gcc help ML, I am no longer sure if volatile will be sufficient. :(",2021-01-07T17:29:56Z
83,756284418,"I think the restructuring of the code you did caused the optimiser to make different decisions. OTOH, now that I have stumbled upon this thread in the gcc help ML, I am no longer sure if volatile will be sufficient. :( That's not good. The only thing I can think of that would definitely work is to pass the result of getauxval() into the asm, and do some branching within the asm.",2021-01-07T18:07:24Z
84,756299738,Wasn't there a fix for this? See . EDIT: That fix was backported after rc2.,2021-01-07T18:34:07Z
85,756302337,So it'll be fixed in rc3?,2021-01-07T18:38:37Z
86,756308357," So it'll be fixed in rc3 Hopefully. We are currently waiting on pyarrow, so I might put one out in a week or two, especially if the OpenBLAS compiler problems currently reported on AARCH64 get addressed.",2021-01-07T18:49:20Z
87,756309124,"Ok, I'll keep an eye out.",2021-01-07T18:50:42Z
88,756312171,"We got around this in NumPy in [npy_get_floatstatus_barrier]( by passing a dummy parameter into the function, which prevented reordering even with O3",2021-01-07T18:56:20Z
89,756319560,Kindly ask if anyone can review this PR? Any response will be appreciated!,2021-01-07T19:09:24Z
90,756375549,"I see, thanks for clarifying, didn't know Python's builtin s.any() and .all(). Since this isn't a bug, I think it makes sense to close issue. ",2021-01-07T20:49:12Z
91,756454343,"Question for folks following this issue that's slightly off topic. I've installed scipy as above and am getting a segfault when I import scipy.integrate. I've filed a bug report [here]( Any one here able to import scipy.integrate after installing per above on M1? I figured I'd ask here as a bunch of folks here probably already have the setup in place. If so, maybe hop over to [that issue]( and leave a note. Thanks!",2021-01-07T23:43:50Z
92,756455422," Question for folks following this issue that's slightly off topic. I've installed scipy as above and am getting a segfault when I import scipy.integrate. I've filed a bug report [here]( Any one here able to import scipy.integrate after installing per above on M1? I figured I'd ask here as a bunch of folks here probably already have the setup in place. If so, maybe hop over to [that issue]( and leave a note. Thanks! segfault here too.",2021-01-07T23:47:27Z
93,756460058,"Well, I added such a comment to the private methods and the internal fromspec (in the header). Also squashed and the doctest flag switched (which means codecov will probably be useless now).",2021-01-08T00:01:08Z
94,756485337,I completely agree. Sayed and the NumPy core developers have done an amazing job! Do you want me to close the issue? I have been deferring to the NumPy developers.,2021-01-08T01:17:09Z
95,756571176,Thanks . Agreed that the solution exceeded expectations. I will close it then.,2021-01-08T06:14:47Z
96,756573673,"Build is failing, typo?: numpy/core/src/multiarray/array_method.c43: error: ‘boundarraymethod__resolve_descripors’ undeclared here ",2021-01-08T06:22:11Z
97,756576062,"This is really low on our priority list. If you wish to move forward with it, please relate to the [last comment on the issue]( We would like to suggest that contributors show the output from the error. This will require you think about the error: is the code path actually hit? Does it make sense to chain the exception?",2021-01-08T06:28:41Z
98,756648188,"Awesome, thanks everyone for the excellent work!",2021-01-08T09:24:54Z
99,756688023,"I'm talking about def stack(arrays, axis=0, out=None): the definition is not consistent with other numpy functions like concatenate. It took me a few minutes to realise the use was the same. not a big deal of course, if I looked at the examples I would have realised it, but I think it would be better if it were consistent over all functions",2021-01-08T10:46:41Z
100,756689367,What are you suggesting that line should be instead?,2021-01-08T10:49:56Z
101,756695210,"numpy.concatenate((a1, a2, ...), axis=0, out=None) like this one",2021-01-08T11:02:59Z
102,756695977,"Sorry, but we have to use legal python syntax: pycon def stack((a1, a2, ...), axis=0, out=None): pass File ""<ipythoninput62524e6e6c5d59"", line 1 def stack((a1, a2, ...), axis=0, out=None): pass ^ SyntaxError: invalid syntax ",2021-01-08T11:04:30Z
103,756707487," np.lcm(219060189739591200, 43, dtype=object) 9419588158802421600 x = np.int64(np.lcm(219060189739591200, 43, dtype=object)) Traceback (most recent call last): File ""<stdin"", line 1, in <module OverflowError: Python int too large to convert to C long It produced the following outputs wieser .",2021-01-08T11:30:59Z
104,756711449,Then the behavior is as expected. It is impossible for your original example to produce a correct result because the correct result cannot be stored in the return type.,2021-01-08T11:40:35Z
105,756767690,What happens if you replace $ OPENBLAS=$OPENBLAS_ROOT/lib64 pip install ... with $ OPENBLAS=$OPENBLAS_ROOT/lib64 python m pip install ... ?,2021-01-08T13:56:18Z
106,756768319,(also note OpenBLAS has released version 0.3.13 ...),2021-01-08T13:57:41Z
107,756797245,"There was only one significant comment, on the mailing list from . I addressed that here with the same answer I gave on the mailing list. I'd like to merge this PR when CI is happy and then follow up on the mailing list to propose this NEP moves from Draft to Accepted.",2021-01-08T14:55:07Z
108,756813378,"Oh man, should have just run the tests even for a comment change I guess...",2021-01-08T15:23:18Z
109,756816696,"The concatenate docstring is a bit weird, because its signature line comes from C and is just a string, but actually does include the invalid syntax. I admit it is probably nicer to read, so I am not sure we have to change it.",2021-01-08T15:29:17Z
110,756823948,"Apologies, this is a duplicate issue ",2021-01-08T15:42:04Z
111,756830704,I'm using python3.9 on a Big Sur 11.1 and I can confirm that the following has worked for me: wget pip3 install numpy1.19.5cp39cp39macosx_10_9_x86_64.whl EDIT: I'm using a 2019 mac with the intel processor,2021-01-08T15:53:27Z
112,756846438,"The problem has been fixed in Homebrew, so with the latest python installed a simple pip install numpy works.",2021-01-08T16:16:46Z
113,756912296,"Thanks, but I am going to close, this. I could imagine adding a link to the NumPy website at but I don't think we need an installation section in the readme probably. I expect those actually interested in a source install will find the docs from there or by searching?",2021-01-08T18:05:06Z
114,756931917," The wheel is for intel chips. It doesn't work for M1 chips. I couldn't find a whl for M1 chips (arm64). Didn't solve the issue. I just updated brew, brew install python and pip3 install numpy didn't work. Big Sur 11.0.1 Apple M1 chip python 3.9.1 pip3 20.3.3 ",2021-01-08T18:45:56Z
115,756936573,"You are right, the problem was only solved for x86.",2021-01-08T18:56:15Z
116,757064324,"Don't worry about it, I would be extremely surprised if it is measurable in this place. We could add something like that to the benchmarks (I don't think we have a ""many sequences"" one), but really no need. ",2021-01-09T00:43:32Z
117,757073968,"Ok, I will go ahead and just remove the test for now then",2021-01-09T01:38:35Z
118,757099973,Thanks! wieser I'll give a thought about it. ,2021-01-09T05:30:36Z
119,757106135,Thank you for everything <3,2021-01-09T06:41:38Z
120,757109839,", please be aware that __cpu_features__ affected by the environment variable NPY_DISABLE_CPU_FEATURES which allows the user to disable certain features in runtime.",2021-01-09T07:21:16Z
121,757119822,"Thank you for your report. This bug has been reported by , and already resolved by as mentioned.",2021-01-09T09:04:06Z
122,757123004,I had the same issue. Had to Reinstall NumPy version 1.19.4 manually to fix the error.,2021-01-09T09:33:46Z
123,757131548,"You are using pip version 19.0.3, however version 20.3.3 is available. You should consider upgrading via the 'pip install upgrade pip' command.",2021-01-09T10:52:05Z
124,757137765,"This seems to be caused by Generator.shuffle() calling ndim() in order to support the axis parameter: It seems like this step can be skipped when axis = 0. Not only because of this warning, but also because of the overhead of asarray.",2021-01-09T11:47:10Z
125,757199994,"OpenBLAS is more memory greedy by default. The short history of this difficulty is: 1.19.3 uses newer OpenBLAS to work around fmod problem, uses more memory by default 1.19.4 uses older OpenBLAS, has fmod problem 1.19.5 uses even newer OpenBLAS to work around fmod problem, tries to use less memory These memory problems seem docker specific, I expect the environment isn't providing accurate information about resources. The best solution going forward may be to using OPENBLAS_NUM_THREADS to limit the number of threads. frbg Here we go again.",2021-01-09T13:39:52Z
126,757220992,"Hm, I thought numpy had settled for building OpenBLAS with BUFFERSIZE=22 to keep the ""old"" memory footprint with 32MB GEMM buffers (at the expense of risking segfaults with large matrix sizes) ?",2021-01-09T14:01:53Z
127,757243417,"frbg It is actually BUFFERSIZE=20. This may be a different problem, but it looks the same at first glance.",2021-01-09T14:28:37Z
128,757331040,"Since the function is documented to support sequences, I agree that this was an oversight. Technically a regression in 1.18 (the API was new in 1.17 though).",2021-01-09T16:29:07Z
129,757335661," while sharing worries about duplication, your solution does seem substantially nicer than using jinja2 we wrapped a C library using ufuncs and the resulting jinja [template file]( is not pretty. Much of the work also ends up being done in python anyway, as we base the template on c function comments. That said, wieser is also right to warn about problems for newcomers I never really understood what is happening in the .c.src files...",2021-01-09T17:02:25Z
130,757350003,Thanks ,2021-01-09T18:45:51Z
131,757350211,"Thanks naut and wieser Is this true then: In Python 3, ""int"" is an arbitrary precision type but numpy still uses ""int"" to represent the C type ""long""? For reference: x = np.lcm(219060189739591200, 43) print (type(x)) <class 'numpy.int64' while in sympy x = sympy.lcm(219060189739591200, 43) print (type(x)) <class 'sympy.core.numbers.Integer'",2021-01-09T18:47:39Z
132,757351184, what is the lower bound of the softlimit for 1.19.3 (and if it is not too much trouble 1.19.2)?,2021-01-09T18:55:00Z
133,757351473,Thanks ,2021-01-09T18:57:24Z
134,757354641," In Python 3, ""int"" is an arbitrary precision type Yes but numpy still uses ""int"" to represent the C type ""long""? np.dtype(int) is the C long type but np.array(262) will always pick np.int64 which is sometimes ""long long"". ",2021-01-09T19:23:40Z
135,757365054,"Don't worry about the warning. No issue to fix in NumPy, Matti already gave the correct workaround, so I'll close this now. ",2021-01-09T20:48:27Z
136,757367743,"Thanks , we hope to do so when it's feasible (in addition to thin arm64 wheels, which seem preferable longterm). One blocker is that we build all our wheels on CI services, and none of them support MacOS ARM64 yet. And none of us have the hardware at the moment. We also still have some issues, in particular this build issue: gh17807. And this performance issue: gh17989. I'll link here as probably the most relevant issue for universal2 wheel building.",2021-01-09T21:10:57Z
137,757368305,This should be picking up a 1.19.x wheel though maybe pip too old?,2021-01-09T21:15:03Z
138,757377383,wieser has there been any progress with this?,2021-01-09T22:32:48Z
139,757384294, what is the lower bound of the softlimit for 1.19.3 (and if it is not too much trouble 1.19.2)? Updated table in original: | version | approx minimum import limit | | | | | 1.19.4 | softlimit a 400000000 python3 | | 1.19.5 | softlimit a 1200000000 python3 | | 1.20.0rc2 | softlimit a 1200000000 python3 | | 1.19.3 | softlimit a 1200000000 python3 | | 1.19.2 | softlimit a 400000000 python3 |,2021-01-09T23:37:26Z
140,757393322," Do you have any specifics here? No :) I'm just spitballing. Someone who knows more about how OpenBLAS preallocates memory will need to address that. I find it curious that the memory usage didn't change between 1.19.3 and 1.19.5, my understanding is that it should have gone down, although I suppose that it is possible that the number of threads allocated increased in 1.19.5.",2021-01-10T01:02:53Z
141,757395097,I'm running into this on an NVIDIA Jetson (aarch64) took me a while to isolate it. Funny thing is that it fails in a virtualenv but seems to be working if you install it at the system level. The NVIDIA NGC containers from install numpy directly into the system libraries and they don't have this issue.,2021-01-10T01:19:53Z
142,757396917,Does this need a backport?,2021-01-10T01:33:31Z
143,757397032,"I tagged it for backport, if it doesn't need it, please remove the tag.",2021-01-10T01:34:51Z
144,757406071,"Just curious about something since I've been following the work to add typing support for dtypes: won't making a mutable container type covariant cause issues? For the usual reasons that, e.g. if my_func accepts ndarray[Any, np.dtype[np.float64]) it could assign values of float64 to the array, but calling my_func with ndarray[Any, np.dtype[np.float32]) would no longer raise typecheck errors despite being wrong.",2021-01-10T03:15:59Z
145,757415875,There's open issue already ,2021-01-10T04:55:24Z
146,757433729,"Yes, I should have marked it for backport since otherwise compilation with visual studio 2019 fails.",2021-01-10T07:47:10Z
147,757447665,"Accelerate should be avoided for NumPy as it has bugs. NumPy doesn't use gfortran, SciPy does. But there currently no arm64darwin OpenBLAS build on I am not sure how we would build that without support for arm64darwin from a CI system.",2021-01-10T09:50:55Z
148,757448101,I opened MacPython/openblaslibs about building OpenBLAS for Allple Apple M1 silicon.,2021-01-10T09:54:07Z
149,757451297," I am not sure how we would build that without support for arm64darwin from a CI system. For building you only need a CI system that provides macOS 11 hosts, or with some effort macOS 10.15 hosts with Xcode 12.2. For testing you actually need M1 systems in the CI system, which will likely be a blocker for you and could take some time (I have no idea how constrained the supply of M1 systems is, but I expect that a CI provider like Azure Pipelines will require a lot of M1 systems). ",2021-01-10T10:20:56Z
150,757451600, Accelerate should be avoided for NumPy as it has bugs. Have you (the numpy project) files bugs about this in Apple's tracker? Or are these bugs in numpy's use of Accelerate? ,2021-01-10T10:22:54Z
151,757454298," Have you (the numpy project) files bugs about this in Apple's tracker? Or are these bugs in numpy's use of Accelerate? Yes, multiple. Also other scientific projects Apple basically doesn't care and hasn't been serious about maintaining Accelerate in a long time. Not enough value in it for them I guess. SciPy has more linear algebra functionality than NumPy, and dropped Accelerate earlier than NumPy. We have two good options for BLAS/LAPACK OpenBLAS (our default for wheels) and Intel MKL. So we'd rather spend our effort on OpenBLAS rather than work around Apple's poor support.",2021-01-10T10:43:43Z
152,757455123,"I feel your pain, I'm hearing similar experiences from app developers and most issues I've filed with them have gone completely unanswered :( ",2021-01-10T10:48:56Z
153,757459007,Could you add the actual module path as well (Cython.__file__ or so)?,2021-01-10T11:19:24Z
154,757465313,"Nope, OpenBLAS does not currently do resource detection beyond counting the number of available cores.Is that number constant for all entries in your table of minimum softrlimits above ? (In a way it would make me happy if the problem turned out to stem from something entirely different than the GEMM buffer, but I do not remember anybody creating huge sinkholes elsewhere in the code)",2021-01-10T12:05:48Z
155,757476401,"k13, I gdb'd into the code, specifically FLOAT_divide, that in turn goes into loops_arithm_fp.dispatch.c.src Nothing to do with loops_arithm_fp.dispatch.c.src, FLOAT_divide only performs true divide. However, this issue has been fixed by since if nonzero divided by zero should return inf. ",2021-01-10T13:28:31Z
156,757488155,"While the error comes from a potentially unexpected place on linux, I can't reproduce the crash. Just to be sure, you are not running on a apple silicon M1?",2021-01-10T14:45:24Z
157,757490636,"Nevermind, as the traceback reads, its a stack issue so just can be hard to reproduce.",2021-01-10T15:01:46Z
158,757512877, see the linked issues this casts python objects to strings which is a huge breaking change,2021-01-10T17:30:59Z
159,757515028,"Hmmm, thanks for the note. I can't reproduce it immediately, so let me dig in. It can't really be this PR, more likely and an error in gh18115 or even gh13578",2021-01-10T17:46:27Z
160,757542575,"Thanks! Increasing the number makes it reproducible easily (anything between 32 and 64 probably is technically broken), I already found the underlying issue and know the fix. I will fix it in the next few days, unless someone has interest in digging into it. (It is likely that this was unreachable code for a good time – although even then it has been around for many years.)",2021-01-10T20:54:41Z
161,757554152,Thanks .,2021-01-10T22:20:56Z
162,757667501,"I could find another solution, which does not require numpy 1.19.5. I am therefore closing this issue. Thank you for your help.",2021-01-11T07:11:10Z
163,757708833, s that number constant for all entries in your table of minimum softrlimits above ? Yes the only difference between the runs are the installed version of numpy,2021-01-11T08:27:36Z
164,757818736," python TypeError: '<' not supported between instances of 'float' and 'str' Here, no exception is raised, but the results are incorrect: Both of these are expected behavior, and match the behavior of the builtin sorted function on lists. We should probably update the documentation for sort to clarify that nans are sorted to the end _only for arrays of subtypes of np.inexact_.",2021-01-11T09:45:59Z
165,757826959," Could you add the actual module path as well (Cython.__file__ or so)? Good idea, done.",2021-01-11T09:51:15Z
166,757853777," Perhaps we can retire that portion of the governance document and have it be superseded by this NEP, and then have this NEP specify more clearly what defines those relationships and how those relationships work. Agreed. There's no compelling reason for that to be in the governance document itself. I'll take that along in my planned update.",2021-01-11T10:17:32Z
167,757857653," Perhaps a sentence after the list of dollar amounts that forward reference the discussions on institutional partners. agreed that would help, done",2021-01-11T10:24:36Z
168,757863466,Thanks ,2021-01-11T10:35:44Z
169,757984190,"Hi , sorry for the long pause in the PR. I noticed the following TC causing crash in specific environment. I think handling return value for self_descr for overflow should take care of it. Now to the overflow: Is it defined behaviour to do np.uint(1)?",2021-01-11T14:25:05Z
170,757997220," Which is not something we can easily do but if we're going to expose an alias in numpy.typing as public API, why not make it the thing people should be using for new code when working with integers? If the plan is to expose them then I'd definitely agree with you. The current aliases are IMO too useful to remove in their entirety, so instead I'd propose to just give them a more apropiate. Perhaps something along the lines of _IntLike_co and _FloatLike_co (the same holds for [_ArrayLike<X](",2021-01-11T14:46:00Z
171,758002854," There is the [numpy.typing]( docs page, though nothing specifically for ndarray[Shape, DType] yet, as this PR was just aimed at making ndarray a parametrized generic. I'm planning on exposing a runtimesubscriptable version of np.ndarray soon, something which I'd like to accompany with a documentation update.",2021-01-11T14:55:06Z
172,758008127," Is it defined behaviour to do np.uint(1)? I think so, although it may not be formalized. C allows it, but maybe we should make an explicit choice.",2021-01-11T15:03:39Z
173,758025507,"Lets say that it was once explicitly implemented that uint(1) will work, so for now I will say yes. For comparisons it would be best if this can't happen (the only problematic thing is probably uint64 == int64 using float and losing precision though). It is something that we could think about changing, but I am not sure it is worth the trouble and how annoying the work around is (since using 1 for maxint is not uncommon, even if technicall undefined by the C standard.)",2021-01-11T15:31:22Z
174,758032496,_co sounds good to me.,2021-01-11T15:42:53Z
175,758149393," oversight was able to split slip past the tests Yep, I thought to delay merging, but the tests passed and I was rushing.",2021-01-11T18:48:52Z
176,758153829,"Luckily our ""extended"" test suits in pandas and astropy both caught it easily ;)",2021-01-11T18:56:20Z
177,758171894,"It is happening for me, using python 3.9 in a completely clean new virtualenv. It seems, as mentioned, it is because the change from tp_print to tp_vectorcall_offset. I cannot install numpy. I am using cython version 0.29.21.",2021-01-11T19:28:42Z
178,758175656,"I am still confused by this. Are you installing from source? In that case, maybe try a git clean xdf (make sure to have no code changes applied or custom site.cfg). Also just to be sure python3.9 mcython V (assuming the python executable you are using is python3.9) is the right version and the pip you are using is right, by using python3.9 mpip instead of pip directly, just in case. Maybe you had a solution? (I am not quite sure why removing ""snoflakeconnector"" would make a difference)",2021-01-11T19:35:39Z
179,758215303,"Hmmm, test failure in: out = subprocess.check_output(['nm.exe', 'help']) because nm was not found, restarted the test probably flaky, just noting in case it happens more often.",2021-01-11T20:51:18Z
180,758222735,I have been seeing the nm.exe failure about once a week or so.,2021-01-11T21:05:21Z
181,758241818,"Looks safe and good to me, lets put this in. Thanks. (I suppose another pass is that _filling_ the result array can be slow. But, I guess that will usually not do much more than double the time it already took, so it is less likely to ""trap"" someone in an interactive session.)",2021-01-11T21:38:52Z
182,758245271,"I am sorry, I was trying to install a previous version of numpy. Current version installs ok with python 3.9. It is ok now.",2021-01-11T21:45:48Z
183,758247164,"Thanks for the followup, will close the issue then.",2021-01-11T21:49:47Z
184,758258944,Thanks Sebastian.,2021-01-11T22:15:29Z
185,758262811,"Thank you, I'm really happy to help, if however small! Perhaps I can play around with the implementation and propose a more effecient solution in the future (if possible.) For example, perhaps it would be more practical to check for signals every max_dims/2 times. ",2021-01-11T22:23:15Z
186,758265456,"Yeah, there may be a better place to check. I was a bit hesitant to just putting it at the entry of the function. Frankly, it is probably so lowoverhead, that would be OK as well, it just didn't seem high priority to check often.",2021-01-11T22:28:52Z
187,758451927,"I only know basics of Python, but can offer some testing under Yocto/OpenEmbedded if it would help.",2021-01-12T07:02:12Z
188,758453296,"I'm really sorry, but my scheduled got filled up so this set of PRs still has some loose ends. Might be able to do it later this month, I'm still interested in the topic.",2021-01-12T07:05:15Z
189,758454540,"We've all got our lifes, no problem. I found this series after 12+ hours fighting with SciPy and Yocto and just couldn't pass by. It's great that you haven't abandoned it. As an additional question any chance of backporting this to 1.17? That's what's included in Yocto's current LTS and would make life easier for people who do custom work adding SciPy on top of it. I'm asking more in the sense of viability, not anyone's timelines and availability.",2021-01-12T07:08:13Z
190,758490399, anybody have an eta on the 1.20.0 release? We will make the release decision Jan 6. One of the reasons for the slight delay was to allow some downstream projects to work around an expired deprecation. Has the decision been made regarding the 1.20.0 release?,2021-01-12T08:22:17Z
191,758497328,"Hmm bit late, not sure if a piecewise function is intended to return more output than input. Could this be done with just pandas by indexing based on your condition and then using the apply function. df[condition].apply(function) or using .loc, .iloc",2021-01-12T08:35:36Z
192,758499362," As an additional question any chance of backporting this to 1.17? Numpy only maintains two versions at a time so the only versions this patch could reach are 1.19 onwards, and it's likely that this won't make 1.19 at all and will end up waiting till 1.20 or 1.21.",2021-01-12T08:39:47Z
193,758552626,"As they say, apply is a convenience function & not the one to go for, if performance is important.",2021-01-12T10:12:44Z
194,758599066, so I was able to sync the fork but now another problem has appeared. I cannot push my local repo to origin. I've tried git push origin master and git push u origin master. I have also commited the branch,2021-01-12T11:38:02Z
195,758609525,Im using the binary version of VSC,2021-01-12T11:58:23Z
196,758622228,You cannot push to origin. You need to fork this repo (look at the top right corner of this page) push your branch to your repo issue a pull request,2021-01-12T12:23:58Z
197,758634557,"I believe the failures are unrelated, but I'm not sure.",2021-01-12T12:49:44Z
198,758651930,"It looks like this can be closed if the newest polynomial documentation is still not sufficient, feel free to reopen.",2021-01-12T13:23:24Z
199,758661236,"Pyarrow release or end of month, whichever comes first.",2021-01-12T13:40:29Z
200,758700164,Is there anything left to do in this issue or can it be closed? ,2021-01-12T14:40:03Z
201,758705632,"Unless we have concrete suggestions for improvements to the current documentation, I'm assuming this issue is no longer relevant. Feel free to reopen.",2021-01-12T14:48:42Z
202,758707183,Closing this as it does seem to be fixed in the [current documentation](,2021-01-12T14:51:09Z
203,758851445," Thanks for the fast reply and all the great links. I am very happy to see that there is actually some interest in this idea. I really like the idea of a ""transform"" hock/function, that would for sure address my needs. I suspect it indeed might be hard to argue for this though as direct use cases seem limited (I really only can think of the histogram one right now). I am happy to do a bit more research though and help work this out if there is something I can do with my ""limited"" knowledge of the ufunc internals.",2021-01-12T18:28:39Z
204,759059107,The generated documentation: ,2021-01-12T21:52:27Z
205,759060189,"As a side note: Let's wait with merging until is merged, as it would allow for the removal of an overload.",2021-01-12T21:53:36Z
206,759088579,"I can see this being nice to have, so I am not opposed. But I am a bit surprised that we want to make nested sequences a common thing for most command? I.e. I would expect it to be mainly a valid input for np.array and friends, and pretty much all other occurrences would require you to call np.asarray manually to type correctly?",2021-01-12T22:53:40Z
207,759243814,trail how did you install sudo aptget install libatlasbasedev in lambda function? ,2021-01-13T06:47:00Z
208,759329415,I worked after I installed the latest version numpy in PyCharm. I was using sympy.plot glad it worked.,2021-01-13T09:40:06Z
209,759341379,"Thanks. We know, working on it in ",2021-01-13T10:01:12Z
210,759344126,Oh sorry for the duplicate then,2021-01-13T10:05:43Z
211,759344715,"no worries, I'll leave this open till it's fixed, or someone else will open a new issue",2021-01-13T10:06:48Z
212,759345516,"For who needs the docs, they're still accessible here: ",2021-01-13T10:08:12Z
213,759371165,"The new page "" "" returns 404! This helps a lot!",2021-01-13T10:55:15Z
214,759389439, will the universal2 python.org installer pick up thin arm64 wheels if they are available and Python is started in native mode? ,2021-01-13T11:31:38Z
215,759403422,"That's more a pip question than python version, but yes pip will use arm64 wheels when running in natively on M1 Macs. ",2021-01-13T12:00:26Z
216,759405300," sorry just checking because I was surprised. You're saying that if I do: python m pip install numpy on an M1 machine, using the Python.org universal2 Python, and PyPI only has an arm64 wheel for Numpy, it will neverthless install that wheel, even though it does not satisfy the x86_64 part of universal2?",2021-01-13T12:04:28Z
217,759431066," ouch! and thanks for working that through, that's very helpful.",2021-01-13T12:56:04Z
218,759433848,"Interesting. I'm not sure that's a bad thing. There are scientific packages that already exceed the PyPI version limit, so a doubling of wheel size for what (for scientific libraries) is a very niche use case doesn't seem very sustainable. I didn't have the energy to jump in that packaging discussion, but I suspect scientific libs may prefer thin wheels. We work hard on keeping binary sizes reasonable.",2021-01-13T13:01:27Z
219,759434296,Should be working now. Please reopen if something is still not working. ,2021-01-13T13:02:20Z
220,759466278,"Yeah, I noticed that some some packages are very large. Those can always choose to not provide universal2 wheels at all, that will work for most users. At a risk of going completely offtopic, I'd like to have more control on what gets installed than pip is currently giving. When I'm doing something on my local machine only I might prefer a thin wheel (smaller, faster install), but when I intend to redistribute I might prefer a universal wheel (and possibly even one that's synthesised from two thin ones on PyPI). ",2021-01-13T13:59:31Z
221,759473439,"More control would be nice indeed, if universal2 stays perhaps as both a pip command line argument and .rcfile setting. I'd actually prefer if it didn't stay, because also for redistribution there isn't much reason to do it as universal2. The only good argument I saw was ""it's less effort to introduce"". There is no other situation where we mix OS and hardware platform support. The old universal PowerPC/Intel thing was also very annoying, and in my experience didn't work for the scientific stack anyway if you invoked it with the nonnative arch.",2021-01-13T14:11:03Z
222,759475884,"Just superficially, wouldn't it be more simple for Python.org to provide separate M1 and x86_64 installers as it does for Windows 32 and 64 bit? It's hard to imagine many people using the universal2 Python.org Python and always / mostly doing: arch x86_64 python3 And, as Ralf says, I bet that will usually break, when you get to install packages.",2021-01-13T14:15:11Z
223,759511015,"I don't think it's much to ask of a user that they know they have an M1 Mac, honestly. And for now, it would be reasonable to make the x86_64 installer the default, so they have to specifically ask for the M1 build. I presume an x86_64 build will also work on M1 via Rosetta? What happens for: arch x86_64 python3 m pip install numpy Does this look for an x86_64 wheel before a universal2 wheel?",2021-01-13T15:10:16Z
224,759511567,"I don't understand what problem universal2 pip packages solve. If the package is purepython, pip will download that. If the package has cextension modules, pip should choose the proper hardware model.",2021-01-13T15:11:09Z
225,759527181," Universal support is more or less required for folks distributing application bundles, having separate application downloads for intel and arm is just not what Mac users expect. Universal support is not about being able to run x86_64 on arm, but about having a single binary that just works everywhere. That's specifically a general ""Mac end user"" problem though, and is unrelated to PyPI and wheels. Having thin wheels plus the right py2app tooling to glue two thin wheels together in a single .pkg/.dmg installer would be much better. ",2021-01-13T15:34:49Z
226,759527981," I expect that it will be years before we can ignore intel Macs, even after Apple transitions all their hardware to arm64. Agreed, at least 67 years I'd think.",2021-01-13T15:36:01Z
227,759529087,"The shape seems clearly a bug. nanpercentile and percentile should ideally return the same thing as well. I don't have much of an opinion on error vs NaN. (Percentile does seem a bit less clear than mean, which naturally goes to 0/0, but especially with axis, a NaN result may be useful).",2021-01-13T15:37:46Z
228,759529893,There was a brief outage of most of the website.,2021-01-13T15:38:59Z
229,759701852,What hardware?,2021-01-13T19:52:19Z
230,759705470,"<img width=""198"" alt=""Screen Shot 20210113 at 12 58 44 PM"" src="" Do you need more info?",2021-01-13T19:59:11Z
231,759707302,"No :0 M1 is not supported, we are working on it but we also need hardware testing support that we do not yet have. There are other issues open for this.",2021-01-13T20:02:29Z
232,759708936,"Ah, thanks for the info!",2021-01-13T20:05:40Z
233,759751404,I'm beginning to suspect this is all caused by the BUFFERSIZE=20 workaround not actually getting through to the compiler/preprocessor when building with gmake. (Looks like I dropped a crucial bit of my original patch that would make Makefile.system append the usersupplied BUFFERSIZE declaration to the CCOMMON_OPT). That _is_ a bit embarassing...,2021-01-13T21:30:11Z
234,759752952,"LGTM, thanks .",2021-01-13T21:33:16Z
235,759753459,"In general it would be better to also use PyMem_Malloc and friends here instead, but isn't really important.",2021-01-13T21:34:26Z
236,759774279,Thank you!,2021-01-13T22:18:56Z
237,759807545,I'm not convinced by this change isn't it sufficient to mark them const?,2021-01-13T23:00:50Z
238,759814756,"Sadly, marking them const produces compiler warnings because PyArg_ParseTupleAndKeywords takes a nonconst array even though it doesn't actually modify the array. git grep F 'char kwlist[]' shows that most of these arrays were already marked static in NumPy, and none are marked const. I'm just proposing that we be consistent and add static to the ones that didn't have it already.",2021-01-13T23:05:58Z
239,759815730,"I just concluded the same thing. The approach seems odd, but it's even how CPython itself does it. I'll maybe play around with godbolt to see if const is ultimately better, but at the end of the day consistency matters more than microoptimization here.",2021-01-13T23:08:42Z
240,759816667," takes a nonconst array even though it doesn't actually modify the array As far as I remember this is due to a shortcoming in C (fixed in C++), where a T const const argument cannot legally be passed a T.",2021-01-13T23:11:17Z
241,759844651,"Hmm, shouldn't valgrind notice this type of thing? I am wondering if we want to add a test for it, because I do not recall valgrind ever complaining about it (I run it semirgularly). OTOH, the type_tuple_type_resolver is a bit obscure and not used much (it is the dtype= argument and especially signature= to ufuncs). Maybe just going to merge this as is soon. Just curious are these PRs based on static code analysis?",2021-01-14T00:30:04Z
242,759853414,I made this pull request and because of [scanbuild]( warnings. was just something I noticed while looking through the code.,2021-01-14T00:54:58Z
243,759957542,"xref xianyi/OpenBLAS I added that change as a patch to openblaslibs and triggered a build in MacPython/openblaslibs. Once the tarballs are uploaded, I can try to play with the resulting openblas and see if it fixes this issue. ",2021-01-14T06:29:43Z
244,760085809,"There is a timeout. Toggling timestamps in the build/test log, it seems 20 minutes elapse between [this line]( and [this line](",2021-01-14T09:51:57Z
245,760170589,"Late reply, but thanks a lot for the quick followup and nice deprecation! Justed tested this, and this should work for us. There is still a difference compared to < 1.20: now __array_interface__ is accessed (and so a return value constructed), while before it was never even accessed at all. But that shouldn't be a problem for us (I only noticed it because for a mock in a test the property did raise NotImplementedError).",2021-01-14T12:39:03Z
246,760219383,What are the next steps? Can I do anything to push this forward? I do not want this to become a stale PR.,2021-01-14T14:09:01Z
247,760252396,"In one of the tests added in there is a note: NOTE: We actually do still call __array__, etc. but ignore the result in the end. For dtype=object we could optimize that away. So this doesn't seem to be true in practice? If the result would actually be ignored (or the error catched and ignored), then the issue would be fixed I think? ",2021-01-14T15:02:06Z
248,760255872,Thanks .,2021-01-14T15:07:07Z
249,760326857,"Seems like a fixed pandas bug, the line isinstance(np.bool_, type(np.dtype)) did change behaviour, but doesn't make much sense to begin with (if anything, it should be isinstance(np.bool, np.generic)). Unfortunately, the only solution is probably to upgrade pandas as well (I only checked that 1.0 doesn't have the fix yet).",2021-01-14T16:59:02Z
250,760330209,"Right, so I was using incompatible version and can fix it by downgrading (which I did) or properly upgrading (which I will do). In the future, I suppose I should check pandas before posting a bug report. Thanks! Closing.",2021-01-14T17:04:21Z
251,760334764,"Yeah, frankly, it would be nicer if a year old pandas would work without issues. But yeah, generally it is better if the base package (NumPy) is not (too much) newer than the downstream (pandas). For NumPy the usual safe bet is that 1 year newer should be fine (i.e. you may see warnings but everything works), this is an unfortunate exception.",2021-01-14T17:11:39Z
252,760372847,"I am not too surprised that we still have such errors in NumPy. I couldn't reproduce it immediately using the debug version shipped with debian (maybe it doesn't have full asserts). If you can do it very quickly, does 2 == np.bool_ and/or np.array([2]) == np.bool_ have the same problem?",2021-01-14T18:14:13Z
253,760384505,"Oh, I didn't fully parse your last comment. So I guess raising the error may be fine for shapely/you if we ensure there is a special case for object + last dimension reached (which is not relevant except for the assignment case). That special case is probably missing in the dtype discovery code right now. (Note: that would not help with np.array(..., dtype=object), however)",2021-01-14T18:25:25Z
254,760385133,"2 == np.bool_ is fine. np.array([2]) == np.bool_ crashes. Here's another example: In [3]: class Foo(object): ...: __array_priority__ = ""bar"" ...: In [4]: np.array([2]) == Foo() Assertion failed: (!PyErr_Occurred()), function _PyObject_FastCallDict, file Objects/call.c, line 90. Abort trap: 6 I think the key is that np.bool_ has an __array_priority__ member but it isn't a float. In [3]: type(np.bool_.__array_priority__) Out[3]: getset_descriptor ",2021-01-14T18:26:35Z
255,760389943,"Thanks, great debugging/tracking down! That explains where the error originates, I was thinking of the wrong place first and couldn't make sense of it. The issue is here: although it is not quite clear of GetPriority might suppress the error (I don't like error supression for no reason, but here it may make sense, unless we first ensure that we look up the priority only on instances.)",2021-01-14T18:35:23Z
256,760413171,"Thanks, I will just merge this. It doesn't seem to fix an actual bug, but is correct. Whether or not this causes undefined behaviour seems to not make a difference in this function (because it is used solely for error reporting, and that branch doesn't do nice error reporting). (This function is a mess, but I hope the solution is to just delete all of it in the next months.)",2021-01-14T19:19:57Z
257,760494361,"Hi , thanks for your PR. In order to get the right people to review it, it's better to be more explicit in the title and description. Similarly, your commit message [should be descriptive]( in order to add context to this fix. ",2021-01-14T21:42:07Z
258,760507699,"If this is still relevant, I'm wondering if the best place to add this discussion on the @ operator is [this page on linear algebra]( Note that this page is actually a mix of common linear algebra operations and functions in the np.linalg module maybe they should be split up? Finally, the [ndarray]( documentation says: Matrix operators @ and @= were introduced in Python 3.5 following PEP465. NumPy 1.10.0 has a preliminary implementation of @ for testing purposes. Further documentation can be found in the matmul documentation. Can we lose the preliminary/testing purposes warning?",2021-01-14T22:10:10Z
259,760525444,"Is this something we still want to do? I'm guessing since this hasn't been brought up in a while that we don't need the manpage, and this issue can be closed. ",2021-01-14T22:47:58Z
260,760572480,"i would still like to have a manpage associated with the tool, at least that's what we prefer in Debian to have (but it's definitely not a strong requirement)",2021-01-15T00:53:30Z
261,760666156,same problem to me while trying installing pandas under macos big sur 11.1 python is 3.8 and pip is 20.3.3,2021-01-15T06:00:25Z
262,760889970,"oopsie daisy, silly me",2021-01-15T11:34:32Z
263,761061078,"Never mind turns out there were, unfortunately, detrimental problems with the therein referenced PR",2021-01-15T17:00:51Z
264,761099971,Pinging and reich. Continuing from it turns out that the np.number variancy could be changed without issue.,2021-01-15T18:12:32Z
265,761101862,"This is expected. p=None triggers a (faster, more accurate) codepath for specifically for unweighted sampling. When you give it an explicit p, we use a different algorithm for weighted sampling. There is not a good way to test the explicit p that it is equivalent to being unweighted because of floating point inaccuracies. I can understand the desire that these two give the same answers, but I don't see a good way to reconcile the two except to abandon the faster and more accurate algorithm for unweighted sampling, which I don't think would be a good idea.",2021-01-15T18:16:00Z
266,761108997,"Thank you for your detailed explanation, I totally understand and agree with you! I assumed that they are equivalent and so this caused a bit of a headache when I was writing unit tests. Would it be possible to highlight this behaviour somehow in the documentation? ",2021-01-15T18:29:16Z
267,761118952,"This is a duplicate of , seems to be due to the compiler rearranging code.",2021-01-15T18:49:07Z
268,761243857, needs a rebase.,2021-01-15T23:06:57Z
269,761245101, needs rebase.,2021-01-15T23:10:36Z
270,761248195,"FWIW, if I set NPY_NUM_BUILD_JOBS=4, build times are actually slightly slower than if I don't set anything. Would love to see this issue fixed. In I'm working on making Spack build all Python packages in parallel by default.",2021-01-15T23:19:31Z
271,761284488,Thanks Bas.,2021-01-16T01:24:34Z
272,761297213," Deprecating sctypeDict would be useful too, but I'd rather we first clean up the documentation and remove usage in SciPy first, give a heads up to others, and do it in 1.22 or 1.23. do you feel it would be worthwhile to add a comment in the release note discouraging the use of sctypeDict?",2021-01-16T02:32:51Z
273,761543622,"A release note sounds like a good idea, as context with this PR. I'm not sure many people read it though; adding that note to is probably more effective longterm.",2021-01-16T10:54:30Z
274,761594949,Is this the beginning of more extensive work? It seems so far you have renamed two files. The issue spoke about There needs to be a clear distinction between testing the ufunc object (test_ufunc) and the ufuncs instances (test_umath). Currently the two types of tests are mixed between the files. The testing of the various ufuncs is far from complete.,2021-01-16T16:46:50Z
275,761614247,I get a segfault from _mac_os_check and I'm not even on ARM. This whole macos/brew/compiler/python3 combination keeps biting me everytime whenever I update something.,2021-01-16T18:45:14Z
276,761661052,"k13, I created a new pr that adds fast integer division intrinsics for all SIMD extensions, it should be merged before this pr.",2021-01-16T21:12:01Z
277,761696583,"The QR decomposition is not unique all the way down to the signs. One can flip signs in Q as long as you flip the corresponding signs in R. Some implementations enforce positive diagonals in R, but this is just a convention. Since we defer to LAPACK for these linear algebra operations, we follow its conventions, which do not enforce such a requirement.",2021-01-16T23:22:12Z
278,761709149,"So this is extreme bikeshedding now that the float issue has been discussed, but would you consider changing floydselect into floydrivestselect? Or perhaps just floydrivest or frselect?",2021-01-17T01:08:51Z
279,761737340,"Thanks, , I'll rebase once that's merged and add the dispatches  ",2021-01-17T05:31:04Z
280,761746590,"I think this should have an attribution, something like Based on the VCL library, which is (c) Copyright 20122020 Agner Fog and licensed under the Apache License version 2.0. is that correct?",2021-01-17T07:17:32Z
281,761772578," I think this should have an attribution, something like Based on the VCL library, which is (c) Copyright 20122020 Agner Fog and licensed under the Apache License version 2.0. is that correct? No that sounds wrong. Reminder, we don't do Apache2, see for example: We could change that at some point, but it's a significant change and needs a strong motivation and decision on the mailing list. VCL is a little hard to find, so here is a link: If this PR took code from there, that seems like a blocker for accepting it.",2021-01-17T11:07:07Z
282,761772753,"Now read the PR description, the whole PR is based on that, and it seems valuable. It may be worth considering, especially if there are no good alternatives. We'd be deciding to give up on GPLv2 compatibility.",2021-01-17T11:08:51Z
283,761779667," Hmmm... Its a little vague, not sure what im supposed to do",2021-01-17T11:24:50Z
284,761802147,"According to the above mention, the NumPy 1.19.5 release uses a workaround for the Windows 2004 bug, instead of waiting for Microsoft to fix it. NumPy 1.19.5 is a short bugfix release. Apart from fixing several bugs, the main improvement is the update to OpenBLAS 0.3.13 that works around the windows 2004 bug while not breaking execution on other platforms. This release supports Python 3.63.9 and is planned to be the last release in the 1.19.x cycle.",2021-01-17T12:12:13Z
285,761812595,", we can ask for relicensing, since we're not taking the same exact code plus the original code itself is based on T. Granlund and P. L. Montgomery work.",2021-01-17T13:26:33Z
286,761814845,", Why do we still support Darwin/PowerPC? this flag faltivec should be removed.",2021-01-17T13:44:07Z
287,761819448,", Much of the work also ends up being done in python anyway That's the whole idea behind this solution. That said, wieser is also right to warn about problems for newcomers I see it as more friendly for newcomers if you compare it with web template engines, It still python after all. ",2021-01-17T14:15:56Z
288,761836146,"I don't know about you but I find I forget how to use Jinja2 every time, and have to remind myself. ",2021-01-17T16:08:15Z
289,761836513," I don't know about you but I find I forget how to use Jinja2 every time, and have to remind myself. I've had that experience a couple times, but at least it's very easy to remind yourself from the documentation and stackoverflow questions.",2021-01-17T16:10:45Z
290,761845477,"I ran into this with CI breaking on Jetson NX & TX2 and took me a good while to isolate as well. Serves me right for using = instead of == in versioning. As noted, sticking with 1.19.4 works for now.",2021-01-17T17:10:03Z
291,761862762,CC: ,2021-01-17T19:03:42Z
292,761864783,"CentOS7 has glibc 2.17, Ubuntu 18.04 has 2.27. Is one of our builds on Ubuntu 18.04 so we can test this?",2021-01-17T19:18:13Z
293,761866660,I am on Ubuntu 18.04.,2021-01-17T19:31:51Z
294,761871065,"I thought one or more of our CI builds use ubuntu 18.04, so how do tests pass?",2021-01-17T20:02:48Z
295,761873215," I thought one or more of our CI builds use ubuntu 18.04, so how do tests pass? Can you point to a CI build using ubuntu 18.04 that runs f2py tests?",2021-01-17T20:19:22Z
296,761878808," Nice! Looks like having number invariant is not too bad after all, if that was the only substantial change. I thought e.g. arithmetic operators might need to get messier, but this version is just as clean (mixedprecision ops returned Unions before anyway so it's just a different Union EDIT: whoops I forgot that with covariance mypy simplifies the Union to just one dtype sorry). If this gets merged, then having ndarray covariant in dtype makes more sense. (I guess users could still do incorrect assignments on arrays of _abstract_ dtype, but that's fine and will probably be rare). ",2021-01-17T20:57:24Z
297,761880914,I'll review this PR. Please don't merge yet.,2021-01-17T21:12:41Z
298,761885851,"Just wanted to add to this old, prescient issue by that a ShapeError and BroadcastError would be really useful!",2021-01-17T21:47:54Z
299,761889974,I think I got away with adding AxisError a few years ago I think a sufficiently motivated contributor could do the same for BroadcastError.,2021-01-17T22:17:17Z
300,761892781," My interpretation of this is assert_almost_equal(bools...) should be an alias for assert_equal(bool...), for the purpose of being able to write parametric tests and do something sensible this is my opinion on what should be done. My understanding of this recommendation is that we check the types of actual and desired inside of assert_almost_equal. If they booleans then return assert_equals instead, as we do for ndarray: python if isinstance(actual, (ndarray, tuple, list)) \ or isinstance(desired, (ndarray, tuple, list)): return assert_array_almost_equal(actual, desired, decimal, err_msg) Is that right?",2021-01-17T22:35:58Z
301,762016884,"Most of the code was rewritten, while some of the code is very similar to Apache 2.0 based VSL, which is not compatible with current 3clause BSD License(Apache 2.0 is more restrictive), so we should either rewrite the similar code or ask Agner Fog modify the License.",2021-01-18T06:32:15Z
302,762056925,Thanks ,2021-01-18T07:52:59Z
303,762058932,"I thought the f2py tests are part of the test suite. We should be running them on at least one of the builds. Since the tests pass without this PR, I guess they are not running. We go through the trouble of installing a gfortran compiler and libraries, so we should run the tests. Maybe as part of this PR you could add whatever is needed to make them run? ",2021-01-18T07:56:56Z
304,762059827,Now I am paranoid that the tests are not running. Could you point to where this new test is run at least once in CI?,2021-01-18T07:58:34Z
305,762069526,See for instance ,2021-01-18T08:15:41Z
306,762077075,Thanks ,2021-01-18T08:28:26Z
307,762084322,"The f2py tests are running. There are other differences between local and CI ubuntu boxes that may affect the build/test results: local: 18.04.4, CI: 18.04.5 I using conda environment to build and test numpy.f2py, CI is installing numpy to system gcc version local: 9.3.0, CI: 7.5.0",2021-01-18T08:40:10Z
308,762105394," I found the explanation of why the callback tests pass in CI but not locally: when using ubuntu gcc 7.5.0 compiler (CI), __STDC_NO_THREADS__ is defined but when using conda gcc 9.3.0 compiler (local), __STDC_NO_THREADS__ is not defined. Hence, gcc 7.5.0 does not try to include threads.h while gcc 9.3.0 does. ",2021-01-18T09:13:57Z
309,762117306,Is the glibc version the standard way to detect threads.h ? [This discussion]( suggests to explicitly <features.h to get the __STDC_NO_THREADS__ definition. ,2021-01-18T09:33:25Z
310,762163997,"On the second thought, since both __STDC_NO_THREADS__ and threads.h are C11only specific, we should use __STDC_VERSION__ == 201112L anyway when trying to include threads.h. The glibc = 2.28 constraint might be redundant though: when using gcc std=c11 ..., __STDC_VERSION__ is 201710 when using gcc 9 and 201112 when using gcc7. What do you think, ?",2021-01-18T10:49:18Z
311,762166579," we should use __STDC_VERSION__ == 201112L anyway when trying to include threads.h. This sounds like the most reasonable choice, without the glibc version check",2021-01-18T10:53:20Z
312,762184132, can you tell me what this means and what i'm supposed to do?: There needs to be a clear distinction between testing the ufunc object (test_ufunc) and the ufuncs instances (test_umath). Currently the two types of tests are mixed between the files. The testing of the various ufuncs is far from complete.,2021-01-18T11:21:43Z
313,762222800, please review.,2021-01-18T12:34:46Z
314,762223632,"you should compare new implements in tril_indices and triu_indices with nonzero, Can you write a benchmark in bench_core.py?",2021-01-18T12:36:09Z
315,762229792,"Any thoughts on this , ?",2021-01-18T12:48:10Z
316,762275267,Don't merge until is resolved.,2021-01-18T14:11:59Z
317,762277900,It's not clear from the pr title but am I right in thinking this is actually just working around a bug in the Intel compiler where the appropriate macro doesn't end up defined? Or are we seeing this problem with gcc alone?,2021-01-18T14:16:30Z
318,762285857," It's not clear from the pr title but am I right in thinking this is actually just working around a bug in the Intel compiler where the appropriate macro doesn't end up defined? Or are we seeing this problem with gcc alone? The problem is with gcc (I don't have icc installed to test this atm). IIUC, gcc 9 is C17 (__STDC_VERSION__ == 201710) and it does not define __STDC_NO_THREADS__.",2021-01-18T14:30:16Z
319,762287890,"My reading of the Intel thread is that glibc itself is responsible for providing __STDC_NO_THREADS__, and that it uses a special gcc hook to inject a header into all source files that does so. The Intel issue is that the hook doesn't exist in icc but if you're using gcc, that suggests that something else is going wrong with the hook.",2021-01-18T14:33:49Z
320,762340980,As a side note: this PR has some minor (namerelated) merge conflicts with ,2021-01-18T16:05:29Z
321,762352718,"Hey , I tried multiple things to reproduce the issue locally, tried quay.io/pypa/manylinux2010_i686 and used a native 32bit system as well, but it's not crashing. Any idea why that particular TC is failing? I followed the memory through the code, no double free or invalid dereference either.",2021-01-18T16:26:40Z
322,762354811,"Also not sure if I am setting quay.io/pypa/manylinux2010_i686 correctly, for instance, I don't see python being installed in the image, yet it's there. I was curious about how are the packages being installed in the image.",2021-01-18T16:30:46Z
323,762358131," Added tests to bench_core.py. I don't know why it crashes though, any ideas? This one seems related: ",2021-01-18T16:36:39Z
324,762443236,"Maybe that overflow assert is simply incorrect? It does seem a bit off. Otherwise, my best idea is to install gdb in the CI and the job with that. Hopefully, getting a traceback will be sufficient. I did something for example [here (the command may be useful)]( (but for the travis CI).",2021-01-18T19:52:15Z
325,762477911," Seems harmless, although the PR title should explain what the change actually is hello, actually the PR title explains what the change actually is I didn't make any big changes all I did is a very small change is __inti_.py I think if you will see the code you will get it. :)",2021-01-18T21:27:02Z
326,762478548,"actually, this was my first PR so sorry if I don't make sense",2021-01-18T21:28:58Z
327,762512665,Looks good to me. Maybe cov instead of co? The latter has no obvious meaning.,2021-01-18T23:29:40Z
328,762525607,"LGTM, just a couple of style suggestions.",2021-01-19T00:19:26Z
329,762529951, The NumPy 1.16.x series was the last to support Python 2.7 and most of the shims have been stripped out. Is there a problem using 1.16?,2021-01-19T00:35:43Z
330,762533195," The NumPy 1.16.x series was the last to support Python 2.7 and most of the shims have been stripped out. Is there a problem using 1.16? Using 1.16 is what caused this issue. We then downgraded to 1.13.2, but the issue remains.",2021-01-19T00:47:08Z
331,762545123,"The relevant code is PY_VERSION_HEX = 0x03000000 NUMPY_IMPORT_ARRAY_RETVAL NULL NUMPY_IMPORT_ARRAY_RETVAL import_array() {if (_import_array() < 0) {PyErr_Print(); PyErr_SetString(PyExc_ImportError, ""numpy.core.multiarray failed to import""); return NUMPY_IMPORT_ARRAY_RETVAL; } } import_array1(ret) {if (_import_array() < 0) {PyErr_Print(); PyErr_SetString(PyExc_ImportError, ""numpy.core.multiarray failed to import""); return ret; } } import_array2(msg, ret) {if (_import_array() < 0) {PyErr_Print(); PyErr_SetString(PyExc_ImportError, msg); return ret; } } Looks to me like NUMPY_IMPORT_ARRAY_RETVAL is not being correctly defined for Python 2.7 for some reason.",2021-01-19T01:31:46Z
332,762547338,"Yes, this was my original thought, but putting a NULL after RETVAL for Python 2 does not resolve the issue. ",2021-01-19T01:40:56Z
333,762552435,"It looks like it _is_ returning NULL, and it shouldn't be. It should just be return;, i.e., no return value.",2021-01-19T02:00:34Z
334,762556129,"I don't think is related, can you add your crash logs here? From the CI perspective, The only failure is TestMatmul.test_vector_matrix_values.",2021-01-19T02:14:26Z
335,762563429,Oh sorry I didn't mean that the pull request was related to the CI issue. But that the solution in might make nonzero on par with my solution. I don't get how TestMatmul.test_vector_matrix_values can crash when I add some performance tests. I'll restart and see if it's just some CI hickup.,2021-01-19T02:39:26Z
336,762751561," , I have replaced the MIN/MAX Macros, placed the NPY_SIMD checking guard at the proper place, merged the count_nonzero_int16/32/64 functions into a single function and added benchmarks for the 4 int types.",2021-01-19T10:28:59Z
337,762759302,"Are you certain that 2.12 is the last version before __STDC_NO_THREADS__ was added, or is that simply the last version you tested without it? Either way, that looks reasonable though you should be checking __GLIBC__ 2 || (__GLIBC__ == 2 && __GLIBC_MINOR__ 12) for forwardscompatibility. I suppose the other option is to just discover if threads.h is available as part of config_test.",2021-01-19T10:43:41Z
338,762760800,"I assume the problem goes away if you use np.append(np.array([], dtype=int), 0), and so this is really just a consequence of [] being largely treated as a shorthand for np.asarray([]).",2021-01-19T10:46:31Z
339,762768273,"What's the status of this PR? Did all the components end up as separate PRs? If so, can we link them here then close this?",2021-01-19T11:00:45Z
340,762774566,"Huh, that commit is authored by someone () I've seen doing something completely different in another repository open source is a small world! Referencing the mailing list post in a comment might be a good idea",2021-01-19T11:12:23Z
341,762795779, sorry for forgetting about this for so long! That's completely fine :),2021-01-19T11:56:48Z
342,762828818," Looks good to me. Maybe cov instead of co? The latter has no obvious meaning. It is used consistently as suffix in both [typeshed]( and [typing]( when referring to covariant typevariables, so I'd argue there is very much a precedent for using co.",2021-01-19T13:08:05Z
343,762837673,wieser Do you want me to squash the commits?,2021-01-19T13:25:58Z
344,762842102," If setting __all__ is not possible, a workaround might be to ""explicitly reexport"" the objects using something this: python from ._array_like import ArrayLike as ArrayLike from ._dtype_like import DTypeLike as DTypeLike Nice and simple, I feel that this is approach would neatly fix the entire issue. Would you be willing to submit a PR?",2021-01-19T13:34:19Z
345,762855182,"Sure no problem with that. Which style is preferable? diff from ._array_like import _SupportsArray, ArrayLike + from ._array_like import _SupportsArray, ArrayLike as ArrayLike from ._shape import _Shape, _ShapeLike from ._dtype_like import _SupportsDType, _VoidDTypeLike, DTypeLike + from ._dtype_like import _SupportsDType, _VoidDTypeLike, DTypeLike as DTypeLike vs diff from ._array_like import _SupportsArray, ArrayLike + from ._array_like import _SupportsArray + from ._array_like import ArrayLike as ArrayLike from ._shape import _Shape, _ShapeLike from ._dtype_like import _SupportsDType, _VoidDTypeLike, DTypeLike + from ._dtype_like import _SupportsDType, _VoidDTypeLike + from ._dtype_like import DTypeLike as DTypeLike ",2021-01-19T13:57:39Z
346,762859701,Personally I prefer the first one. Feel free to use a (parenthesized) multiline import if the statement becomes too long.,2021-01-19T14:04:52Z
347,762864526,"Thanks , I'll try to refactor that bit. I had asked the python guys here: , seems like we can use PyLong_AsLong directly and try. The gdb in CI is very useful, thanks.",2021-01-19T14:12:39Z
348,762877840,"Indeed, I would classify this as correct, it doesn't seem feasible to special case []. If the list may be empty, you will just have to go the long route unfortunately. Happy to reopen, especially if anyone has an idea how to handle it better.",2021-01-19T14:32:09Z
349,762934071,Thanks Matti.,2021-01-19T15:52:40Z
350,762935043,Thanks Bas.,2021-01-19T15:54:01Z
351,762935578,Needs rebase.,2021-01-19T15:54:45Z
352,762939849,Thanks .,2021-01-19T16:00:44Z
353,762978498,"After more research, the [bug report]( is still marked as UNCONFIRMED.",2021-01-19T16:58:33Z
354,763020388,Benchmarks after the changes: before after ratio [f5f845b7] [fe3a8975] <master <enh_14415_fast_compare + 583±4ns 622±4ns 1.07 bench_scalar.ScalarMath.time_addition('longfloat') 3.63±0.03μs 3.45±0.03μs 0.95 bench_scalar.ScalarMath.time_addition_pyint('float32') 584±0.8ns 554±2ns 0.95 bench_scalar.ScalarMath.time_multiplication('complex256') 563±7ns 531±2ns 0.94 bench_scalar.ScalarMath.time_addition('float32') 567±2ns 527±3ns 0.93 bench_scalar.ScalarMath.time_multiplication('int64') 23.7±0.2μs 11.5±0.07μs 0.49 bench_scalar.ScalarMath.time_compare('int32') 24.1±0.7μs 11.7±0.2μs 0.48 bench_scalar.ScalarMath.time_compare('int16') 25.9±0.3μs 12.1±0.05μs 0.47 bench_scalar.ScalarMath.time_compare('complex64') 26.2±0.2μs 12.0±0.07μs 0.46 bench_scalar.ScalarMath.time_compare('float32') 26.3±0.5μs 11.9±0.03μs 0.45 bench_scalar.ScalarMath.time_compare('float16') ,2021-01-19T18:03:56Z
355,763047411,Thanks Bas.,2021-01-19T18:47:45Z
356,763050400,Thanks ,2021-01-19T18:53:05Z
357,763050573,Thanks .,2021-01-19T18:53:23Z
358,763051878,Needs rebase,2021-01-19T18:55:39Z
359,763155300,Thanks Pearu.,2021-01-19T21:33:14Z
360,763172041,"Unfortunately I cannot reproduce this (the underlying problem with OpenBLAS' cpu detection code) on my hardware, so I cannot confirm that the trivial attempt at fixing it with a volatile keyword in the current develop branch actually works. Are all failure reports from Nvidia Jetson devices ?",2021-01-19T22:06:10Z
361,763187846,"My initial report was using a Jetson device, I can try it on a different ARM CPU tomorrow",2021-01-19T22:36:48Z
362,763246983,raspberry pi 4 (armv7l) has no problems.,2021-01-20T00:48:17Z
363,763254096,wieser The final part is about to come.,2021-01-20T01:09:19Z
364,763283289,Can you provide the benchmark result by running python runtests.py benchcompare master bench_core.Core?,2021-01-20T02:24:35Z
365,763389429,"OpenBLAS does not provide DYNAMIC_ARCH for ARMV7 (no practical difference between the provided cpu targets), but I could not reproduce the problem on a Pi4 in 64bit ARMV8 mode.",2021-01-20T07:10:07Z
366,763403454,"I can't reproduce the issue on AWS Graviton2 either (ARMv8) but it does definitely occur in Jetson Nano, Jetson TX2 and Jetson Xavier NX. ",2021-01-20T07:40:27Z
367,763403571,Superseeded by which includes the present fix regarding __user__.,2021-01-20T07:40:42Z
368,763482477,My reproduction was not on a Jetson. I'm actually not sure what it is it's just a (pretty powerful) server I ssh into.,2021-01-20T09:53:04Z
369,763484857,"frbg if you can disassemble gotoblas_dynamic_init and look for the mrs instruction and the getauxval bl instruction before it, you can check the ordering of the tbz instruction relative to mrs (see [my previous comment](756230802)).",2021-01-20T09:56:55Z
370,763535153,"Thanks, for the explanation. Here's a 4line implementation of numpy_distutils.py module that is pure Python and should provide the required functionality: numpy_distutils.py import builtins as _builtins _builtins.__NUMPY_SETUP__ = True from numpy.distutils import _builtins.__NUMPY_SETUP__ = False However, as noted above, this will not be sufficient to build scipy: numpy.f2py has to be adjusted to not use numpy.core tools.",2021-01-20T11:20:16Z
371,763636807,"I just checked and the sever that reproduced on has a Cavium ThunderX CN8890 CPU (ARMv8.1), notably not a Jetson.",2021-01-20T14:20:53Z
372,763744543, If it fixes either of the two problems I'll definitely backport. I will be happy if we can get over the problems that have ensued since Windows 2004.,2021-01-20T16:14:42Z
373,763812152,"Interesting, thanks ThunderX is what the drone.io CI uses, though the OpenBLAS build has been using gcc 7.5 there. (Checked now that the current develop branch passes with gcc 10.2 _and_ the tentative patch for this issue)",2021-01-20T17:32:43Z
374,763888885,"Closing, this is configurable with np.errstate and a deliberate choice.",2021-01-20T19:45:29Z
375,763909860, could you confirm that this issue should indeed be closed? You should be able to find wheels in the [weekly builds]( in a few days,2021-01-20T20:24:14Z
376,763922085,Thanks Ralf.,2021-01-20T20:47:35Z
377,763963914,"OK, just opened a small PR to just revert it. I am very happy to add the deprecation (or even the more complex logic), but it just isn't very important, so doing anything for 1.20 already seems not even worth back porting release note additions :).",2021-01-20T21:35:32Z
378,763970718, I think is most of your problem here,2021-01-20T21:48:23Z
379,763973916,"Having this issue as well, it shows up with 1.20.0rc2 while testing. Ended up just watching some random YouTube video and took a risk. arch 86_64 zsh will push you to an x86_64 instance of zsh, then running pip3 install opencvpython seemed to install numpy1.19.5cp39cp39maxosx_10_9_x86_64.whl without issue. The solution above mentioning some openblas commands failed on the third install. Not sure what's going on but I now have like 4 versions of python installed, 2 versions of python3.9, 1 version of python 3.8, and another install of python 2.7.16. that ships with macOS.",2021-01-20T21:54:07Z
380,764006341,The new code improves the accuracy due to the use of FMA too. we will have to dispatch AVX2&FMA3 and AVX512F in runtime.,2021-01-20T22:54:20Z
381,764128909,Thanks Sebastian. I assume you will issue a FutureWarning at some point in 1.21 development. ,2021-01-21T00:58:44Z
382,764136996,Thanks Chunlin.,2021-01-21T01:02:37Z
383,764156076,"The dispatching solution for nonUFunc is not recommend in the past, If it is acceptable now, then we can have a discussion on the mailing list.",2021-01-21T01:23:00Z
384,764174382,"Mission Accomplished, closing now.",2021-01-21T01:51:35Z
385,764203736,you can read the benchmark guide [here]( ,2021-01-21T03:11:11Z
386,764438030,Thanks a lot!,2021-01-21T07:21:19Z
387,764640903,The branch has been rebased and the tests seem to be passing.,2021-01-21T13:25:46Z
388,764802091,RuntimeError: Broken toolchain: cannot link a simple C program ,2021-01-21T17:13:20Z
389,764802885," since the small integers are cached. I am not sure if that is a language feature or an implementation detail. While PyPy did implement a work around, other implementations may not appreciate being punished for not implementing this.",2021-01-21T17:14:17Z
390,764810429,"I'd assume it's an implementation detail, and certainly wouldn't recommend having _behavior_ depend on it. Having performance rely on implementation details doesn't strike me as a particularly big problem though, especially since things like function call overhead are influenced by that anyway.",2021-01-21T17:25:40Z
391,764837712,Hi! I'm new. Can I work on this issue?,2021-01-21T18:09:21Z
392,764860633,Why are you installing from source?,2021-01-21T18:49:52Z
393,764870429," Why are you installing from source? Sorry I might not understand this 100%, but we have a requirements.txt file in my repository with numpy being in it. When i run python m pip install r requirements.txt numpy fails. Is there a better way to install dependencies?",2021-01-21T19:07:03Z
394,764891893,"Pip is using a cached zip file, e.g., a file of source code. You can force it to use binary wheels python mpip install onlybinary numpy But the question is where the zip file came from, and if you are on a supported platform. What hardware? ",2021-01-21T19:43:50Z
395,764892103,"Unless there is a comment that someone is already busy on it (fairly recently), everything is always up for grabs. We don't really assign issues. So of course, go ahead.",2021-01-21T19:44:10Z
396,765012914,Thanks Bas.,2021-01-21T23:45:11Z
397,765060943,"I've added some comments on pytorch/pytorch I suspect that we'll need input from both communities to properly resolve this, so I apologize to all for the bifurcated discussion.",2021-01-22T01:57:27Z
398,765146469,"Sorry to hijack this thread but on a related topic on nonzero(), is there a reason why calling nonzero on a 1D array is orders of magnitude faster than a multidimensional array? For example, calling it on a Boolean array of shape (1000000,) is taking 40 µs, while it takes 1400 µs for an array of shape (1000,1000). Both arrays are identical in values and only differ in shape. Any idea what's the significant overhead cost here?",2021-01-22T05:53:04Z
399,765167975,"Duplicate of , . You should have led with the data point that you are using the new Apple Silicon M1 hardware. We don't currently support that hardware until we can run CI on it. You may have more luck with conda, I think they have binary packages for M1. In the meantime you can use the x86_64 Rosetta emulation.",2021-01-22T06:28:47Z
400,765172219," , without further investigation, I am speculating the overhead is coming from the use of an iterator and calls to get_multi_index function in whenever a multidimensional array is passed. In contrast, when a single dimensional array is passed, without an iterator the indices are computed using the npy_memchr function which is quite fast : ",2021-01-22T06:37:49Z
401,765278133,"Perhaps a better solution could be to replace line 157: epsneg_f128 = exp2(ld(113)) by something like: if( ld(1)!=ld(1)exp2(ld(113)) ): epsneg_f128 = exp2(ld(113)) elif( ld(1)!=ld(1)exp2(ld(64)) ): epsneg_f128 = exp2(ld(64)) elif( ld(1)!=ld(1)exp2(ld(53)) ): epsneg_f128 = exp2(ld(53)) else: raise Exception(""Problem with longdouble"") ",2021-01-22T09:43:33Z
402,765299943,"This is not a problem localized to import, right? If someone uses numpy and happens to get an overflow error, NumPy (or any wellbehaved library) internally detect that and converts the value to a +float('inf') or float('inf') without a signal handler. We could consider adding a signal handler of our own, but that just seems like an arms race: whoever wants to detect this situation will just invent a better way to work around our mask. When would a user of scientific python want to trap overflow signals that may naturally occur in mathematical calculations? ",2021-01-22T10:15:25Z
403,765313821,"I think that what to do with an overflow is user dependant. Some people may want use it as inf, other should stop the code with an error if overflow appears because it must not appear. The question could be ""Is it an expected behaviour that an overflow occurs ?"" During an application run, I think it is application (user and/or developer) dependant. During a library importation, I would answer no it isn't.",2021-01-22T10:39:53Z
404,765336275,"Hey , can you have a look at this PR. Thanks.",2021-01-22T11:26:08Z
405,765414346,this link: has disappeared since.. it would sure nice to find that excellent explanation again from somewhere..,2021-01-22T13:55:32Z
406,765439733,"It is but it is just a call to a LAPACK function DTRTRS so not technically impossible. I still think it is too specialized for inclusion in NumPy. Moreover, if a package wants really fast sampling for a particular covariance/precision structure, it is much simpler to write a lowlevel, highperformance implementation that can be shipped to users of the downstream package. This said, I do still there there is a case for the ""factor"" method. ",2021-01-22T14:29:52Z
407,765441291, I see you have done exactly that in your implementation. It is nice to see that this feature is being picked up.,2021-01-22T14:31:56Z
408,765455705,"I forgot about this NEP. Nice timing to comment on it . I'll at least link to it from my upcoming NEP about the array API standard. I'd also be interested to push this forward, just a matter of finding the time .....",2021-01-22T14:50:36Z
409,765457533,Let me also link which has all the relevant public/private/inbetween parts listed.,2021-01-22T14:52:53Z
410,765501136,Still trying to figure out why the azure tests have been failing. Hopefully will grant a bit of insight.,2021-01-22T15:51:07Z
411,765515918,"Thanks . charris added this to the 1.20.0 release milestone 14 hours ago would indeed be nice to take this along, the change should not require an RC3 I'd think. I'm happy to submit a PR this weekend is Sunday in time?",2021-01-22T16:10:25Z
412,765555826,"Arrow tagged 3.0.0 Monday, but a new pyarrow isn't up on PyPI yet, so Sunday will be fine. I'm planning on making the 1.20.0 release the weekend after this.",2021-01-22T17:02:50Z
413,765574521,Thanks .,2021-01-22T17:36:17Z
414,765577363,"Maybe we should change the assert to not fail for signal 9 and xfail instead. Note that the test didn't raise, so didn't run to completion.",2021-01-22T17:41:39Z
415,765607733," I still think it is too specialized for inclusion in NumPy. Sure, no worries I have an implementation of this here. Cool, thanks",2021-01-22T18:34:48Z
416,765690671,Is this page what you are looking for? ,2021-01-22T21:21:29Z
417,766038838,Thanks. Fix in gh18211.,2021-01-23T13:26:41Z
418,766097827,+1 the whole point of this new M1 mac is to do ML and numpy is pretty integral..,2021-01-23T15:34:16Z
419,766157468," if the warning is actually manually set, it would be something like npy_set_floatstatus. But most likely the opposite is the case: The C code (machine code), sets an error flag (the CPU does this, or well, the basic math library) for a certain operation. And our code would have to avoid taking that code path alltogether.",2021-01-23T18:32:45Z
420,766165770,"Thanks for the reviews. I updated the text for most of them, and replied to the ones I didn't quite agree with.",2021-01-23T19:24:13Z
421,766175121,Sorry for the clutter :) Github isn't made for line editing documents.,2021-01-23T20:35:10Z
422,766248652,The final fix is nearing release and is now in beta and release channels. Hopefully out next month. Windows 10 build 19042.782 We fixed an issue that causes the 64bit fmod() and remainder() functions to damage the Floating Point Unit (FPU) stack. ,2021-01-24T00:32:44Z
423,766297024,Thanks ,2021-01-24T05:52:51Z
424,766297128,Thanks ,2021-01-24T05:54:16Z
425,766298724,Adopted the xfail idea.,2021-01-24T06:12:15Z
426,766316579,"I disagree that it is ""too specialized to be included in Numpy"". The precision matrix parametrization of any statistical model is as natural as the covariance matrix parametrization. There is no good reason to consider the covariance matrix as ""natural"" and the precision matrix as ""specialized"". Secondly, a key feature of Numpy is speed and (related to this) scalability. Inverting a matrix is neither fast nor scalable.",2021-01-24T09:19:56Z
427,766323349,Thanks . It would be helpful if you could add a benchmark for matrix_power in benchmarks/benchmarks/bench_linalg.py that shows the performance benefit.,2021-01-24T10:16:48Z
428,766323665,"Merged, thanks . Please note that we prefer not to get pure style change PRs, because they're on average not worth the churn, review effort and CI time. If you want to try another PR, tackling a bug or documentation issue would be great.",2021-01-24T10:19:46Z
429,766323935, are you planning to update this PR with the change requested by ?,2021-01-24T10:22:22Z
430,766324287," I should probably also figure out why they're segfaulting at some point. /proc/cpuinfo mentions AVX2 and FMA but not FMA3, and that roughly exhausts my knowledge of how to debug the crashes. or any advice here on how to debug?",2021-01-24T10:25:29Z
431,766335257," Sorry for the clutter :) Github isn't made for line editing documents. No worries, thanks for the copy edits. All adopted.",2021-01-24T11:52:04Z
432,766345487,"Yes On Sun, Jan 24, 2021, 15:52 Ralf Gommers <notifications.com wrote: < are you planning to update this PR with the change requested by < — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub < or unsubscribe < . ",2021-01-24T13:09:27Z
433,766373481,This is a meaningless change. Please do not practice your git skills here: make your own repo for that.,2021-01-24T16:04:16Z
434,766373607,"This is a meaningless change. Please do not practice your git skills here, use a personal repo.",2021-01-24T16:05:08Z
435,766394990,"Let's give it a shot. We might want to make the xfail more general, i.e., exitcode anything but 0, but this should cover the immediate problem. ",2021-01-24T17:03:54Z
436,766395019,Thanks Matti.,2021-01-24T17:04:08Z
437,766397778,Thanks Bas.,2021-01-24T17:21:01Z
438,766402826,Did you mean to update the version of the theme in [the requirements file]( They have released 0.4.2,2021-01-24T17:55:27Z
439,766403090,Are you sure this was merged? I see a WARNING: unsupported theme option 'logo_link' given,2021-01-24T17:57:16Z
440,766403491,"Right, that was just a rebase just now. Will push once I confirm the docs in my local.",2021-01-24T17:59:56Z
441,766406214,"Whoops you're right, the merge wasn't included in this release, gotta wait until they do I guess.",2021-01-24T18:14:42Z
442,766421548,Thanks Ralf.,2021-01-24T19:55:44Z
443,766424010,"Hmm, ignoring the warning seems correct. Strange that it doesn't show in most test runs. Maybe warning tests can still sometimes be flaky :(. I thought the new python versions (and pytest) were smart enough to avoid flakiness... Thanks Chuck!",2021-01-24T20:11:23Z
444,766429209,", we use the term FMA3 instead of FMA to avoid any confusion with AMD/FMA4. , any advice here on how to debug? Testing CPU detecting mechanism python runtests.py t numpy/core/tests/test_cpu_features.py NOTE: You will have to patch test_cpu_features.py to enables Cygwin. Testing _SIMD module itself without any dispatched features: python runtests.py cpudispatch=""none"" t numpy/core/tests/test_simd_module.py python runtests.py cpudispatch=""none"" t numpy/core/tests/test_simd.py Testing _SIMD module with certain dispatched features, e.g. sse41 python runtests.py simdtest=""baseline sse41"" t numpy/core/tests/test_simd.py Checking the build log, and finally GDB or LLDB ",2021-01-24T20:47:06Z
445,766487394,The performance improvements looks good to me.,2021-01-25T01:44:13Z
446,766593745,Thanks ,2021-01-25T06:59:15Z
447,766600004,"Only 597 warnings left :(. Any idea why we get this one, which is probably 2/3 of the warnings? WARNING: c:identifier reference target not found: PyObject ",2021-01-25T07:10:16Z
448,766623509,"The code change here should not trigger the CI test failure of numpy.core.tests.test_multiarray.TestMethods.test_no_dgemv, will restart for verification.",2021-01-25T07:55:49Z
449,766623512,"It may look confusing to those who have written kLOCs of Python2 code. But every new Pythonista learns that 1 / 60 is a float. And there are 60 minutes in an hour, not 60.0. Of course I can remove these changes. But that would be my question: would you actually like to get the code to Python3 level entirely, i.e. remove all Python2 relics? ",2021-01-25T07:55:50Z
450,766623533,Thanks ,2021-01-25T07:55:52Z
451,766636563,"I think this change is fine but there are subtleties to watch out for 1 / float(x) and 1/x have different types when x has type np.float64. The two aren't quite interchangeable, but the former is sometimes risky as on longdouble it causes precision loss.",2021-01-25T08:18:30Z
452,766705174,"This was likely closed by , though it's still missing the acceptance.",2021-01-25T10:10:02Z
453,766711296,"Thanks vetinari, I forgot about this one. I'll close it through the next PR which changes the status to accepted and adds the mailing list links.",2021-01-25T10:20:20Z
454,766754039,A couple of release highlights for mypy 0.800: By far the most interesting seems to be the added support for [PEP 604]( _i.e._ we can now use the | operator for writing unions: python def func(a: int | None) None: ... ,2021-01-25T11:34:41Z
455,766782445,"Anyone knows what's up with the DOWNLOAD_OPENBLAS=1 ATLAS=None tests on [travis]( It seems to be failing quite a bit recently, despite a lack of any clear reason as to why.",2021-01-25T12:28:21Z
456,766876833,Thanks ,2021-01-25T15:01:30Z
457,766883369,"That runs on ppc64le (PowerPC), and the hardware has been flaky in the past. ",2021-01-25T15:10:23Z
458,766884163,Thanks and dependabot. ,2021-01-25T15:11:31Z
459,766887364,Comparing an anchor of the new [version of the theme]( to the [current version]( they fixed the problem with the header covering the test. (issue ),2021-01-25T15:16:13Z
460,766888175,Closed by updating the theme version to 0.4.2.,2021-01-25T15:17:18Z
461,766891406,"Good old dependabot, what a dependable fellow.",2021-01-25T15:21:59Z
462,766908651,"I don't think this is fixed by 0.4.2 (it's only a tiny bug fix release to fix a xarray display compatibility issue, it was not a release of latest master). It is fixed on master though, for the upcoming 0.5 release (and then you will need to set a headerheight variable)",2021-01-25T15:46:19Z
463,766912054,"Something changed. [This page]( (from the CI build of the new theme) no longer overlaps for us. For the next few minutes, until CI finishes, you can see the old page [here](",2021-01-25T15:51:14Z
464,766922727,"Hmm, on my browser (firefox, Ubuntu), both links give the incorrect offset ..",2021-01-25T16:05:32Z
465,766927114,On Firefox I can confirm the title is still overlapping for the CI build of the new theme. ,2021-01-25T16:12:04Z
466,766943522,"Closing, pyarrow should be fine now (and about to release).",2021-01-25T16:36:02Z
467,766968094, Any progress?,2021-01-25T17:12:12Z
468,766969826, Ping. Any update on progress getting the ARM infrastructure in Azure?,2021-01-25T17:14:58Z
469,766972598,This looks ready to go. Comments?,2021-01-25T17:17:21Z
470,766972624," Ping. Any update on progress getting the ARM infrastructure in Azure? Unfortunately, I cannot access the Azure infra, but the code looks great and it works great IMHO.",2021-01-25T17:17:24Z
471,766973461,Needs rebase. Is this still relevant?,2021-01-25T17:18:38Z
472,766974379,What is the status of this?,2021-01-25T17:20:01Z
473,766975313,wieser Look good to you?,2021-01-25T17:21:24Z
474,766980838," yes, made this PR to close , waiting to be released",2021-01-25T17:29:29Z
475,767014840,"nop, no clues. Most of the things I fix are due to another unrelated project that does not use sphinx/docutils. Might be from numpy/polynomial/, some functions seem to have a lot of c , and maybe sphinx don't resolve them properly ? and a few functions like polyvander seem to reference c but have no c parameters",2021-01-25T18:18:40Z
476,767017247,still present in version 1.19.2,2021-01-25T18:22:48Z
477,767019270,"Ahh, I see. I chose a page that is quite small. Because of differing zoom betwee the two versions, in one case the whole page fit better and showed the header, on the morezoomedin one it did now. ",2021-01-25T18:25:51Z
478,767025725,"I tried to give some more details about integers in the release notes, and mention the precision version float64 rather than float_ in the message. So hopefully it is a bit more clear, it may be slightly opinionated.",2021-01-25T18:35:48Z
479,767029488,Needs rebase. I'm inclined to close this unless it is still relevant. wieser thoughts?,2021-01-25T18:42:27Z
480,767031271,Is this still relevant after recent changes to stride_tricks and the addition of a rolling window?,2021-01-25T18:45:42Z
481,767033103,Does the SIMD work supercede this? Thoughts?,2021-01-25T18:48:52Z
482,767040538,"Since it wold require a rebase, lets close it. I think there may still be some value in it, so if anyone wants to pick it up, that sounds reasonable to me. But I agree that in the majority of cases the new sliding window function or broadcast_to probably will do the trick better. So the need should be much lower, and a keyword argument is probably not as convenient as the alternatives.",2021-01-25T19:01:03Z
483,767062505, Thanks for making the PR.,2021-01-25T19:34:37Z
484,767063935,"Hmmm, I thought I would look into this. But I am honestly not sure where we should be adding this flag, or if we actually should (additionally) even prod Python itself to add this flag. Am I right to think that most clang specific flags currently are inherited from Python itself?",2021-01-25T19:37:11Z
485,767080881,Where do we stand with this? Has it been discussed on the mailing list? Does it need to be or is this niche enough we can merge it as is?,2021-01-25T20:07:16Z
486,767085153,Last night's wheels built cleanly. Can we close this?,2021-01-25T20:15:08Z
487,767086166,"This has been mentioned multiple times on the list, just not super recently.",2021-01-25T20:16:58Z
488,767089123,"Yes. This was a problem on aarch64, but it seems fixed now. Triggering rebuilds succeeded in 2 out of 3, and the last after the fix went in (showed xfail).",2021-01-25T20:22:11Z
489,767183632,"I had the same problem. But I came up with a pretty simple solution. Please refer to the link below. Note: I think enabling/disabling mmap_mode is not really related to this problem. So in this case, feel free to do whatever you think is better for your particular situations.",2021-01-25T23:41:21Z
490,767257136,"I've validated I can get basic loading under softlimit 400MB with numpy1.21.0.dev0+485.gbec2b07dbcp38cp38manylinux2010_x86_64.whl, and a manually invoked build seems to build OK under 1GB softlimit. Our formal build processes won't have any change until 1.19.6 or later comes out on pip Thanks for your help",2021-01-26T03:02:43Z
491,767284249,Thanks vetinari .,2021-01-26T04:11:31Z
492,767288919,What was the reason to change the 1.20.0 release note?,2021-01-26T04:29:38Z
493,767291192,"Thanks, have to try how to fix that reference. Will fix (maybe tomorrow morning). There was some discussion that this release note/deprecation is maybe a bit unclear. I hope this is a bit clearer (and ""recipe style""), so the idea was to change it before the release.",2021-01-26T04:37:45Z
494,767341664,Thanks ,2021-01-26T06:49:59Z
495,767366158,"Thanks for the review , I shall fix the scalar conversion of other and then add the tests you mentioned as well.",2021-01-26T07:48:33Z
496,767373265,"IMO, A memcpy&memset method is provided to replace SIMD/uloop method for special cases such as readonly masked arrays, but I don't see enough evidence to proves the benefits here. if It's shows better performance than SIMD/uloop, then we should accept it. OTOH, this PR reminds me that is extending current sse2 functions to universal intrinsics, we can have a bench test after is merged.",2021-01-26T08:04:29Z
497,767394663,"No more followup, and indeed looks like a duplicate. I've also changed the title, because this seems unrelated to Alpine.",2021-01-26T08:47:29Z
498,767404482,I don't see a difference in the rendered documents [before]( and [after]( Am I missing something?,2021-01-26T09:05:47Z
499,767468332,"I have been bitten several times by this problem. Since the docstring says about shift If a tuple, then axis must be a tuple of the same size. it would be great to enforce this in the code and raise an exception if this condition is not fulfilled, rather than carrying out an operation which the documentation clearly declares invalid.",2021-01-26T11:01:29Z
500,767651773,Thanks ,2021-01-26T16:12:42Z
501,767663721,"I just hit this through pyupgrade also. The fact that %.4fformatting works but fstrings/.format doesn't, seems like a genuine bug.",2021-01-26T16:30:39Z
502,767667405,"I checked that the latest master, and geopandas tests are passing again now (didn't check with the 1.20.x maint branch, but assuming that that will be OK as well since the fix is backported). Thanks!",2021-01-26T16:36:10Z
503,767670613,The documentation is correct in its current form. The following paragraphs explain the difference between __array__ and other methods like __array_ufunc__ and __array_function__ in terms of how output types are handled.,2021-01-26T16:40:56Z
504,767682734,"No opinions from me note that I originally opened the issue only to track a TODO that was removed when some documentation was updated. I personally don't use the setops often, so I'm not sure what the most common usecases are.",2021-01-26T16:59:28Z
505,767684403,"Allow me to insist: the arr object is of type DiagonalArray, not ndarray, so the current block is incorrect. The following paragraph explains how one can make np.multiply return a DiagonalArray instead of a ndarray: How can we pass our custom array type through this function?... I believe that the current version assumes that arr is the output from np.multiply above, but it's not.",2021-01-26T17:01:47Z
506,767695913," I believe that the current version assumes that arr is the output from np.multiply above, but it's not. The output of np.multiply(arr, 2) is the object returned by __array__, i.e. the result of self._i np.eye(self._N): np.eye creates an ndarray object. Broken into finer steps: python arr = DiagonalArray(5, 1) type(arr) __main__.DiagonalArray b = np.multiply(arr, 2) type(b) numpy.ndarray ",2021-01-26T17:19:50Z
507,767700021," I understand all that but please read the document. It goes like this: py arr = DiagonalArray(5, 1) type(arr) __main__.DiagonalArray np.multiply(arr, 2) type(arr) numpy.ndarray which is wrong. Note how the output of np.multiply is not arr and it's not assigned to anything.",2021-01-26T17:26:32Z
508,767719630,"Just to be clear, type(np.multiply(arr, 2)) is calling type() on the output of np.multiply(arr, 2), which, as you point out, is not arr. Thus the original docs are correct. In your above example, you have python np.multiply(arr, 2) type(arr) Which would be incorrect, but is different than the original documentation.",2021-01-26T17:56:57Z
509,767725268," Thus the original docs are correct. Sorry, no but is different than the original documentation. No, it's not different. Are we looking at the same document? ",2021-01-26T18:05:54Z
510,767727763,"Wow huge brainfart on my part I just failed to interpret the diff correctly (I was assuming your change was what was already in the docs). So when I say ""the original docs are correct"", what I meant was ""your original proposed changes are correct! Sorry for all the noise.",2021-01-26T18:10:10Z
511,767730276,No wonder I couldn't figure out why we were disagreeing even though I felt like we were saying the same thing :) thanks for bearing with me ,2021-01-26T18:13:37Z
512,767740064,"This looks great, thanks !",2021-01-26T18:29:26Z
513,767797166,"Thanks Eric and Ross for figuring out how to correctly link the user guide... applied the changes, so should be good to go (assuming CI passes now).",2021-01-26T20:09:18Z
514,767803640," Any update on progress getting the ARM infrastructure in Azure? Let me be careful here... there is _progress_, but there is no _update_... yeah I think I can say that much. :)",2021-01-26T20:21:03Z
515,767828083," I think the test results above hold, for my purposes its working ok.",2021-01-26T21:07:09Z
516,767831325,Thanks sebastian.,2021-01-26T21:13:15Z
517,767896522,Thanks .,2021-01-26T23:26:15Z
518,768003980," and thanks for the comments/suggestions. I apologize for letting this sit, the Holidays were much more hectic than I imagined. I made most of the suggested changes, but then hit checks were not successful again. I'm having some trouble building NumPy with pip install .. I hit the import error: We have compiled some common reasons and troubleshooting tips at: Am I missing a step in the build process? or doing it wrong?",2021-01-27T03:52:09Z
519,768067439,More information needed. What error exactly are you seeing?,2021-01-27T06:34:07Z
520,768074889,The documentation for the [__array__ function]( is not very descriptive. It describes what the ufunc will not do if __array__ is present. It should describe what the prospective __array__ user is expected to do with self and optional dtype. That would be nice to expand upon here or in a followup PR.,2021-01-27T06:51:52Z
521,768147317,"There is a stray "" in longer build or would give incorrect results.""",2021-01-27T09:15:08Z
522,768317608,"It seems like we now have a (draft) pep for variadic generics, _i.e._ generics with not one but an arbitrary number of variables (like typing.Tuple): [PEP 646]( ",2021-01-27T14:21:24Z
523,768393910,"Just noticed this function has been added to version 1.20.0, hooray! Although writing np.lib.stride_tricks.sliding_window_view is a mouthful, would be great if this could be moved to the main module and have its name shortened ",2021-01-27T16:11:05Z
524,768397281,"Well, you're free to do python from numpy.lib.stride_stricks import sliding_window_view I think voiced against putting this in the main namespace on the mailing list, although I may be mistaken. But yes, sliding_window_view(arr, window_shape=(2, 2)) should do exactly what this issues asks for, so I think this issue can be closed. If you want to discuss naming, I'd recommend finding and responding to the mailing list thread, or opening a new issue.",2021-01-27T16:16:03Z
525,768417915,Sounds like we should consider this a bug (although when fixing I would probably just go with a VisibleDeprecationWarning just in case).,2021-01-27T16:47:24Z
526,768422683,"Seems correct to me, in the sense that max(..., 0) can (and does in the test run) return an integer and it is entirely possible for you to ensure that it does not. In many cases defining otypes may be a good thing to ensure this in any case.",2021-01-27T16:54:16Z
527,768477669,"On my side things are ok, just pending the indentation issues. Thanks, !",2021-01-27T18:17:51Z
528,768490168," On my side things are ok, just pending the indentation issues. Thanks, ! Thanks ! regarding indentation, do you mean the outputs or the code blocks? I think I caught the indentation issues we discussed above. I'll build again and check formatting. ",2021-01-27T18:39:19Z
529,768515076,Checkout the CI failure on cirecleCI build details. It is complaining about indentation and line 34.,2021-01-27T19:17:04Z
530,768518680,"We had discussed this, and we somewhat agreed that this is desireable, and should have FutureWarnings (or VisibleDeprecationWarning. A specific issue may be np.loadtext and genfromtxt since that might be a more common use case for using 0 and 1 as boolean, so that may need some special consideration. I hope to look at this, hopefully, FutureWarnings (and opting in to future behaviour) will be reasonably possible.",2021-01-27T19:22:51Z
531,768546192," Checkout the CI failure on cirecleCI build details. It is complaining about indentation and line 34. Thanks, I see that. I don't know why its complaining about the indent for the list. I tried making it one line in the last commit since it doesn't like my indents (third time's a charm). ",2021-01-27T20:09:54Z
532,768551057,"SeedSequence.spawn(n) always returns a list of n elements, even when n==1. Use child_seq = old_seed_seq.spawn(1)[0] to make sure that the child_seq is the actual desire SeedSequence.",2021-01-27T20:19:03Z
533,768580300,"Agreed in fact the documentation is so unclear, that I am not even sure what it is saying! Thus I am afraid that I might not be the one to come up with better wording.",2021-01-27T21:12:16Z
534,768585641,"If you are up for it, that would be great anyway, we can always iterate together (no pressure though). The only subtlety I can really think of, is that it should be completely fine to ignore dtype which would just trigger casting in NumPy itself.",2021-01-27T21:22:07Z
535,768601498,"I argued against step_size when we designed this function, because you can get that behavior already with sliding_window_view(x, 3)[::2].",2021-01-27T21:53:05Z
536,768610616," Checkout the CI failure on cirecleCI build details. It is complaining about indentation and line 34. Thanks, I see that. I don't know why its complaining about the indent for the list. I tried making it one line in the last commit since it doesn't like my indents (third time's a charm). L34 indentation was changed to a single line. It fixed the circleci build test. Looks like we are ready to merge. ",2021-01-27T22:07:21Z
537,768644629,"On second thought, we should probably just get rid of make dist completely, and build all release artifacts via CI. make dist is really weird, the Makefile for the docs should just build the docs, not jump through weird hoops to build numpy and put it in a nonstandard place.",2021-01-27T23:24:44Z
538,768649065,Thanks Ralf.,2021-01-27T23:34:25Z
539,768650619,runtests has most of the machinery needed to build docs.,2021-01-27T23:38:22Z
540,768814989,"OK, let's put this in asis, it is a good first step.",2021-01-28T05:44:53Z
541,768815084,Thanks ,2021-01-28T05:45:10Z
542,768817130,Thanks ,2021-01-28T05:51:15Z
543,768820366,Thanks ,2021-01-28T06:00:05Z
544,769150287,The test Build_Test / debug (pull_request) failed but not due to my commits but the container cannot download a package: E: Failed to fetch 404 Not Found [IP: 52.252.75.106 80] Unfortunately I don't know how to restart the test.,2021-01-28T15:09:59Z
545,769199368,"You return max(0.1, 0) which is an integer for the first value, and that is how it is designed to work; maybe that can be changed, but I doubt it, you always have similar issues if the type is fixed at some point.",2021-01-28T16:19:18Z
546,769484437,I make heavy use of numpy.nonzero() on computational geometry workwill be great to see some potential speedups there.,2021-01-29T00:18:25Z
547,769492740," Unfortunately I don't know how to restart the test. Don't worry about it. Because this adds a new function it needs to be run by the mailing list first. My own preference would be to start with an absolute_square ufunc, which ISTR has been discussed before. ",2021-01-29T00:40:03Z
548,769495112,"TBH, I'd rather see an attempt to add some abbreviated explanations to the ""see"" references, and I suspect a stab could be made for the types also.",2021-01-29T00:47:02Z
549,769548038,"range() generate python3 builtin integers that have unlimited precision, while numpy's arange() produces numbers are implemented using int32/int64 in C. so there is an implied information that ndarray uses ctypes. If you have a clearer explanation, welcome to submit a document PR.",2021-01-29T03:22:45Z
550,769582664,"Yeah, i'm expecting other things to be wrong most of those changes were autogneratd.",2021-01-29T05:22:35Z
551,769599315,"Merged, thanks ",2021-01-29T06:07:57Z
552,769624037, was this eventually fixed?,2021-01-29T07:10:11Z
553,769637500,Thanks ,2021-01-29T07:41:50Z
554,769646623,"apparently not yet import numpy as np np.array(['a', 1]).dtype dtype('<U1') np.array([1, 'a']).dtype dtype('<U21') np.__version__ '1.19.2' ",2021-01-29T08:03:55Z
555,769766820,"Merged, thanks!",2021-01-29T12:07:08Z
556,769773173,"All green, merging. Thanks ",2021-01-29T12:20:45Z
557,769874297,"Its fixed and deprecated in 1.20, you have to use np.array([1, ""a""], dtype=""U""), which will give you the identical result in both cases to begin with. The result is than always length 1, just because that is the more common variant currently.",2021-01-29T15:30:58Z
558,769877420,"Sorry, its a bit ""weirder""... Just to be clear: Right now, its deprecated, but you get U21 always. If you use dtype=""U"" (which you should and will have to), the result is unchanged and gives you U1 always.",2021-01-29T15:36:35Z
559,770020488, Can this be closed?,2021-01-29T20:03:36Z
560,770022373,What is the status of this?,2021-01-29T20:07:16Z
561,770024043,"I was having bad errors trying to install numpy from a pyenv install of python3 on my M1 until I used this: OPENBLAS=""$(brew prefix openblas)"" pip install forcereinstall nocachedir numpy and numpy installed correctly (I had previously done brew install openblas). Also was still having problems installing pandas until I installed cython.",2021-01-29T20:10:27Z
562,770033988,I kicked this off to the 1.20.1 release as I expect the Windows update will be out before then.,2021-01-29T20:31:52Z
563,770052523,Thanks Sebastian.,2021-01-29T21:11:58Z
564,770055961,"Changes are harmless, so in it goes. Thanks .",2021-01-29T21:19:42Z
565,770198448,"Any news about that stuff? It's a while ago, but the issue remains, I guess.",2021-01-30T11:34:17Z
566,770234413,Should we close this?,2021-01-30T16:06:53Z
567,770234678,wieser Just thought I'd ping this to check the status.,2021-01-30T16:08:48Z
568,770244252,"I'm not sure that I find the motivation for adding a separate function for this convincing. The accuracy issue in the example given is 2 eps, which is negligible: x = np.array([[1, 2], [3, 4]]) y = np.array([[2, 2], [4, 4]]) (np.linalg.norm(x y)2 2)/ np.finfo(np.float64).eps 2.0 The performance improvement amounts to avoiding one square root of a scalar on the order of 1 us. Neither seems worth going through a lot of trouble for.",2021-01-30T17:01:45Z
569,770247115,"Your code example doesn't actually use numpy in any meaningful way. You would see exactly the same behavior by defining nan = float(""nan"") and using nan instead of np.nan so while I agree this is annoying, I don't think numpy can do anything about it.",2021-01-30T17:23:33Z
570,770266416,data[39] == 27 which is the maximum value in this list.,2021-01-30T19:15:00Z
571,770266765,"If I clip the data such that there are multiple maximum values of 20, we get the argmax of 12 as expected: [] |9 clipped = np.clip(data, 100, 20) [] |10 np.argmax(clipped) 12 I don't think there is a bug here.",2021-01-30T19:17:23Z
572,770290216, thanks. Is the doc update on the TODO list?: copy the 1.20 docs to the repo move the stable symlink to the 1.20 directory,2021-01-30T22:26:59Z
573,770292331,I posted an updated version of this issue in the python bug tracker and got back a response that explains why this is expected behavior. Linking it here in case anyone else runs into the same or a similar issue: ,2021-01-30T22:46:53Z
574,770292590," Is the doc update on the TODO list?: It is, but make dist fails in the 1.20.x branch with cythonize problems, so I've put it on hold until I can deal with it. Not sure why runtests works and make dist fails though. I'm hoping that a backport of will fix things.",2021-01-30T22:49:47Z
575,770294608,"Problem seems to have been due to two installs of Python3.9 after I upgraded the system Jan 5, one in /usr/lib the other in /usr/local/lib.",2021-01-30T23:07:42Z
576,770299964,Lot of oddball fonts needed that will take a while to track down and install.,2021-01-30T23:54:44Z
577,770311409,Documentation up.,2021-01-31T01:49:49Z
578,770334297,Thanks ,2021-01-31T06:20:26Z
579,770334465,PR welcome,2021-01-31T06:22:42Z
580,770352505,This is presumably an artefact of a bad rst to Markdown converter?,2021-01-31T09:25:06Z
581,770353798,Ah very odd. I didn't even consider that this stuff was not present in source/release/1.20.0notes.rst. Seems to be a new GitHub rendering bug. Thanks for pointing that out. Closing.,2021-01-31T09:34:23Z
582,770375887,"cibuildwheel has full support now it looks like. Doesn't help without hardware or fixing the critical bugs we still have for M1 (see my first comment on this issue), but it's a good point of reference.",2021-01-31T12:36:49Z
583,770380108,"Bah! I'm an idiot, fixed it. Was a local installation issue.",2021-01-31T13:08:13Z
584,770380615,I think multibuild has full support too thanks to .,2021-01-31T13:12:35Z
585,770383740,"Restarting CI, something weird happened.",2021-01-31T13:36:40Z
586,770385252,This looks like a mistake during doc build uploading for 1.20.0. did you do that upload? ,2021-01-31T13:48:03Z
587,770385457,That makes very little sense. __array__ is supposed to simply return an ndarray with either a view or a copy of the data.,2021-01-31T13:49:43Z
588,770386747, It's the end of the month and pyarrow still hasn't released a fix pyarrow has been released beginning this week and NumPy 1.20 has also been released.,2021-01-31T13:59:34Z
589,770389008,"By ""that"" I assume you mean the current documentation that states: class.__array__([dtype]) If a class (ndarray subclass or not) having the __array__ method is used as the output object of an ufunc, results will not be written to the object returned by __array__. This practice will return TypeError. ",2021-01-31T14:15:59Z
590,770389671,I convert the notes from rst to md for github using pandoc. The conversion often needs a little fixing up and I missed that. The big fixup needed for 1.20.0 was a table. It would be nice if the github window accepted rst files.,2021-01-31T14:20:26Z
591,770389723,I think all the pieces are in place to build a wheel. The question is if we wish to upload a wheel without testing it.,2021-01-31T14:20:42Z
592,770389950,"Maybe we can do a short delay, while we wait for some set of nominated volunteers to test the wheel, for now.",2021-01-31T14:22:35Z
593,770390962,"That explains it, thanks.",2021-01-31T14:29:38Z
594,770391077,Indeed.,2021-01-31T14:30:14Z
595,770393442,"The main translation problem is folks using back ticks for links in the release notes. That works fine when building the docs, not so well with standalone release notes.",2021-01-31T14:47:11Z
596,770395723," did you do that upload? Yes. The problem was that I was unable to build the docs before updating the release branch for further development. I thought that might be a problem but didn't see it after a quick check. I think the fix is to just check out the proper commit before the build and that was already on my list for today, the problem wasn't unexpected :)",2021-01-31T15:01:47Z
597,770401537,"The problem is that the documentation was built against my installed NumPy. I thought it was supposed to be built in its own environment. I can fix that, but it either needs fixing or to be in the notes. EDIT: That is one reason I'd like to see documentation builds made part of runtests, it already sets up an appropriate environment.",2021-01-31T15:40:51Z
598,770403613," EDIT: That is one reason I'd like to see documentation builds made part of runtests, it already sets up an appropriate environment. I'd like to build them in CI together with wheels, much more controlled. And same for the sdist, for SciPy we recently had an issue with that ( that could easily happen for NumPy as well.",2021-01-31T15:55:57Z
599,770403897," The problem is that the documentation was built against my installed NumPy. I thought it was supposed to be built in its own environment. I can fix that, but it either needs fixing or to be in the notes. make dist is indeed supposed to do that. Maybe you had PYTHONPATH defined that was overriding it or something.",2021-01-31T15:57:54Z
600,770404945," make dist is indeed supposed to do that. Maybe you had PYTHONPATH defined IIRC, make dist has never worked that way for me. I don't have PYTHONPATH in my environment. OTOH, NumPy is installed in /.local/lib/python3.9/sitepackages, which might be a problem for some reason.",2021-01-31T16:04:59Z
601,770407493,"OK, sadly this doesn't seem to be working very well. The Sphinx build on CircleCI timed out. I guess I'll have to find a way to make fewer git calls. For now, I'll close this PR.",2021-01-31T16:21:39Z
602,770408913,"This extension looks like a great idea. I wonder if by using GitPython you can avoid some overhead from repeated subprocess calls I get the impression it might work directly with the git filesystem and/or cache commit information, although I may be mistaken.",2021-01-31T16:30:46Z
603,770410348,"Thanks for the suggestion! AFAICT, GitPython also simply calls git: I don't know about caching though ...",2021-01-31T16:40:12Z
604,770410844,my understanding was that most of GitPython does not use the git.cmd module. There seems to be a lot of code that walks the git filesystem in other files in that repo.,2021-01-31T16:43:12Z
605,770412219,"Appears to have been fixed in master, but sadly not backported to the maintenance 1.20 branch. Okay, closing. ",2021-01-31T16:52:04Z
606,770421204,I was looking around to learn more about BLAS (still new to looking at lowlevel issues with numpy tbh). Found out that Apple has their own BLAS framework that might be usable. ,2021-01-31T17:50:51Z
607,770424103,": The functions numpy and scipy use from Acclerate are buggy and Apple has not shown an interest engaging with us to fix them, so we blocklisted using it for numpy and scipy. ",2021-01-31T18:09:03Z
608,770425788,"On that topic I suppose that there may be a much larger performance difference for some BLAS routines in Accelerate, on M1, compared to openblas. Has anyone checked for such differences? Is there any way we can call into the Accelerate routines that are not buggy, and give good performance?",2021-01-31T18:18:18Z
609,770427477,"If a commercial vendor wants to get their software integrated into an open source project, I think it is incumbent upon that vendor to a) show an interest b) promise long term support with a channel for interacting with the community and c) do most of the work themselves or pay us to maintain it. I think that reflects our current relationship with Intel and MKL, why should Apple and Accelerate be any different?",2021-01-31T18:30:10Z
610,770429099,"Two reasons Accelerate is more or less an operating system library and I think that is different from something like MKL, where we have to ship the library ourselves. And I suspect the performance differences for MKL are in the order of 10s of percents, almost all the time, but for Accelerate on M1, in particular, the differences may be much larger.",2021-01-31T18:42:32Z
611,770431888,Thanks ,2021-01-31T19:01:38Z
612,770436229,"brett / I'm sure the community would be supportive of you guys putting in the effort and energy to explore which numpy functions are compatible with Accelerate, then submitting a pull request that makes the appropriate adjustments to numpy code. As someone who has an M1 MBA, I'm looking forward to seeing your contributions in the coming weeks and months! ",2021-01-31T19:26:33Z
613,770436288," If Accelerate had fixed the bugs, this issue (and the others about avoiding Accelerate when building from source on M1) would not be open., it simply would have worked out of the box. So if you are using Accelerate for machine learning, I hope you have a very good test suite to verify your results.",2021-01-31T19:26:56Z
614,770437300,"Note that [in 1.10 at least]( the description is different: If a class (ndarray subclass or not) having the __array__ method is used as the output object of an ufunc, results will be written to the object returned by __array__. Similar conversion is done on input arrays. Checking with git blame, the change was made last May, in .",2021-01-31T19:33:08Z
615,770438184,"I should say that there is some interest in getting this working, so hopefully these problems will soon not be relevant anymore.",2021-01-31T19:38:49Z
616,770438755," Yes, NumPy leaks memory on import, but only once. Can you explain if this is a problem for you? I do check numpy with valgrind, but there are certain things during Python and NumPy import that simply are designed in a way that they must ""leak"" (exactly once).",2021-01-31T19:43:03Z
617,770443627," one small thing, if you this inside one of the many tests. Just in case it helps my little pytest plugin: could help with finding which test is failing exactly. (The github repo is in a slightly newer state, but it shouldn't matter much.)",2021-01-31T20:16:25Z
618,770447258,"Oh, I should specify one more thing, perhaps: we were holding back on having more general rules (e.g., just the stack up to fun:tuple_alloc), because there doesn't seem to be a reference to NumPy in there, and we didn't want to potentially mask leaks not caused by NumPy. Apart from that, we're perfectly happy. The main goal is to know what to suppress such that we can have Valgrind run in CI for pybind11.",2021-01-31T20:41:45Z
619,770448257,"Yeah, same running from the shell directly; no reported leaks with just import (and version printing). Maybe the cython version matters? import cython cython.__version__ '0.29.21' ",2021-01-31T20:48:01Z
620,770448965,"I did that first, when cleaning out NumPy: manual bisecting (and in the end, once you hit the individual test level, it is often necessary in any case). Then I wrote that, thinking it is a bit better ;). The plugin just flushes all initial errors right now, I happy to change that if it helps you. Just an offer if you think it might be useful. Aside from narrowing down tests, it doesn't actually do much.",2021-01-31T20:53:03Z
621,770449489," Yeah, same running from the shell directly; no reported leaks with just import (and version printing). Maybe the cython version matters? import cython cython.__version__ '0.29.21' $ python3.9dbg Python 3.9.1 (default, Dec 8 2020, 0352) [GCC 7.5.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. import cython cython.__version__ '0.26.1' That seems to be the same? Besides, I believe I really just installed the wheel from PyPI. I'm also using deadsnakes' python3.9dbg (which I suppose has a bunch of Debian patches), and so is our CI. Can that be a factor playing into this?",2021-01-31T20:56:53Z
622,770451160,"Thanks , I can reproduce it with the wheel, did not yet try to check the 1.20.x branch, but master did not diverge a whole lot. what cython version did we use for the wheel build? I still feel the cython modules are the most likely cause.",2021-01-31T21:08:19Z
623,770453011,"Nevermind, I can reproduce the leak on the 1.20 branch, just not on master branch (I am super surprised, because I would think the cython generation is pretty much unchanged; so maybe have to triple check). Note sure what relevant change happened in between, but at least I can reproduce it now. I also doubt it is a big issue (since it is on import only probably). But I would like it if you guys wouldn't even need a numpy suppression file :).",2021-01-31T21:21:17Z
624,770453336," But I would like it if you guys wouldn't even need a numpy suppression file :). That would be amazing! But if you just tell us ""these are the leaks you need to live with and you don't need to report similar ones in the future"", we're also totally OK with that :)",2021-01-31T21:23:35Z
625,770455398,"Thanks for confirming. We'll stick to the current suppression files, then, and hopefully in some future, we try without and find a beautiful world without suppression files ;)",2021-01-31T21:39:56Z
626,770455431,"Thanks for all your effort, !!",2021-01-31T21:40:13Z
627,770459019,"Can you update the annotations in [arrayprint.pyi]( as well? In total two changes will be necessary: 1. Import the SupportsIndex protocol right [here]( python if sys.version_info (3, 8): from typing import SupportsIndex else: from typing_extensions import SupportsIndex 2. Replace int with SupportsIndex (for the precision parameter) in the following functions: [set_printoptions]( [array2string]( [array_repr]( [array_str]( [printoptions](",2021-01-31T22:05:22Z
628,770481663," to play devils advocate here, but is the result correct? Issues with float32 dot products where quite common bug reports when users picked up accelerate (intentionally or not).",2021-02-01T00:25:32Z
629,770517277,"Having the .local install wasn't a problem, the newly compiled numpy version was properly installed, but could not be found. So this seems like a path problem. I just installed the 1.20.0 release in .local and everything was OK except for an extra compile step.",2021-02-01T02:14:20Z
630,770529528,Thanks .,2021-02-01T02:55:21Z
631,770678522,"It's possible, it just needs specialcasing. ",2021-02-01T08:39:42Z
632,770739325,Any updates here ? I can update [PR 18183]( once this gets merged.,2021-02-01T10:10:38Z
633,770875892,Thanks .,2021-02-01T13:58:19Z
634,770882325,Thanks Faas .,2021-02-01T14:07:06Z
635,770883048,Closed by .,2021-02-01T14:08:10Z
636,770895553,"I had previously tried to build a patched numpy against Accelerate for both BLAS and LAPACK using this: (before being able to test the Accelerate BLAS + netlib combo obtained from [gist]( With my patched numpy with Accelerate BLAS+LAPACK I had many tests failing with bad numerical values, including the polyfit check at numpy import time. But maybe something was wrong in my build config. In particular, my patched numpy build did not use [isuruf/vecLibFort](",2021-02-01T14:25:45Z
637,770902767,"Ok, I'll try to make a PR today if possibleto me it makes sense not to impose the change downstream for the special case if not broken/needed.",2021-02-01T14:36:11Z
638,770920234,"Can you list your dependencies? One that comes to mind is numba, which is only just merging my 5monthold PR to fix these warnings in their code (",2021-02-01T15:01:08Z
639,770924179,"Sure: numpy, pyproj, h5py, pykdree, netCDF4, libxml2, pint (no numba)",2021-02-01T15:06:56Z
640,770924758,"Can you run with warnings as errors, to get the full traceback?",2021-02-01T15:07:45Z
641,770925453,"I can do that this evening, I'll update here with the results ",2021-02-01T15:08:48Z
642,770929817,"Memoryviews can be detected and converted to ndarrays with asarray, then passed through the rest of the function just fine.",2021-02-01T15:14:25Z
643,770974866,"I wonder if we should instead change the MutableSequence check to Sequence? That would allow that path to automatically work for memoryview, and would avoid spurious warnings that precede errors when readonly sequences like tuple.",2021-02-01T16:17:53Z
644,770981517,"A missing object in the stub files is arguably ground for a backport, especially since these are rather simple and straightforward annotations. Are there objections against backporting these 3 lines to 1.20.1? ",2021-02-01T16:27:25Z
645,770982543," There's certainly some contention that me doing this downstream was suboptimal anyway, but I don't think there's any claim that it should be disallowed/considered a bug to shuffle a memoryview. That was specific to that particular use of memoryviews postshuffle, not the shuffling per se. This is a good change.",2021-02-01T16:29:00Z
646,770985299,I don't have a problem with backporting this.,2021-02-01T16:33:04Z
647,770987048, Want to make a PR?,2021-02-01T16:35:50Z
648,770989538,"Sure, I'll make a PR a bit later this week. Before that, I'm going to comb through the stub files once more though, to check if we missed any similar cases.",2021-02-01T16:39:29Z
649,771046076,"I have convinced myself there is nothing actionable in NumPy, so opened a cython issue: just in case. My hope is that with some new Python or Cython version, this just goes away. I feel this has to do with module cleanup, and I am not even certain Python itself is cleaned up in that regard yet.",2021-02-01T18:05:31Z
650,771050712," Thanks for investigating! I feel this has to do with module cleanup, and I am not even certain Python itself is cleaned up in that regard yet. CPython definitely isn't clean, but there are efforts to clean it up in some future version. Maybe one day...",2021-02-01T18:12:18Z
651,771051741,"Thanks again, !",2021-02-01T18:13:43Z
652,771078424,Thanks Bas.,2021-02-01T18:53:55Z
653,771080643,Thanks Bas.,2021-02-01T18:57:41Z
654,771082742,There is also a float96 that I don't see anywhere. It is extended precision (long double) on 32 bit linux platforms.,2021-02-01T19:01:24Z
655,771083322,"Hmmpf, allclose isn't really very useful for datetimes probably, but breaking this was not intentional really. The reason is that previous versions of NumPy were ill defined: In [31]: np.promote_types(""m8[ns]"", np.float64) TypeError: invalid type promotion In [32]: np.promote_types(np.float64, ""m8[ns]"") Out[32]: dtype('<m8[ns]') was assymetric with respect to timedelta promotion. Since simple math seems unaffected (it never worked), I am not sure we should try to undo this. Should we make isclose work specifically for timedelta64 maybe?",2021-02-01T19:02:23Z
656,771094835,"I'm afraid this one is on you , it didn't exist before ",2021-02-01T19:19:22Z
657,771097391,Thanks Bas.,2021-02-01T19:23:51Z
658,771142710,"Actually, looks like this was fixed [a couple of weeks ago]( in netCDF4 , just not made its way into the package yet. So please close this.",2021-02-01T20:39:21Z
659,771150291,Thanks for tracking this down yourself!,2021-02-01T20:53:19Z
660,771176439,"I think we can close this as wontfix/invalid, there's nothing actionable here as far as I can tell.",2021-02-01T21:37:44Z
661,771182400,"That makes sense. If two or more people confirm the tests pass and it's as fast as or faster than the x86_64 wheel, it should be safe to upload to PyPI.",2021-02-01T21:49:18Z
662,771193160,"Sorry, but this code was better unchanged using from None was a deliberate choice which I asked for here: ",2021-02-01T22:08:55Z
663,771206138,"I see. Thanks for the quick reply! wieser Sorry, but this code was better unchanged using from None was a deliberate choice which I asked for here: [ (comment)]( ",2021-02-01T22:31:48Z
664,771227764,"Does anyone know what these warnings are about, that show up in the travis build?: numpy/core/src/umath/scalarmath.c.src1: note: the ABI of passing aggregates with 16byte alignment has changed in GCC 5 I don't really see that they can be related to these changes.",2021-02-01T23:16:26Z
665,771241304,Hi I have a failed commit earlier. How can I suppress the former one? Thanks!,2021-02-01T23:49:15Z
666,771272445,I pushed in Eric's suggestion after confirming the random suite still passes locally for this feature branch.,2021-02-02T01:07:30Z
667,771274178,"Is there any chance these changes are related to the error reported here on aarch64?: I tried to look into it, but couldn't make much headway. I don't see anything wrong with our call. For all I know, it is just as likely this was always flaky.",2021-02-02T01:12:35Z
668,771329575,"shm.buf is basically a memoryview of an mmap.mmap, which gets closed as soon as shm goes out of scope. my suggestion is to not let shm go out of scope by subclassing ndarray and attaching the shm as an attribute. This way, the shm lives as long as the array does. class SHMArray(np.ndarray): from def __new__(cls, input_array, shm=None): obj = np.asarray(input_array).view(cls) obj.shm = shm return obj def __array_finalize__(self, obj): if obj is None: return self.shm = getattr(obj, 'shm', None) ",2021-02-02T03:33:49Z
669,771335237,this is a duplicate of ,2021-02-02T03:49:16Z
670,771336803,"Yeah, although it highlights a slight variation in np.ndarray rather than np.frombuffer. It would be nice if shared_memory.SharedMemory implemented the buffer protocol, then numpy could directly pick it as a base. Right now, I guess the trick with an array subclass is best. You can still call np.asarray() on that to get a baseclass array if you like.",2021-02-02T03:53:51Z
671,771418311,"Is timedelta64 the only builtin dtype that will fail [multiarray.result_type(y, 1.)]( I think we will need to specialclass datetime64 and what about S and U (on HEAD).",2021-02-02T07:07:19Z
672,771421682,"Linking to the PR for future reference: Unidata/netcdf4python, this is marked for release with netCDF4 1.5.6",2021-02-02T07:13:56Z
673,771424964,Thanks ,2021-02-02T07:20:17Z
674,771596620,"You're seeing two things happening here: unique requires entries to be sortable. The array np.array([1, ""1""], dtype=object) is not sortable, and also fails in unique. I think there are other issues open about this. There's some weird coercion going on that makes np.array([1, ""1""]) a string array but np.array([1, ""1"", None]) an object array. This means that unique somewhat works without None in the array, but notice that it replaced your 1 with ""1"". I think probably wants to remove this behavior, which would make both cases in your report fail.",2021-02-02T12:18:34Z
675,771612323, thoughts?,2021-02-02T12:49:16Z
676,771777781,"I think computing pvalues is out of scope for NumPy, and this issue would be more appropriate for SciPy. SciPy has both scipy.stats.pearsonr and scipy.stats.linregress. Both of these currently accept just a 1d array for x and y, but there are issues on the SciPy github repository related to extending pearsonr to handle higher dimensional arrays.",2021-02-02T16:46:39Z
677,771806790," datetime64 already cannot do any promotions for example (unlike for timedeltas, it never makes sense to convert a number to a date, while it can make sense to convert it to a time difference measured in ""seconds"" or ""milliseconds""). S and U are supposed to error in result_type. The consistent error behaviour seems like the only reasonable thing to me right now. But we could probably hack that behaviour above back for a while at least. Or we try and fix the places like np.isclose where it seems to affect users (which may be playing catchup a bit, admittedly).",2021-02-02T17:16:20Z
678,771819035, and would qualify for a backport in my opinion. The remaining two commits aren't nearly as important; I'd say that can remain exclusive to = 1.21 unless someone has a strong opinion about.,2021-02-02T17:29:51Z
679,771819370,"Marking as bug, because I think we should add the workaround (which will cause a Python error rather than a segfault). That won't actually help you but it would at least make it more clear that this pattern is invalid (and maybe why). We do have np.memmap, I wonder if we should consider including an arrayfromsharedmemory factory in NumPy independently. Although, there are likely good libraries that include that and more.",2021-02-02T17:30:11Z
680,771820431,It would be best if you made the backport PRs.,2021-02-02T17:31:13Z
681,771820436,I've found a handful of other missing annotations in I'll get create a PR to close this issue after it's been merged.,2021-02-02T17:31:13Z
682,771822437, It would be best if you made the backport PRs. Will do; I'll create the backport after this PR has been merged.,2021-02-02T17:33:17Z
683,771830274,"Sorry, looks like replaces this PR.",2021-02-02T17:42:33Z
684,771841976,"Thanks, LGTM. I find it a bit surprising that a memoryview isn't considered a mutable sequence (is that supposed to be only the length?), but I guess if Python changes it, we have a test that notifies us. I would be fine with a memoryview fast path as you had it, although I guess this might be nicer for backporting.",2021-02-02T17:56:12Z
685,771852768,"MutableSequences have to implement insert, which memoryview would never do.",2021-02-02T18:07:58Z
686,771877437,"Ah right. So I think this is still good enough to warn for tensorflow as well? In which case I think we should go with it or is there some other reason to prefer MutableSequence, that I am missing? Although, I would be happy to entertain the thought of making things more strict in 1.21.",2021-02-02T18:39:34Z
687,771884018,"Thanks, to be clear, are you actually running into this error?",2021-02-02T18:47:54Z
688,771930353,"The reason I ask is, that I do not really expect anyone hitting this. So if you do and it isn't completely obviously just an oversight I would like to know (even if you trivially work around or so).",2021-02-02T19:50:04Z
689,771940993,"Sorry, forgot about these. Adding static uniformly seems great to me (some of this might cause me a merge conflict, but that is trivial). Thanks .",2021-02-02T20:04:27Z
690,771942654,Thank you!,2021-02-02T20:06:48Z
691,771945663,"I doubt that is a measurable performance impact (and compilers might just optimize it in the next version), and am pretty ambivalent about the style change. So, I guess I will just put it in the next time I see the PR, unless anyone has an opinion.",2021-02-02T20:10:55Z
692,771951644,"Yes, I am, while debugging [lscsoft/lalsuite!1526]( What is the correct way in C to declare a new PyArray_Descr to wrap a specific kind of Python object?",2021-02-02T20:18:52Z
693,771972109,I'll do this one!,2021-02-02T20:46:55Z
694,771972372,"Out of curiosity, should abs(inf + nanj) be inf or nan? My first thought was nan because there's a nan in there, but the tests seem to be expecting inf.",2021-02-02T20:47:18Z
695,771975100," should abs(inf + nanj) be inf or nan? I am sure NaN is correct. If an inf result is tested, that could well be a bug in the complex abs implementation. EDIT: I am basing this on the fact that ""all NaNs are NaNs"" for complex numbers (i.e. inf+nanj should be the same as nan+nanj). If you disregard that, I guess an inf result does make sense. A NaN result would seem safer to me though.",2021-02-02T20:51:17Z
696,771990288," I tried to understand what the code does to see if there is some clear different solution, or this is a problem I need to dig into. But I have to admit, the whole SWIG logic is making it hard for me to really follow things. The dtype doesn't really seem to do a whole lot? For example, your copyswap function seems to simply copy memory around, so do you actually need the HAS_REF flag at all?",2021-02-02T21:11:42Z
697,771993284,"Just to note, my half plan is that we deprecate ""S"" mostly (which doesn't actually change much). It would still be valid input to some functions like np.array() that do honor the fact that it can represent any length; and those should be the vast majority of use cases. For all other cases, we would then force ""S0"" to get the same result without any ambiguity (at least in the long run).",2021-02-02T21:16:18Z
698,772009234,", , would you care to take over from here and follow up with ?",2021-02-02T21:39:32Z
699,772013849,"I have also spent the day struggling with this. I tried a few things and, ultimately, ended up reverting my Lambda function to runtime of Python 3.7. Then I used of the AWS layer for SciPy (provides a working numpy) + a shared layer for pandasgbq (provides a working pandas), found here ",2021-02-02T21:44:34Z
700,772033348,Thanks .,2021-02-02T22:04:30Z
701,772035578,Thanks Bas.,2021-02-02T22:06:59Z
702,772036118,Thanks Bas.,2021-02-02T22:07:36Z
703,772037780,Resolved as of ,2021-02-02T22:09:26Z
704,772038655,Thanks ,2021-02-02T22:10:28Z
705,772042955,"I tracked the numpy.core._multiarray_tests.npy_cabs call to numpy/core/src/npymath/npy_math_complex.c.src line 121, where it calls npy_hypotl and got stuck there. I'm going to leave that be and post my other work.",2021-02-02T22:15:43Z
706,772081487,"Hmm, the last commit seems to have removed that issue. I have honestly no clue if it was related or not and would be happy to back it out to see if anyone is curious.",2021-02-02T23:16:54Z
707,772083763, Do you want me to keep this up?,2021-02-02T23:22:16Z
708,772087070,"You can close it if you want. I'm running a last round of tests before creating the pull request, to see if any of the failing tests are passing now.",2021-02-02T23:30:06Z
709,772092209,"Makes sense, the usual safe bet is to say is to replace NaN with ""any possible value"". The oddball is just that for complex two values for which np.isnan() is True, will have different values for these cases. But I suppose that isn't really an issue.",2021-02-02T23:42:15Z
710,772116074,"Oh, one thing. If you want v.data[0].value to work, you would have to use np.record. That is a bit weirder, but np.dtype((np.record, np.dtype([(""value"", ""d"")]))) for example should do the trick if used as a dtype.",2021-02-03T00:38:45Z
711,772199603,The powerpc CI failure is certainly not related,2021-02-03T03:42:59Z
712,772199753,thanks ,2021-02-03T03:43:25Z
713,772240429,"Build errors appear not from this PR, see ",2021-02-03T05:27:35Z
714,772294768,"i think we should move the fast check build to the linux_native build so the flaky mingw does not prevent the rest of the CI from running. We could also mark that test as flaky, but then we will never pay attention to it when it fails",2021-02-03T07:21:56Z
715,772295697,Thanks ,2021-02-03T07:23:39Z
716,772296742,You're welcome . :),2021-02-03T07:25:41Z
717,772301902,Huh. It is still here but not easily accessible with the new theme. I wonder where we should link it. The [theme docs]( which use the theme themselves do not really expose it. I opened an issue with the theme authors to see if we are missing something.,2021-02-03T07:35:43Z
718,772302953,"Do you know how difficult it would be to set up a CI run with cygwin, at least for the 64bit case? Is it supported by github actions/Azure?",2021-02-03T07:37:51Z
719,772323618,"Annex G of the C standard is also a useful (and more easily available) source of information for these corner cases. For the C11 standard, §G.6p6 has: Each of the functions cabs and carg is specified by a formula in terms of a real function (whose special cases are covered in annex F): cabs(x + iy) = hypot(x, y) [...] while §F.10.4.3p1 has: hypot(±∞, y) returns +∞, even if y is a NaN.",2021-02-03T08:18:43Z
720,772328518," Now, I don't actually directly see an easy link to the genindex on Where is the link to be found? Upper right corner, there is a blue box ""index""",2021-02-03T08:27:48Z
721,772330309,"Ah, I see there was this ""index"" link in the top right corner, which is what was probably meant? ![image]( That indeed seems to be a feature of the old theme, that's not present in the new theme. Do you have an idea where you would like to see it in the new layout?",2021-02-03T08:31:05Z
722,772338497,Can you show what the error traceback used to look like vs what it now looks like?,2021-02-03T08:44:41Z
723,772379433,wieser can you please guide me on how can I check this. I tried to run the test using python c 'import numpy; numpy.test()' but this is not working for me. Thank you.,2021-02-03T09:51:29Z
724,772513944,"I've heard of someone who's done it. I'll see if I can find their work. The Azure WindowsFast failure seems to be a problem finding nm.exe, which I think is unrelated.",2021-02-03T13:41:06Z
725,772531437,[egortensin/setupcygwin]( looks promising. Do you want me to try adding that here? In another PR?,2021-02-03T14:00:24Z
726,772535308, i think we should move the fast check build to the linux_native build Good idea.,2021-02-03T14:05:47Z
727,772552294,You are correct.,2021-02-03T14:31:05Z
728,772570340,"PR welcome. As this is a bugfix, I think we should fix both [mtrand.pyx]( and [_generator.pyx](",2021-02-03T14:56:35Z
729,772594589,"I expect it will stay, unless this is an issue for you? But if you put in np.array(np.nan), chances are you get the old behaviour, and that difference makes things a bit tricky.",2021-02-03T15:28:56Z
730,772597976,"Thanks, seems good. This makes the initial check a ""full"" test run (instead of fast), but maybe it doesn't matter.",2021-02-03T15:33:06Z
731,772604641,"There are a few things already going on in this PR. test parameterization refactor a helperfunction implementation for cygwin blocklist some system library functions check for sys.platform == 'cygwin' or sys.platform == 'win32' rather than just the latter I think we should try to set up CI for cygwin in another PR, then circle back to this once it is working. That github action looks plausible, you would need the python package and then could follow the [way pypy overrides the PATH]( to make sure the cygwin python and gcc get used.",2021-02-03T15:42:22Z
732,772605169, does that make sense as a plan?,2021-02-03T15:43:08Z
733,772608457,"It is still the [fastest Azure job]( Maybe we should be looking at caching the 32bit build tools, which take 5 minutes to install per job.",2021-02-03T15:47:40Z
734,772610876,"Oh wow, lets just put it in. Thanks Matti!",2021-02-03T15:51:02Z
735,772611896,"I haven't seen [this compiler failure]( before: during IPA pass: profile numpy/core/src/multiarray/einsum_sumprod.c.src: In function ‘longdouble_sum_of_products_contig_three’: numpy/core/src/multiarray/einsum_sumprod.c.src1: internal compiler error: in coverage_begin_function, at coverage.c:656 1264 | } | ^ Please submit a full bug report, with preprocessed source if appropriate. See < for instructions. But I don't see it here, it all passed. Does github actions rerun automatically?",2021-02-03T15:52:21Z
736,772619204,"python import numpy as np x = np.arange(10) x = np.pad(x, 1, ""constant"", constant_values=np.array(np.nan)) Also errors for me. I actually prefer an error over a weird number. I assume this comes back to (Honestly, that paragraph is super confusing to me & I cannot reproduce the mentioned changes between 1.19.5 and 1.20.0. Also the paragraph suggest that numpy will error in fewer cases which contrasts to what I we see here for pad. And why would fewer errors that be a good change here? But I guess I am missing something...) ",2021-02-03T16:01:59Z
737,772619483,"Uhoh, I don't remember ever seeing it either. Thats a strange error, just pointing at the end of the file. Why didn't this cause CI to fail?!",2021-02-03T16:02:21Z
738,772648781,"I am not picky, myself. It would be excellent if it was in the sidebar or in the top banner. Even if it was present only in the main panel of the home page, that would be better than the current situation.",2021-02-03T16:40:41Z
739,772659782,wieser closing this PR as it's mentioned in the file that this is an autogenerated file and this does not need to be edited. Thank you.,2021-02-03T16:55:33Z
740,772674379,Thanks Tyler.,2021-02-03T17:15:11Z
741,772683390,"I opened gh18316 for the moment. But we may have to make a call what to do about it. Right now, my first thought is to just roll with it, but it depends a bit on how much this affects you/others. For the future? I don't quite know... Since I expect everyone prefers the error behaviour, I would expect us to stick with it, at least unless we want np.array([np.nan], dtype=int) to be a warning (forever), in which case aligning it in the long run may make sense.",2021-02-03T17:27:36Z
742,772748829,"In the meantime I've created a PR with some refactoring in order to reduce the git calls: I've reopened this PR as WIP and added my refactored branch: ab712cc54c1491baafd6c2d2257be8e5d62d4705 Sadly, this still times out the build process: Any ideas how I could improve this? I guess I'll have to display some timing information to see where most of the time is spent ...",2021-02-03T19:09:49Z
743,772755745,I'm pretty sure this file is _not_ autogenerated this is the file that generates the generated files!,2021-02-03T19:20:22Z
744,772768007,"This in itself is normal behaviour, of how the operating system and python reclaim memory. Try allocating x again repeatedly and you will see that the memory ""settles"" around those 800 MB.",2021-02-03T19:35:41Z
745,772778826,"Right. I once noticed this behaviour in one of my C extensions where I needed to repeatedly allocate and deallocate memory. It also happens with other memory allocators specially when each deallocation is significantly small compared to the total allocated memory. Memory allocators usually use a threshold on the amount of deallocation for deciding whether to give back that memory to prevent memory fragmentation, hence prevent performance degradation.",2021-02-03T19:52:45Z
746,772780966,The fix is now available in [KB4598291](,2021-02-03T19:56:25Z
747,772782560,"All the pages in generated are built from the numpy docstrings, so you could skip those.",2021-02-03T19:59:07Z
748,772829139,"Thanks for the first concise reproducer for this, that made it relatively simple to pinpoint the problem. Closing.",2021-02-03T21:16:14Z
749,772831123,Windows update is out. can you update to get the fix and try to reproduce?,2021-02-03T21:19:20Z
750,772863548, E AssertionError: assert False E + where False = <builtin method startswith of str object at 0xee344710('| Iterator SizeOf: 488') E + where <builtin method startswith of str object at 0xee344710 = '| Iterator SizeOf: 264'.startswith ,2021-02-03T22:15:49Z
751,772872687,"Went into a rabbit hole trying to add the other test, but actually found a way I think (although not the one I expected to use. The one I expected to use found an unrelated bug of course...). Thanks, the size of intp and pointers isn't fixed, so that makes sense. Just deleting should solve that issue (assuming we are OK with this stdout capture; I am struggling on capsys for the other test as well, but I think I will just give up there).",2021-02-03T22:33:32Z
752,772881258,The test failures look legitimate.,2021-02-03T22:51:49Z
753,772895950,If the test becomes to convoluted we might want to omit it.,2021-02-03T23:23:04Z
754,772896076,"OK, hopefully CI will be happy now. I realized that I have to use capfd instead of capsys and then I can avoid those ugly subprocess calls...",2021-02-03T23:23:20Z
755,772983258,"I have a look at NumPy repo, and find that not every Python file has a corresponding stub file, and not every function is typeannotated. It seems like, there is a certain decision or choice to select/decide which Python files or functions should be added type annotations in priority. I am curious that what is the selection decision? Besides the toplevel functions listed here, do Numpy consider any other plan? Thanks. :)",2021-02-04T02:41:42Z
756,772989725," has been merged now; all (public) objects in np.random are currently annotated as Any. While still very broad, this will silence all ""Module has no attribute ..."" errors. I am not sure whether adding these Any types for silencing Mypy errors is a good practice? My meaning is that type annotation aims to help understand code and static check type errors, but any types take no such role. I just feel contradictory about this. But I don't have any other ideas to quickly remove ""Module has no attribute ..."" errors.",2021-02-04T02:58:56Z
757,773006266," The test failures look legitimate. Yes, was looking on mobile and didn't read correctly. I'll start looking at this right now. If it does indeed seem like a headache, will omit the tests. Fingers crossed for an easy fix.",2021-02-04T03:47:02Z
758,773008239,"yes, you are right. I did it wrong. So is the code correct? should I reopen this PR?",2021-02-04T03:53:14Z
759,773055849,Hey sorry for the long pause. Basically when I added np.float128 test i noticed a crash. I'm working on fixing that adding some more cases.,2021-02-04T06:06:16Z
760,773241013,"Hi , Due to the sheer size and complexity of the numpy library, a large number of functions are currently defined (as Anybased placeholders) in the main namespace. The general idea is to move them to their own stub file once the annotations are properly implemented.",2021-02-04T11:33:54Z
761,773243904," I am not sure whether adding these Any types for silencing Mypy errors is a good practice? Considering the typing of numpy is very much a work in progress, I don't see any problem with this: The Anys will naturally disappear as more annotations will be added over the course of time. Improvements are of course possible and, in fact, would be very much welcome additions.",2021-02-04T11:39:28Z
762,773245625,Removed 7 aliases/classes annotated in bool8 bytes0 complex256 float128 object0 string_ void0,2021-02-04T11:42:26Z
763,773262055,I agree that this is the pragmatic approach that allows for improvement when people who rely on these modules have time to invest.,2021-02-04T12:13:06Z
764,773380406,"This style was introduced deliberately in by . I also think it looks a bit odd, and personally would have diverged from the pydata theme only when it came to numpy colors and/or fonts but I don't feel strongly enough to back those lines out.",2021-02-04T15:10:34Z
765,773398354,"Fair point, but I'd guess partitioning two elements should not take more than twice the time it takes to partition just one element(?), and the overhead is much more than twofold for small arrays.",2021-02-04T15:35:25Z
766,773399593,"Long pauses are typical, just make sure to bug me/us when you feel it would help. It is pretty heroic to dabble in these parts of NumPy!",2021-02-04T15:37:08Z
767,773405465,Wouldn't it make more sense to put the links in the table above these paragraphs?,2021-02-04T15:45:28Z
768,773432969,"Anyone knows what's going with the PyPy tests? The failure seems to be unrelated, but it's failing nevertheless.",2021-02-04T16:23:48Z
769,773442249,Looks odd to me also. EDIT: Might be an error?,2021-02-04T16:37:21Z
770,773447679,I brought this up here: ,2021-02-04T16:44:56Z
771,773452691,"I don't mind the right alignment. But if it trips anyone, lets just undo it? Maybe one thing is that it works best only at the very top of a page and not in the middle of a long text. (Think like a chapter start in a book.)",2021-02-04T16:50:52Z
772,773471872,"Umm, one thing is that most of our getitem functions do have this line if ((ap == NULL) || PyArray_ISBEHAVED_RO(ap)) { and I think there is some internal code that assumes passing NULL will work for basic/naive dtypes. Honestly, while calling with NULL should not be done (especially outside of NumPy), I think this is ""by design"" that we do call it with NULL for basic dtypes in a few places. This happens to be one of them, and I guess before that path was never taken for longdouble.",2021-02-04T17:18:46Z
773,773489547,Anecdotally I've found it 6x faster to manually loop rather than use add.at (not a completely elementwise loop) ,2021-02-04T17:46:22Z
774,773498730, Anyone knows what's going with the PyPy tests? It uses a nightly build since there is not an official release for win64 yet. Let's see if this repeats.,2021-02-04T18:01:21Z
775,773499677,"Hey, thanks for getting back to me! Your explanation makes a lot of sense. I see what the code is doing now, and it makes sense. I guess I just find it surprising that if you specify the bin width method, the produced bin width is not what the formula in the documentation. However, if the code is behaving as expected I guess it's best to close the issue.",2021-02-04T18:02:47Z
776,773509133,wieser Excellent idea. Looks like it worked: ,2021-02-04T18:18:21Z
777,773520766,", just checked the numpy==1.20.0 with both Python 3.8 and 3.9 after today's Windows patches the issue has gone.",2021-02-04T18:38:00Z
778,773525744, Thanks for the update. I'll close this now. Feel free to reopen if the problem returns.,2021-02-04T18:46:17Z
779,773527948, actually fixed this as well.,2021-02-04T18:49:47Z
780,773538141," do we have a nice place in np.typing to put things like np.typing.Float64 there? Such a rename would be the simplest solution... DTypes probably should/could be proper heaptypes as well, which would probably allow to fix this more easily, but it a swath to wade through (In some future, we probably want everything to be heap types anyway, because it should make certain things cleaner, such as HPy support). EDIT: I guess a toplevel name wouldn't solve the issue for rational and other user DTypes, although they are probably of niche significance here.",2021-02-04T19:07:20Z
781,773542026,Thanks ,2021-02-04T19:14:05Z
782,773546539,"It may be worth documenting this somewhere. A sentence like ""The actual number of bins is always chosen to divide the range into an integer number of bins that is at least as large as the estimate."", or something to that effect in histogram. Would you like to open a PR to include that, or would you like me to do it?",2021-02-04T19:21:29Z
783,773548226," Even then, centered would make sense, leftaligned would look good, but rightaligned is weird. I think it's legible either way, but I'll leave the issue open until the maintainers come to a consensus. ",2021-02-04T19:24:29Z
784,773557222,I can give that a go thanks!,2021-02-04T19:40:45Z
785,773596329,"The attempt at a 64bit Cygwin CI run using GitHub actions is up at . Setting up the Cygwin environment seems to be working fine, it looks like just the running the tests bit that's having trouble. I'm hoping someone here has experience with setting up that side of CI and can help get that working.",2021-02-04T20:51:45Z
786,773603521,"I suppose I should mention: python runtests.py gets through the build process fine, and I'm assuming it starts the testing process because it breaks when trying to import numpy.linalg because it can't find lapack_lite or _umath_linalg. I'm assuming the other CI has a way of setting things up so those modules are found, but I'm not sure what that is. I've been using python runtests.py from the NumPy root directory for testing on Cygwin for a while now without issues at this point. I'm not sure what's going on here, and have not been able to reproduce this locally.",2021-02-04T21:06:14Z
787,773607870,"Oops, I just realized that there is actually a simple work around that seems to be sufficient and only rely on the existence of dtype.type, which is not a very strong requirement.",2021-02-04T21:14:32Z
788,773615324,"I just realized that there is after all a simple solution to this. Sorry about this, we will have a fix in 1.20.1 I think, and that is currently planned to be released by next week.",2021-02-04T21:28:54Z
789,773635254,"Is this the same text used to skip CI on travis and/or Azure and/or shippable? It would be nice to have a githubspecific skip phrase as well, so we could select which CI service to skip. We should also document the skip phrases somewhere. Where would be a better place than googling it each time?",2021-02-04T22:08:17Z
790,773635451," would you be able to check if gh18332 works for you? It will allow pickling the class, but I am starting to expect the class is just part of a more elaborate scheme, and I am not sure that pans out with the fix?",2021-02-04T22:08:43Z
791,773658668," just to double check: will already resolve the issue, right?",2021-02-04T22:59:03Z
792,773660150,"Ofc, thanks for working on this Sebastian  Should I rebuild from source to try or do you think I can just apply that change to an existing build of NumPy 1.20?",2021-02-04T23:02:36Z
793,773664499,", yeah, thanks for looking. I had not thought of this approach/realized it works for static ""builtin"" types. I am not certain it will solve all issues around this, since joblib (and similar) might do more than just pickle the class (for one try to actually instantiate it with some arguments ;)... outrageous, I know!).",2021-02-04T23:12:44Z
794,773664667,"No need to rebuild, its just a pure python change. Thanks!",2021-02-04T23:13:09Z
795,773766979,"Ohh I see, yeah in other places, we are not calling getitem in a way we are calling here. I am experimenting with a few NumPy capi for getting the scalar values instead of just calling getitem. I think one of them will work eventually, I'll keep you updated. Thanks.",2021-02-05T03:44:51Z
796,773789721,"Just a FYI in joblib, we decided to change slightly the way our hashing work to avoid the problem: Just trying to be more explicit: if the only complaint about this problem was from joblib then one option could be to not fix this issue. Reading the comments above, it seems that Dask may be affected by this from (but not 100% clear yet)",2021-02-05T05:02:17Z
797,773790267,"Looks good! Thanks again for the quick fix Sebastian  python In [1]: import pickle ...: import numpy as np In [2]: t = type(np.arange(10, dtype=""int64"").dtype) In [3]: pickle.loads(pickle.dumps(t)) Out[3]: numpy.dtype[int64] ",2021-02-05T05:03:52Z
798,773813752,Thanks ,2021-02-05T06:05:51Z
799,773906129," Is this the same text used to skip CI on travis and/or Azure and/or shippable? It would be nice to have a githubspecific skip phrase as well, so we could select which CI service to skip. Yes, this PR contains a check for [skip github] which is specific.",2021-02-05T09:20:42Z
800,773906652, We should also document the skip phrases somewhere. Where would be a better place than googling it each time? Good idea. How about ,2021-02-05T09:21:41Z
801,773955629,Firstparent sounds like the right choice to me.,2021-02-05T10:48:12Z
802,773962009," , I have pushed updates.",2021-02-05T11:00:35Z
803,773985168,"OK I understand that you won't fix it, but at least please don't claim in the [documentation]( that they're equivalent! I know the documentation has a disclaimer that says ""note that allclose has different default values"", but the sentence ""The test is equivalent to allclose(actual, desired, rtol, atol)"" is very confusing.",2021-02-05T11:48:10Z
804,773994213,"The pypy37 build via setuppython succeeded, once again seeing the compiler error on the ""full"" CI run: numpy/core/src/multiarray/einsum_sumprod.c.src: In function ‘longdouble_sum_of_products_contig_three’: numpy/core/src/multiarray/einsum_sumprod.c.src1: internal compiler error: in coverage_begin_function, at coverage.c:656 1264 | } | ^ Please submit a full bug report, with preprocessed source if appropriate. See < for instructions. is there some bad interaction between SIMD and coverage?",2021-02-05T12:07:54Z
805,773998268,"Documenting it either in [Asking for your changes to be merged]( or [Additional things ...]( sounds good, with a small preference for the former. Would you like to add that here or in a separate PR?",2021-02-05T12:16:41Z
806,774034929," Documenting it either in Asking for your changes to be merged or Additional things ... sounds good, with a small preference for the former. Would you like to add that here or in a separate PR? Agree, I can put it there. Prefer a followup PR. This is actively annoying me with failed builds whenever I push any commit to my own fork, so I'd like to get it in as is.",2021-02-05T13:32:15Z
807,774044208,Can you suggest a less confusing version?,2021-02-05T13:50:07Z
808,774045989,The same error also occurs in numpy==1.19.5. Maybe I am just missing something and there is no bug?,2021-02-05T13:53:34Z
809,774048941,"Suggestion, maybe something like the following? ""Due to different default parameter values, its behaviour is different to allclose, but they are functionally equivalent.""",2021-02-05T13:59:02Z
810,774075085, Thoughts?,2021-02-05T14:44:36Z
811,774076674,Thanks ,2021-02-05T14:47:13Z
812,774082863, it may be necessary to copy this to each job Appveyor is the only platform that may need copies and a yml alias would be easier to maintain than multiple long strings.,2021-02-05T14:56:21Z
813,774082897,"Verified it works as expected on my fork: <img width=""678"" alt=""image"" src="" <img width=""378"" alt=""image"" src="" ",2021-02-05T14:56:26Z
814,774084689," what's a yml alias? What I meant is, you can only put this under a job, so may need to repeat it multiple times in a file (example: and per yml file.",2021-02-05T14:59:22Z
815,774089345, what's a yml alias? ,2021-02-05T15:07:13Z
816,774091334,"This problem also causes the @ operator to fail, since it relies on \_\_matmul__: py import numpy as np a = np.ones((3,4)) b = np.ones((4,3)) ma = np.ma.masked_array(a, mask=np.ones(a.size)) mb = np.ma.masked_array(b, mask=np.ones(b.size)) ma @ mb Result: Traceback (most recent call last): File ""<stdin"", line 1, in <module File ""/opt/anaconda3/envs/xelab/lib/python3.7/sitepackages/numpy/ma/core.py"", line 3049, in __array_wrap__ m = reduce(mask_or, [getmaskarray(arg) for arg in input_args]) File ""/opt/anaconda3/envs/xelab/lib/python3.7/sitepackages/numpy/ma/core.py"", line 1764, in mask_or return make_mask(umath.logical_or(m1, m2), copy=copy, shrink=shrink) ValueError: operands could not be broadcast together with shapes (3,4) (4,3) ",2021-02-05T15:10:38Z
817,774091740,"Thanks, learned something. Likely won't work though, from the atlassian link: YAML anchors and aliases cannot contain the ' [ ', ' ] ', ' { ', ' } ', and ' , ' characters. So looks like you cannot encode [ci skip] et al.",2021-02-05T15:11:18Z
818,774096266,Nit: !f2py integer intent(in): code should read !f2py integer intent(in):: code I cannot reproduce the failure: I pasted the Fortran code to issue18335.f90 and run $ f2py c m foo /test/f2py/issue18335.f90 with success using $ f2py | grep Version Version: 1.21.0.dev0+655.g2f466b318 ,2021-02-05T15:18:42Z
819,774106476,"Oh, thanks for checking. I am/was a bit worried that the original issue was more general and possibly did something like: arg = ""f8"" dt = np.dtype(arg) dtype_type = type(dt) dtype_type(arg) I.e. after unpickling, you would instantiate the dtype again with arguments originally meant for np.dtype. But that would break currently (and I am not sure I want it to work generally, since np.dtype is a minilanguage basically).",2021-02-05T15:34:28Z
820,774111228," I think that refers to the variable names, not the values. Strings are strings.",2021-02-05T15:42:32Z
821,774112770,"OK, glad it seems to work for you. I do think that NPY_ITEM_IS_POINTER might have bought you some small things for the structs with dynamic data (and its techncially more correct there). Basically, it probably would disallow a few things that cannot work in any case. By no means would it protect you from all the ways it could go wrong, though. Unfortunately that may just be impossible right now, but maybe we can give you new API in the future!",2021-02-05T15:44:58Z
822,774119895," aliases: &test ""[hi]"" hi: &test Passed a yaml validator. ",2021-02-05T15:56:29Z
823,774139589,"No worries. At least in our use case, we created an array first I know I have written and seen code like np.dtype(arg).type. Though that is a bit different than what we have here. Also I don't think I've seen the result be pickled in that case. That said, this case does appear to pickle correctly python In [1]: import pickle ...: import numpy as np In [2]: pickle.loads(pickle.dumps(np.dtype(""int64"").type)) Out[2]: numpy.int64 ",2021-02-05T16:29:49Z
824,774142808, are you happy with this now?,2021-02-05T16:35:14Z
825,774143459,"Yeah, pickling the dtype.type is totally fine (in fact, it is the solution here at the moment). dtype.type is the scalar type though which is different from type(dtype) its a bit confusing :).",2021-02-05T16:36:18Z
826,774144409,Does this need a backport? ,2021-02-05T16:38:02Z
827,774147317,"Yes, I think it should be good; if someone else with a pickle mind has a quick look, that would be great, though :).",2021-02-05T16:43:07Z
828,774147660," the ""build devdocs"" step times out. I have rebased this PR and now CircleCI succeeds, hooray! Rendered: I guess the remaining CI failures are unrelated, right?",2021-02-05T16:43:43Z
829,774151168,"I tried devel version 1.21.0.dev0+655.g2f466b318 3.9.1 (default, Dec 16 2020, 0101) [GCC 10.2.1 20201125 (Red Hat 10.2.19)] but same result.",2021-02-05T16:48:59Z
830,774151245," No, no back port needed. The win64 PyPy run was added after the 1.20 branch, and the Linux one works the same, just cleaner.",2021-02-05T16:49:06Z
831,774152223,"BTW, the reason why I tried this change kin the first place is because with the argument in the callback I get a segfault with numpy 1.20 that does not occur with 1.19.5. I will file separate report.",2021-02-05T16:50:57Z
832,774152274,In it goes. Thanks Matti.,2021-02-05T16:51:03Z
833,774157871," can you send the full output of the f2py process? It is strange that the code argument is typed as signed_char, could you try a rename code foo?",2021-02-05T17:00:28Z
834,774159874,Thanks Sebastian.,2021-02-05T17:03:34Z
835,774216693,Thanks Bas.,2021-02-05T18:42:03Z
836,774225127,"Using var instead of sigma may be acceptable. However, using the latter would have the advantage that we would be consistent with the notation used by [scipy.optimize.curve_fit]( Scipy explicitly distinguishes the cases where a user passes a vector of sigma or a full 2D covariance matrix.",2021-02-05T18:58:40Z
837,774233153,It looks like a f2py bug related to needs processing. I wish I could reproduce it.. Can you reproduce it with the Fortran code given in this issue only or does the reproducer require the full set of Fortran source you have?,2021-02-05T19:14:23Z
838,774248803,"There are lots of uses for weights besides normalizing the variance, for instance, masking or robust least squares (IRLS).",2021-02-05T19:44:42Z
839,774257965,"It is part of a larger code project, so not so easy to tear out. Linking would not work, but since the issue is at compile time, I may attempt to just selectively uncomment routines. Here a set files that may suffice for compilation [xxx.zip]( the attached version is the one that compiles; for the error you'd need to replace by fortran subroutine pyexit(code) implicit none integer(kind=4), intent(IN):: code !f2py integer intent(in) :: code !f2py intent(callback, hide) :: endkepler() external endkepler call endkepler() end subroutine pyexit ",2021-02-05T20:02:32Z
840,774262436,"No, even removing all the comments still break it. SInce it works for you, am I use wrong f2py flags?",2021-02-05T20:11:52Z
841,774278689,"Marked for 1.20.1 for now, but I doubt that we should delay the quick release of that, if this isn't a simple and quick thing.",2021-02-05T20:44:31Z
842,774282310,No worries. I very much appreciate the NumPy community looking at my issues.,2021-02-05T20:52:21Z
843,774286602,"The more I look at it, the more awkward the right alignment looks. When I read the docs, I do so to get the most information in the shortest amount of time. I read English from left to right. It takes an extra moment to adjust to the weird alignment. My personal feeling is that these docs should be all about utility: form follows function and all that.",2021-02-05T21:01:45Z
844,761737340,"Thanks, , I'll rebase once that's merged and add the dispatches  ",2021-01-17T05:31:04Z
845,761746590,"I think this should have an attribution, something like Based on the VCL library, which is (c) Copyright 20122020 Agner Fog and licensed under the Apache License version 2.0. is that correct?",2021-01-17T07:17:32Z
846,761772578," I think this should have an attribution, something like Based on the VCL library, which is (c) Copyright 20122020 Agner Fog and licensed under the Apache License version 2.0. is that correct? No that sounds wrong. Reminder, we don't do Apache2, see for example: We could change that at some point, but it's a significant change and needs a strong motivation and decision on the mailing list. VCL is a little hard to find, so here is a link: If this PR took code from there, that seems like a blocker for accepting it.",2021-01-17T11:07:07Z
847,761772753,"Now read the PR description, the whole PR is based on that, and it seems valuable. It may be worth considering, especially if there are no good alternatives. We'd be deciding to give up on GPLv2 compatibility.",2021-01-17T11:08:51Z
848,761779667," Hmmm... Its a little vague, not sure what im supposed to do",2021-01-17T11:24:50Z
849,761802147,"According to the above mention, the NumPy 1.19.5 release uses a workaround for the Windows 2004 bug, instead of waiting for Microsoft to fix it. NumPy 1.19.5 is a short bugfix release. Apart from fixing several bugs, the main improvement is the update to OpenBLAS 0.3.13 that works around the windows 2004 bug while not breaking execution on other platforms. This release supports Python 3.63.9 and is planned to be the last release in the 1.19.x cycle.",2021-01-17T12:12:13Z
850,761812595,", we can ask for relicensing, since we're not taking the same exact code plus the original code itself is based on T. Granlund and P. L. Montgomery work.",2021-01-17T13:26:33Z
851,761814845,", Why do we still support Darwin/PowerPC? this flag faltivec should be removed.",2021-01-17T13:44:07Z
852,761819448,", Much of the work also ends up being done in python anyway That's the whole idea behind this solution. That said, wieser is also right to warn about problems for newcomers I see it as more friendly for newcomers if you compare it with web template engines, It still python after all. ",2021-01-17T14:15:56Z
